<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Flora Wenyao Jiang</title>
        <link>https://ffflora.cat/posts/</link>
        <description>Recent content in Posts on Flora Wenyao Jiang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Tue, 13 Apr 2021 23:26:49 -0700</lastBuildDate>
        <atom:link href="https://ffflora.cat/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>AWS Machine Learning Specialty Knowledge Pickup</title>
            <link>https://ffflora.cat/posts/2021/04/aws-machine-learning-specialty-knowledge-pickup/</link>
            <pubDate>Tue, 13 Apr 2021 23:26:49 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/04/aws-machine-learning-specialty-knowledge-pickup/</guid>
            <description>This notes is some catchups for better prep for AWS machine learning specialty cert.
Data Engineering   In Kinesis Data Stream, number_of_shards = max (incoming_write_bandwidth_in_KB/1000, outgoing_read_bandwidth_in_KB/2000)
where
incoming_write_bandwidth_in_KB = average_data_size_in_KB multiplied by the number_of_records_per_seconds.
outgoing_read_bandwidth_in_KB = incoming_write_bandwidth_in_KB multiplied by the number_of_consumers.
  Glue cannot write the output in RecordIO-Protobuf format. Lambda is not suited for long-running processes such as the task of transforming 1TB data into RecordIO-Protobuf format. Kinesis Firehose is not meant to be used for batch processing use cases and it cannot write data in RecorIO-Protobuf format.</description>
            <content type="html"><![CDATA[<p>This notes is some catchups for better prep for AWS machine learning specialty cert.</p>
<h1 id="data-engineering">Data Engineering</h1>
<ol>
<li>
<p>In Kinesis Data Stream, number_of_shards = max (incoming_write_bandwidth_in_KB/1000, outgoing_read_bandwidth_in_KB/2000)</p>
<p>where</p>
<p>incoming_write_bandwidth_in_KB = average_data_size_in_KB multiplied by the number_of_records_per_seconds.</p>
<p>outgoing_read_bandwidth_in_KB = incoming_write_bandwidth_in_KB multiplied by the number_of_consumers.</p>
</li>
<li>
<p><strong>Glue cannot write the output in RecordIO-Protobuf forma</strong>t. Lambda is not suited for long-running processes such as the task of transforming 1TB data into RecordIO-Protobuf format. Kinesis Firehose is not meant to be used for batch processing use cases and it cannot write data in RecorIO-Protobuf format. Apache Spark (running on the EMR cluster in this use-case) can write the output in RecorIO-Protobuf format.</p>
</li>
<li>
<p>Converting the data to recordIO-protobuf file type can significantly improve the training time with a marginal increase in cost to store the recordIO-protobuf data on S3. Spinning up EMR clusters would be costly and require complex infrastructure maintenance.</p>
</li>
<li>
<p>For those SageMaker supervised learning algorithms which require the training data to be in CSV format, the target variable should be in the first column and it should not have a header record.</p>
</li>
<li>
<p>Kinesis Firehose can transform data to Parquet format and store it on S3 without provisioning any servers. Also this transformed data can be read into an Athena Table via a Glue Crawler and then the underlying data is readily available for ad-hoc analysis. Although Glue ETL Job can transform the source data to Parquet format, it is best suited for batch ETL use cases and it’s not meant to process streaming data. EMR cluster is not an option as the company does not want to manage the underlying infrastructure.</p>
</li>
<li>
<p><strong>Kinesis Data Firehose is used for streaming data scenarios</strong>. <strong>AWS Glue ML Transforms job can perform deduplication</strong> in a serverless fashion.</p>
</li>
<li>
<p>There is no such thing as a LOAD command for Redshift. COPY is much faster than insert. UNLOAD is used to write the results of a Redshift query to one or more text files on Amazon S3.</p>
</li>
<li>
<p>If you want Amazon SageMaker to replicate a subset of data on each ML compute instance that is launched for model training, specify <code>ShardedByS3Key</code> for S3DataDistributionType field.</p>
</li>
<li>
<p>Kinesis Product Library provides built-in performance benefits and ease of use advantages. Please review more details here:</p>
<p><a href="https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html#developing-producers-with-kpl-advantage">https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html#developing-producers-with-kpl-advantage</a></p>
</li>
<li>
<p>Kinesis Data Streams PutRecord API uses name of the stream, a partition key and the data blob whereas Kinesis Data Firehose PutRecord API uses the name of the delivery stream and the data record.</p>
</li>
<li>
<p>Hyperparameters should be tuned against the Validation Set.</p>
</li>
</ol>
<h1 id="exploratory-data-analysis">Exploratory Data Analysis</h1>
<ol>
<li>
<p>In case of a binary classification model with strongly unbalanced classes, we need to over-sample from the minority class, collect more training data for the minority class and create more samples using algorithms such as SMOTE which effectively uses a k-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. Over-sampling from the positive class or collecting more training data for the positive class would further aggravate the situation.</p>
<p><a href="http://www.svds.com/learning-imbalanced-classes/">http://www.svds.com/learning-imbalanced-classes/</a></p>
<p><a href="https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes">https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes</a></p>
</li>
<li>
<p>A data warehouse can only store structured data whereas a data lake can store structured, semi-structured and unstructured data.</p>
</li>
<li>
<p>Great reference for the most common probability distributions: <a href="https://medium.com/@srowen/common-probability-distributions-347e6b945ce4">Common Probability Distributions: The Data Scientist’s Crib Sheet</a></p>
</li>
<li>
<p>You can use Inference Pipeline to package Spark and scikit-learn based preprocessors into containers:</p>
<p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-mleap-scikit-learn-containers.html">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-mleap-scikit-learn-containers.html</a></p>
</li>
<li>
<p>Tf-idf is a statistical technique frequently used in Machine Learning areas such as text-summarization and classification. Tf-idf measures the relevance of a word in a document compared to the entire corpus of documents. You have a corpus (D) containing the following documents:</p>
<p>Document 1 (d1) : “A quick brown fox jumps over the lazy dog. What a fox!”</p>
<p>Document 2 (d2) : “A quick brown fox jumps over the lazy fox. What a fox!”</p>
<p>Which of the following statements is correct:</p>
<ul>
<li>Using tf-idf, the word “fox” is equally relevant for both document d1 and document d2</li>
</ul>
<p>tf is the frequency of any &ldquo;term&rdquo; in a given &ldquo;document&rdquo;. Using this definition, we can compute the following:</p>
<p>tf(“fox”, d1) = 2/12 , as the word &ldquo;fox&rdquo; appears twice in the first document which has a total of 12 words</p>
<p>tf(“fox”, d2) = 3/12 , as the word &ldquo;fox&rdquo; appears thrice in the second document which has a total of 12 words</p>
<p>An idf is constant per corpus (in this case, the corpus consists of 2 documents) , and accounts for the ratio of documents that include that specific &ldquo;term&rdquo;. Using this definition, we can compute the following:</p>
<p>idf(“fox”, D) = log(2/2) = 0 , as the word &ldquo;fox&rdquo; appears in both the documents in the corpus</p>
<p>Now,</p>
<p>tf-idf(“fox”, d1, D) = tf(“fox”, d1) * idf(“fox”, D) = (2/12) * 0 = 0</p>
<p>tf-idf(“fox”, d2, D) = tf(“fox”, d2) * idf(“fox”, D) = (3/12) * 0 = 0</p>
<p>Using tf-idf, the word “fox” is equally relevant (or just irrelevant!) for both document d1 and document d2</p>
<p>You can further read about tf-idf on this reference link:</p>
<p><a href="https://en.wikipedia.org/wiki/Tf">https://en.wikipedia.org/wiki/Tf</a>–idf</p>
</li>
<li>
<p>Logarithm transformation and Standardization are the correct techniques to address outliers in data. Please review this reference link:</p>
<p><a href="https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114">https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114</a></p>
</li>
<li>
<p>ElasticSearch, EMR and EC2 are not “serverless”.</p>
</li>
<li>
<p>The best way to engineer the cyclical features is to represent these as (x,y) coordinates on a circle using sin and cos functions. Please review this technique in more detail here -</p>
<p><a href="http://blog.davidkaleko.com/feature-engineering-cyclical-features.html">http://blog.davidkaleko.com/feature-engineering-cyclical-features.html</a></p>
</li>
<li>
<p>Interquartile Range (IQR) = Q3-Q1</p>
<p>Minimum outlier cutoff = Q1 - 1.5 * IQR</p>
<p>Maximum outlier cutoff = Q3 + 1.5 * IQR</p>
<p>More details on the box plot statistical characteristics:</p>
<p><a href="https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51">https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51</a></p>
</li>
</ol>
<h1 id="modeling">Modeling</h1>
<ol>
<li>
<p>Object2Vec can be used to find semantically similar objects such as questions. BlazingText Word2Vec can only find semantically similar words.</p>
</li>
<li>
<p><strong>Incremental Training</strong> in Amazon SageMaker</p>
<p>Over time, you might find that a model generates inference that are not as good as they were in the past. With incremental training, you can use the artifacts from an existing model and use an expanded dataset to train a new model. Incremental training saves both time and resources.</p>
<p>Use incremental training to:</p>
<ul>
<li>Train a new model using an expanded dataset that contains an underlying pattern that was not accounted for in the previous training and which resulted in poor model performance.</li>
<li>Use the model artifacts or a portion of the model artifacts from a popular publicly available model in a training job. You don&rsquo;t need to train a new model from scratch.</li>
<li>Resume a training job that was stopped.</li>
<li>Train several variants of a model, either with different hyperparameter settings or using different datasets.</li>
</ul>
</li>
<li>
<p>Blazing Text algorithm can be used in both supervised and unsupervised learning modes.</p>
</li>
<li>
<p>SageMaker DeepAR algorithm specializes in forecasting new product performance.</p>
</li>
<li>
<p>Specificity = (True Negatives / (True Negatives + False Positives))</p>
<p>If the model has a high specificity, it implies that all false positives (think of it as false alarms) have been weeded out. In other words, the <strong>specificity</strong> of a test refers to how well the test identifies those who have not indulged in substance abuse.</p>
<p>Please read this excellent reference article for more details:</p>
<p><a href="https://www.statisticshowto.datasciencecentral.com/sensitivity-vs-specificity-statistics/">https://www.statisticshowto.datasciencecentral.com/sensitivity-vs-specificity-statistics/</a></p>
</li>
<li>
<p>LDA: Observations are referred to as documents. The feature set is referred to as vocabulary. A feature is referred to as a word. And the resulting categories are referred to as topics.</p>
</li>
<li>
<p><strong>mode</strong> is the mandatory hyperparameter for both the Word2Vec (unsupervised) and Text Classification (supervised) modes of the SageMaker BlazingText algorithm.</p>
</li>
<li>
<p>Factorization Machines algorithm specializes in building recommendation systems.</p>
</li>
<li>
<p>Image Classification is used to classify images into multiple classes such as cat vs dog. Object Detection is used to detect objects in an image. Semantic Segmentation is used for pixel level analysis of an image and it can be used in this computer vision system to detect misalignment</p>
</li>
<li>
<p>feature_dim and k are the required hyperparameters for the SageMaker K-means algorithm</p>
</li>
<li>
<p>When you use automatic model tuning, the linear learner internal tuning mechanism is turned off automatically. This sets the number of parallel models, num_models, to 1.</p>
</li>
<li>
<p>AUC/ROC  is the best evaluation metric for a binary classification model. this metric does not require you to set a classification threshold.</p>
<p>For imbalanced datasets, you are better off using another metric called - <strong>PR AUC</strong> - that is also used in production systems for a highly imbalanced dataset, where the fraction of positive class is small, such as in case of credit card fraud detection.</p>
</li>
<li>
<p>You can think of L1 as reducing the number of features in the model altogether. L2 “regulates” the feature weight instead of just dropping them. Please review the concept of L1 and L2 regularization in more detail:</p>
<p><a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c</a></p>
</li>
<li>
<p>Factorization Machine can be used to capture click patterns for a click prediction system:</p>
</li>
<li>
<p>The residuals plot would indicate any trend of underestimation or overestimation. Both Mean Absolute Error and RMSE would only give the error magnitude. AUC is a metric used for classification models.</p>
</li>
<li>
<p>LDA is a &ldquo;bag-of-words&rdquo; model, which means that the order of words does not matter</p>
</li>
</ol>
<h1 id="ml-implementation-and-operation">ML Implementation and Operation</h1>
<ol>
<li>
<p>Inference Pipeline can be considered as an Amazon SageMaker model that you can use to make either real-time predictions or to process batch transforms directly without any external preprocessing.</p>
</li>
<li>
<p>Neo currently supports image classification models exported as frozen graphs from TensorFlow, MXNet, or PyTorch, and XGBoost models.:</p>
<p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html">https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html</a></p>
</li>
<li>
<p>The default Sagemaker IAM role gets permissions to access any bucket that has sagemaker in the name. If you add a policy to the role that grants the SageMaker service principal S3FullAccess permission, the name of the bucket does not need to contain sagemaker. Granting public access to S3 bucket is not recommended. You can read further on this -</p>
<p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html">https://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html</a></p>
</li>
<li>
<p>version tag should be used in the Registry Paths:</p>
<p>“:1” is the correct version tag for <strong>production</strong> systems.</p>
</li>
<li>
<p>SageMaker does not support resource based policies. You can create a role to delegate access or provide access via identity federation.</p>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>React.js Related Notes (1)</title>
            <link>https://ffflora.cat/posts/2021/03/react.js-related-notes-1/</link>
            <pubDate>Wed, 24 Mar 2021 00:08:29 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/03/react.js-related-notes-1/</guid>
            <description>This series of posts summaries the problems/issues I met during work, and the solutions of how I solved these problems.
  How do I parse a .json data file to pass into this DevExtreme component and render out a series of area charts?
Data looks like:
type Data = { period_index: number, breakdown: { sessions: number, conversions: number, avg_page_views: number, avg_time_on_page: number, new_users: number, } }[] parseData = (data: Data | any) =&amp;gt; { if (data.</description>
            <content type="html"><![CDATA[<p>This series of posts summaries the problems/issues I met during work, and the solutions of how I solved these problems.</p>
<ol>
<li>
<p>How do I parse a <code>.json</code> data file to pass into <a href="https://js.devexpress.com/Demos/WidgetsGallery/Demo/Charts/Area/React/Light/">this</a> DevExtreme component and render out a series of area charts?</p>
<p>Data looks like:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#a6e22e">type</span> <span style="color:#a6e22e">Data</span> <span style="color:#f92672">=</span> {
  <span style="color:#a6e22e">period_index</span>: <span style="color:#66d9ef">number</span>,
  <span style="color:#a6e22e">breakdown</span><span style="color:#f92672">:</span> {
    <span style="color:#a6e22e">sessions</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">conversions</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">avg_page_views</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">avg_time_on_page</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">new_users</span>: <span style="color:#66d9ef">number</span>,
  }
}[]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-react" data-lang="react"><span style="color:#a6e22e">parseData</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">data</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">Data</span> <span style="color:#f92672">|</span> <span style="color:#a6e22e">any</span>) =&gt; {
  <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">length</span> <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) {
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">sampleData</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">data</span>[<span style="color:#ae81ff">0</span>];
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">keys</span> <span style="color:#f92672">=</span> Object.<span style="color:#a6e22e">keys</span>(<span style="color:#a6e22e">sampleData</span>.<span style="color:#a6e22e">breakdown</span>);
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">resultWithMonth</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">map</span>((<span style="color:#a6e22e">d</span><span style="color:#f92672">:</span> { <span style="color:#a6e22e">period_index</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>; <span style="color:#a6e22e">month</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>; <span style="color:#a6e22e">date</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>; }) =&gt; {
   
      <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">month</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">period_index</span> <span style="color:#f92672">%</span> <span style="color:#ae81ff">12</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
      <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">date</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">moment</span>(<span style="color:#e6db74">`</span><span style="color:#e6db74">${</span><span style="color:#a6e22e">month</span><span style="color:#e6db74">}</span><span style="color:#e6db74">`</span>, <span style="color:#e6db74">&#34;M&#34;</span>).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;MMM&#39;</span>)
      <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">month</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">month</span>
      <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">date</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">date</span>
      <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">d</span>
    })
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">result</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">keys</span>.<span style="color:#a6e22e">map</span>(<span style="color:#a6e22e">key</span> =&gt; {
      <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">dataSource</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">resultWithMonth</span>.<span style="color:#a6e22e">map</span>((<span style="color:#a6e22e">d</span><span style="color:#f92672">:</span> { <span style="color:#a6e22e">period_index</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">any</span>; <span style="color:#a6e22e">breakdown</span><span style="color:#f92672">:</span> { [<span style="color:#a6e22e">x</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>]<span style="color:#f92672">:</span> <span style="color:#a6e22e">any</span>; }; <span style="color:#a6e22e">date</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">any</span>; }) =&gt; {
        <span style="color:#66d9ef">return</span> {
          <span style="color:#a6e22e">period_index</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">period_index</span>,
          [<span style="color:#a6e22e">key</span>]<span style="color:#f92672">:</span> <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">breakdown</span>[<span style="color:#a6e22e">key</span>],
          <span style="color:#a6e22e">monthString</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">date</span>
        }
      })
   
      <span style="color:#66d9ef">return</span> {
   
        <span style="color:#a6e22e">title</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">key</span>,
        <span style="color:#a6e22e">subtitle</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">numeral</span>(<span style="color:#a6e22e">dataSource</span>[<span style="color:#a6e22e">dataSource</span>.<span style="color:#a6e22e">length</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>][<span style="color:#a6e22e">key</span>]).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;0,0&#39;</span>),
        <span style="color:#a6e22e">dataSource</span>
      }
    })
   
    <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">result</span>
  }
  <span style="color:#66d9ef">return</span> []
}
<span style="color:#a6e22e">parseMonthData</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">period</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>) =&gt; {
   
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">month</span> <span style="color:#f92672">=</span> (parseInt(<span style="color:#a6e22e">period</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">12</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">year</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">1900</span> <span style="color:#f92672">+</span> (parseInt(<span style="color:#a6e22e">period</span>) <span style="color:#f92672">-</span> (parseInt(<span style="color:#a6e22e">period</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">12</span>)) <span style="color:#f92672">/</span> <span style="color:#ae81ff">12</span>
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">date</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">moment</span>(<span style="color:#e6db74">`</span><span style="color:#e6db74">${</span><span style="color:#a6e22e">month</span><span style="color:#e6db74">}</span><span style="color:#e6db74">`</span>, <span style="color:#e6db74">&#34;M&#34;</span>).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;MMMM&#39;</span>)
  <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">date</span>
}
</code></pre></div><p>Chart looks like:</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/blog/1.png?raw=true" alt="image-20210324014548148"></p>
</li>
<li>
<p>How to parse data and get color ranges so that the colors in the table are dynamically changed based on the value in the cell?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-react" data-lang="react"><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">COLUMNS</span> <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;value&#39;</span>, <span style="color:#e6db74">&#39;count&#39;</span>, <span style="color:#e6db74">&#39;Clicks&#39;</span>, <span style="color:#e6db74">&#39;Conversions&#39;</span>, <span style="color:#e6db74">&#39;Cost&#39;</span>, <span style="color:#e6db74">&#39;avg_cpm&#39;</span>, <span style="color:#e6db74">&#39;avg_cpc&#39;</span>, <span style="color:#e6db74">&#39;avg_cpa&#39;</span>]
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">COLOR</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#1c558e&#39;</span>
   
   
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">getColorRanges</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">data</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">DataType</span>[] <span style="color:#f92672">|</span> <span style="color:#a6e22e">any</span>)<span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>[][] =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorRanges</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">COLUMNS</span>.<span style="color:#a6e22e">map</span>(<span style="color:#a6e22e">col</span> =&gt; {
    <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">col</span> <span style="color:#f92672">===</span> <span style="color:#e6db74">&#39;value&#39;</span>) <span style="color:#66d9ef">return</span> [] <span style="color:#75715e">//keywords, do nothing. 
</span><span style="color:#75715e"></span>   
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">sorted</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">data</span>
      .<span style="color:#a6e22e">map</span>((<span style="color:#a6e22e">x</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">DataType</span>) =&gt; <span style="color:#a6e22e">x</span>[<span style="color:#a6e22e">col</span>])
      .<span style="color:#a6e22e">sort</span>((<span style="color:#a6e22e">a</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>, <span style="color:#a6e22e">b</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>) =&gt; <span style="color:#a6e22e">a</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">b</span>);
   
    <span style="color:#66d9ef">return</span> [<span style="color:#a6e22e">sorted</span>[<span style="color:#ae81ff">0</span>], <span style="color:#a6e22e">sorted</span>[<span style="color:#a6e22e">sorted</span>.<span style="color:#a6e22e">length</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]];
  })
   
  <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">colorRanges</span>
}
   
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorRanges</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">getColorRanges</span>(<span style="color:#a6e22e">data</span>);
   
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">getBackgroundColor</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">column</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>, <span style="color:#a6e22e">value</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>) =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">columnIndex</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">COLUMNS</span>.<span style="color:#a6e22e">indexOf</span>(<span style="color:#a6e22e">column</span>);
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorRange</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">colorRanges</span>[<span style="color:#a6e22e">columnIndex</span>];
  <span style="color:#66d9ef">const</span> [<span style="color:#a6e22e">min</span>, <span style="color:#a6e22e">max</span>] <span style="color:#f92672">=</span> <span style="color:#a6e22e">colorRange</span>;
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorWeight</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">value</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">min</span>) <span style="color:#f92672">/</span> (<span style="color:#a6e22e">max</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">min</span>);
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">color</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">COLOR</span> <span style="color:#f92672">+</span> Math.<span style="color:#a6e22e">ceil</span>(<span style="color:#ae81ff">128</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">colorWeight</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">127</span>).<span style="color:#a6e22e">toString</span>(<span style="color:#ae81ff">16</span>);
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">opacity</span> <span style="color:#f92672">=</span> (<span style="color:#ae81ff">128</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">colorWeight</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">127</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255</span>
  <span style="color:#66d9ef">return</span> [<span style="color:#a6e22e">color</span>, <span style="color:#a6e22e">opacity</span>];
}
<span style="color:#75715e">// inside the devExpress data Grid component, there&#39;s a config is called `cellRender`, where you can adjust the style and/or render config for cells.
</span><span style="color:#75715e"></span><span style="color:#a6e22e">cellRender</span><span style="color:#f92672">=</span>{<span style="color:#a6e22e">cell</span> =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">columnWithDollar</span> <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Cost&#39;</span>, <span style="color:#e6db74">&#39;CPC&#39;</span>, <span style="color:#e6db74">&#39;CPM&#39;</span>, <span style="color:#e6db74">&#39;CPA&#39;</span>]
  <span style="color:#66d9ef">const</span> { <span style="color:#a6e22e">caption</span> } <span style="color:#f92672">=</span> <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">column</span>
  <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">col</span> <span style="color:#f92672">!==</span> <span style="color:#e6db74">&#39;value&#39;</span>) {
          <span style="color:#66d9ef">const</span> [<span style="color:#a6e22e">backgroundColor</span>, <span style="color:#a6e22e">opacity</span>] <span style="color:#f92672">=</span> <span style="color:#a6e22e">getBackgroundColor</span>(<span style="color:#a6e22e">col</span>, <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]);
         <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">cellElement</span>.<span style="color:#a6e22e">style</span>.<span style="color:#a6e22e">backgroundColor</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">backgroundColor</span>
         <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">opacity</span> <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.9</span>) {
           <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">cellElement</span>.<span style="color:#a6e22e">style</span>.<span style="color:#a6e22e">color</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#ddd&#39;</span>
  }
  <span style="color:#66d9ef">return</span> (<span style="color:#a6e22e">columnWithDollar</span>.<span style="color:#a6e22e">includes</span>(<span style="color:#a6e22e">caption</span>) <span style="color:#f92672">?</span> 		(<span style="color:#a6e22e">numeral</span>(<span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;$0,0.00&#39;</span>)) <span style="color:#f92672">:</span> (<span style="color:#a6e22e">numeral</span>(<span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;0,0&#39;</span>)));
  }
  <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]
  }}
</code></pre></div><p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/blog/2.png?raw=true" alt="image-20210330155336896"></p>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>AWS SageMaker Deep Dive</title>
            <link>https://ffflora.cat/posts/2021/02/aws-sagemaker-deep-dive/</link>
            <pubDate>Sat, 06 Feb 2021 17:02:04 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/02/aws-sagemaker-deep-dive/</guid>
            <description>This post consist of the notes that are based on the series of AWS SageMaker videos provided by Amazon Web Services Amazon SageMaker Technical Deep Dive Series.
Fully-Managed Notebook Instances with Amazon SageMaker Create your Notebook Instance:
 Pick the right family:  t - tiny &amp;lt; m, c - computed optimized, p - GPU   Pick the right size:  From medium to very very large.   Pick the right version:  ml.</description>
            <content type="html"><![CDATA[<p>This post consist of the notes that are based on the series of AWS SageMaker videos provided by Amazon Web Services <a href="https://www.youtube.com/playlist?list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz">Amazon SageMaker Technical Deep Dive Series</a>.</p>
<h1 id="fully-managed-notebook-instances-with-amazon-sagemaker">Fully-Managed Notebook Instances with Amazon SageMaker</h1>
<p>Create your Notebook Instance:</p>
<ol>
<li>Pick the right family:
<ul>
<li>t - tiny &lt; m,   c - computed optimized, p - GPU</li>
</ul>
</li>
<li>Pick the right size:
<ul>
<li>From medium to very very large.</li>
</ul>
</li>
<li>Pick the right version:
<ul>
<li>ml.t<strong>3</strong>.medium: 3 means the latest version in the T family</li>
</ul>
</li>
</ol>
<p>There are ~200 example notebooks in SageMaker Notebook, definitely try them out.</p>
<h1 id="built-in-machine-learning-algorithms-with-amazon-sagemaker">Built-in Machine Learning Algorithms with Amazon SageMaker</h1>
<p>** = Distributed Training*</p>
<p><em>&lt;&gt; = incremental training</em></p>
<table>
<thead>
<tr>
<th>Type of Problem</th>
<th>Algorithms</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classification</td>
<td>Linear Learner *<!-- raw HTML omitted -->XGBoost<!-- raw HTML omitted -->KNN<!-- raw HTML omitted -->Fatorization Machines</td>
</tr>
<tr>
<td>Computer Vision</td>
<td>Image Classification &lt;&gt;<!-- raw HTML omitted -->Object Detection<!-- raw HTML omitted -->Semantic Segmentation</td>
</tr>
<tr>
<td>Topic Modeling</td>
<td>LDA<!-- raw HTML omitted -->NTM</td>
</tr>
<tr>
<td>Working with Text</td>
<td>Blazing Text<!-- raw HTML omitted -->- Supervised<!-- raw HTML omitted -->- Unsupervised *</td>
</tr>
<tr>
<td>Recommendation</td>
<td>Fatorization Machines * (+ KNN)</td>
</tr>
<tr>
<td>Forecasting</td>
<td>DeepAR *</td>
</tr>
<tr>
<td>Anomaly Detection</td>
<td>Random Cut Forests *<!-- raw HTML omitted -->IP Insights *</td>
</tr>
<tr>
<td>Clustering</td>
<td>KMeans *<!-- raw HTML omitted -->KNN</td>
</tr>
<tr>
<td>Sequence Translation</td>
<td>Seq2Seq *</td>
</tr>
<tr>
<td>Regression</td>
<td>Linear Learner<!-- raw HTML omitted -->XGBoost<!-- raw HTML omitted -->KNN</td>
</tr>
<tr>
<td>Feature Reduction</td>
<td>PCA <!-- raw HTML omitted -->Object2Vec</td>
</tr>
</tbody>
</table>
<h2 id="heading"></h2>
<h2 id="heading-1"></h2>
<h2 id="heading-2"></h2>
]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Specialty Prep List</title>
            <link>https://ffflora.cat/posts/2021/01/aws-machine-learning-specialty-prep-list/</link>
            <pubDate>Sun, 31 Jan 2021 19:29:26 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/01/aws-machine-learning-specialty-prep-list/</guid>
            <description>Here are some resources I collected for better preparing the AWS Machine Learning Specialty Exam.
Practical experience aws/amazon-sagemaker-examples
Object Detection with Amazon Sagemaker
AWS Training offers digital courses on machine learning (ML).   Data Analytics Fundamentals
  Exam Readiness: AWS Certified Machine Learning - Specialty
  Deep Learning on AWS
  Elements of Data Science
  Math for Machine Learning
  Linear and Logistic Regression</description>
            <content type="html"><![CDATA[<p>Here are some resources I collected for better preparing the AWS Machine Learning Specialty Exam.</p>
<h3 id="practical-experience">Practical experience</h3>
<p><a href="https://github.com/aws">aws</a>/<strong><a href="https://github.com/aws/amazon-sagemaker-examples">amazon-sagemaker-examples</a></strong></p>
<p><a href="https://www.coursera.org/projects/object-detection-sagemaker">Object Detection with Amazon Sagemaker</a></p>
<h3 id="aws-training-offers-digital-courses-on-machine-learning-ml">AWS Training offers digital courses on machine learning (ML).</h3>
<ul>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=35364">Data Analytics Fundamentals</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=42183">Exam Readiness: AWS Certified Machine Learning - Specialty</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/training/course-descriptions/deep-learning/">Deep Learning on AWS</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=26598">Elements of Data Science</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=26597">Math for Machine Learning</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=26599">Linear and Logistic Regression</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=12531">Machine Learning Use Case</a></p>
</li>
</ul>
<h3 id="aws-whitepapers">AWS whitepapers</h3>
<blockquote>
<p>The following are whitepapers that have been written about ML and deep learning that you will want to read as part of your exam study:</p>
</blockquote>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/whitepapers/Deep_Learning_on_AWS.pdf?did=wp_card&amp;trk=wp_card">Deep Learning on AWS</a></li>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/whitepapers/aws-power-ml-at-scale.pdf?did=wp_card&amp;trk=wp_card">Power machine learning at scale</a></li>
</ul>
<h3 id="aws-documentation">AWS documentation</h3>
<blockquote>
<p>Review the following AWS service documentation as part of your study:</p>
</blockquote>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/sagemaker/?id=docs_gateway">Amazon SageMaker</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/dlami/?id=docs_gateway">Deep Learning AMI</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/polly/?id=docs_gateway">Amazon Polly</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/lex/?id=docs_gateway">Amazon Lex</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/transcribe/?id=docs_gateway">Amazon Transcribe</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/translate/?id=docs_gateway">Amazon Translate</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/rekognition/?id=docs_gateway">Amazon Rekognition</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/s3/?id=docs_gateway">Amazon S3</a></li>
</ul>
<h3 id="service-faqs">Service FAQs</h3>
<blockquote>
<p>Here are links to service FAQs that have some helpful information when looking to better understand ML-related AWS services:</p>
</blockquote>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/sagemaker/faqs/">AWS SageMaker FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/comprehend/faqs/">AWS Comprehend FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/lex/faqs/">AWS Lex FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/polly/faqs/">AWS Polly FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/rekognition/faqs/">AWS Rekognition FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/translate/faqs/">AWS Translate FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/transcribe/faqs/">AWS Transcribe FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/deeplens/faqs/">AWS DeepLens FAQ</a></li>
</ul>
<h3 id="aws-blogs">AWS blogs</h3>
<blockquote>
<p>There is an <a href="https://aws.amazon.com/blogs/machine-learning/">AWS machine learning blog</a> that you should read as part of your exam study.</p>
</blockquote>
<h3 id="official-exam-guide-sample-exam-questions">Official Exam Guide, sample exam questions</h3>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/training-and-certification/docs-ml/AWS-Certified-Machine-Learning-Specialty_Exam-Guide.pdf">Download the exam guide</a></li>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/training-and-certification/docs-ml/AWS-Certified-Machine-Learning-Specialty_Sample-Questions.pdf">Download the sample questions</a></li>
<li><input disabled="" type="checkbox"> <a href="https://www.udemy.com/course/aws-certified-machine-learning-specialty-full-practice-exams/">Udemy - AWS Certified Machine Learning Specialty: 3 PRACTICE EXAMS</a></li>
<li><input disabled="" type="checkbox"> <a href="https://amazonmr.au1.qualtrics.com/reports/RC/public/YW1hem9ubXItNWQ0YWQzYTViZjczMDIwMDBmNTc4ZWY3LVVSX2RvRFVNUlZZS1NtQjUyWg==">Exam Readiness - Sample Questions</a></li>
</ul>
<h3 id="other-videos">Other Videos</h3>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://www.youtube.com/playlist?list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz">Amazon SageMaker Deep Dive Series</a></li>
</ul>
<h1 id="-٩-ω-و-">＼＼\ ٩( &lsquo;ω&rsquo; )و //／／</h1>
]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Exam Readiness with Sample Questions</title>
            <link>https://ffflora.cat/posts/2021/01/aws-machine-learning-exam-readiness-with-sample-questions/</link>
            <pubDate>Sat, 30 Jan 2021 00:20:32 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/01/aws-machine-learning-exam-readiness-with-sample-questions/</guid>
            <description>These are my study notes directly from AWS Training cource - AWS Machine Learning Exam Readiness.
Domain 1: Data Engineering Create Data Repo for Machine Learning Identify and implement a data ingestion solution To use the data for ML, you need to ingest it into a service like Amazon S3
Batch and stream processing are two kinds of data ingestion.
Batch processing Batch processing periodically collects and groups source data.With batch processing, the ingestion layer periodically collects and groups source data and sends it to a destination like Amazon S3.</description>
            <content type="html"><![CDATA[<p>These are my study notes directly from AWS Training cource - <em>AWS Machine Learning Exam Readiness</em>.</p>
<h2 id="domain-1-data-engineering">Domain 1: Data Engineering</h2>
<h3 id="create-data-repo-for-machine-learning">Create Data Repo for Machine Learning</h3>
<h3 id="identify-and-implement-a-data-ingestion-solution"><strong>Identify and implement a data ingestion solution</strong></h3>
<p>To use the data for ML, you need to ingest it into a service like Amazon S3</p>
<p><strong>Batch</strong> and <strong>stream</strong> processing are two kinds of data ingestion.</p>
<h4 id="batch-processing">Batch processing</h4>
<p><!-- raw HTML omitted -->Batch processing periodically collects and groups source data.<!-- raw HTML omitted --></p>
<p>With batch processing, the ingestion layer periodically collects and groups source data and sends it to a destination like Amazon S3. Batch processing is typically used when there is no real need for real-time or near-real-time data, because it is generally easier and more affordably implemented than other ingestion options.</p>
<h4 id="aws-solutions">AWS Solutions:</h4>
<p>AWS <strong>Glue</strong>(ETL tool), AWS <strong>Database Migration</strong>, AWS <strong>Step Functions</strong>.</p>
<p>(You can automate various ETL tasks that involve complex workflows by using AWS Step Functions.)</p>
<h4 id="stream-processing">Stream Processing</h4>
<p><!-- raw HTML omitted -->Stream processing manipulates and loads data as it’s recognized in real time.<!-- raw HTML omitted --></p>
<h4 id="aws-solutions-1">AWS Solutions:</h4>
<p>**Amazon Kinesis **</p>
<h3 id="identify-and-implement-a-data-transformation-solution"><strong>Identify and implement a data transformation solution</strong></h3>
<h5 id="some-solutions">Some Solutions:</h5>
<ul>
<li>Using Apache Spark on Amazon EMR provides a managed framework</li>
</ul>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114223729670.png?raw=true" alt="image-20210114223729670"></p>
<ul>
<li>
<p>If your datasets or computations are not optimally compatible with SQL, you can use AWS Glue to seamlessly run Spark jobs (Scala and Python support) on data stored in your Amazon S3 buckets.</p>
</li>
<li>
<p>You can store a single source of data in Amazon S3 and perform ad hoc analysis.</p>
<p>This reference architecture shows how AWS services for big data and ML can help build a scalable analytical layer for healthcare data. Customers can store a single source of data in Amazon S3 and perform ad hoc analysis with Athena, integrate with a data warehouse on Amazon Redshift, build a visual dashboard for metrics using Amazon QuickSight, and build an ML model to predict readmissions using Amazon SageMaker. By not moving the data around and connecting to it using different services, customers avoid building redundant copies of the same data.<img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114224855633.png?raw=true" alt="image-20210114224855633"></p>
</li>
</ul>
<h3 id="sample-questions">Sample Questions</h3>
<ol>
<li>
<p>A data engineer needs to create a <strong>cost-effective</strong> data pipeline solution that ingests <strong>unstructured data from various sources</strong> and stores it for downstream analytics applications and ML. The solution should include a data store <strong>where the processed data is highly available for at least one year</strong>, so that data analysts and data scientists can run analytics and ML workloads on the most recent data. For compliance reasons, the solution should include both processed and raw data. The raw data does not need to be accessed regularly, but when needed, should be <strong>accessible within 24 hours.</strong></p>
<p><strong>ANS</strong>:</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114225713429.png?raw=true" alt="image-20210114225713429"></p>
</li>
<li>
<p>An ad-tech company has hired a data engineer to create and maintain a machine learning pipeline for its clickstream data. The data will be gathered <strong>from various sources,</strong> including on premises, and will need to be <strong>streamed</strong> to the company’s <strong>Amazon EMR instances</strong> for further processing.</p>
<p><strong>ANS</strong>:</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114230019872.png?raw=true" alt="image-20210114230019872"></p>
</li>
<li>
<p>A transportation company currently uses Amazon EMR with Apache Spark for some of its data transformation workloads. It transforms columns of geographical data (like latitudes and longitudes) and adds columns to segment the data into different clusters per city to attain additional features for the k-nearest neighbors algorithm being used.</p>
<p>The company wants less operational overhead for their transformation pipeline. They want a new solution that does not make significant changes to the current pipeline and only requires minimal management.</p>
<p>What AWS services should the company use to build this new pipeline?</p>
<ul>
<li><strong>Use AWS Glue to transform files. Use Amazon S3 as the destination.</strong> ✅</li>
<li>Use AWS Glue to transform files. Use Amazon EMR HDFS as the destination. ❌</li>
<li>Use Amazon EMR to transform files. Use Amazon S3 as the destination. ❌</li>
<li>Use Lambda to transform files. Use Amazon EMR HDFS as the destination. ❌</li>
</ul>
</li>
</ol>
<h2 id="domain-2-eda">Domain 2: EDA</h2>
<h3 id="sample-questions-1">Sample Questions:</h3>
<ol>
<li>
<p>A team of data scientists in a company focusing on security and smart home devices created an ML model that can classify guest types at a front door using a video doorbell. The team is getting an accuracy of 96.23% on the validation dataset.</p>
<p>However, when the team tested this model in production, images were classified with a much lower accuracy. That was due to weather: The changing seasons had an impact on the quality of production images.</p>
<p>What can the team do to improve their model?</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210118003049500.png?raw=true" alt="image-20210118003049500"></p>
</li>
<li>
<p>A team of data scientists in a financial company wants to predict the risk for their incoming customer loan applications. The team has decided to do this by applying the <strong>XGBoost</strong> algorithm, which will predict the probability that a customer will default on a loan. In order to create this solution, the team wants to first <strong>merge the customer application data with demographic and location data</strong> before feeding it into the model.</p>
<p>However, the <strong>dimension of this data is really large</strong>, and the team wants to <strong>keep only those features that are the most relevant</strong> to the prediction.</p>
<p>What techniques can the team use to reach the goal? (Select TWO.)</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210118003924933.png?raw=true" alt="image-20210118003924933"></p>
<ul>
<li>AWS glue doesn&rsquo;t have that shit.</li>
<li>clustering doesn&rsquo;t help with dimensionality reduction.</li>
<li><strong>A</strong> helps the dimensionality problem, but doesn&rsquo;t help the feature selection problem.</li>
</ul>
</li>
<li>
<p>A social networking organization wants to analyze all the comments and likes from its users to flag offensive language on the site. The organization’s data science team wants to use a Long Short-term Memory (LSTM) architecture to classify the raw sentences from the comments into one of two categories: offensive and non-offensive.</p>
<p>What should the team do to prepare the data for the LSTM?</p>
<ul>
<li>Vectorize the sentences. Transform them into numerical sequences. Use the sentences as the input. ❌</li>
<li>Convert the individual sentences into numerical sequences starting from the number 1 for each word in a sentence. Use the sentences as the input. ❌</li>
<li>Convert the individual sentences into sequences of words. Use those as the input. ❌</li>
<li><strong>Vectorize the sentences. Transform them into numerical sequences with a padding. Use the sentences as the input. ✅</strong></li>
</ul>
</li>
</ol>
<h2 id="domain-3-modeling">Domain 3: Modeling</h2>
<h3 id="31-frame-business-problems-as-ml-problems">3.1 Frame Business Problems as ML problems</h3>
<p>Regression problem(prediction) vs classification problem</p>
<h3 id="32-select-the-appropriate-models-for-an-ml-problem">3.2 <strong>Select the appropriate model(s) for an ML problem</strong></h3>
<p>Amazon <strong>SageMaker</strong> provides a few built-in algorithms that work for <strong>classification</strong> situations: <strong>Linear Learner, XGBoost, and K-Nearest Neighbors.</strong> <strong>XGBoost</strong>, for instance, is an open-source implementation of the gradient-boosted trees algorithm. <strong>Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler, weaker models.</strong></p>
<p>In terms of the built-in Amazon <strong>SageMaker</strong> algorithms you could choose for <strong>regression problems</strong>, it’s pretty similar. Again, you could choose <strong>Linear Learner and XGBoost.</strong> <!-- raw HTML omitted -->The difference is that you set the hyperparameters to direct these algorithms to produce quantitative results.<!-- raw HTML omitted --></p>
<p><strong>There are Amazon SageMaker built-in algorithms for natural language processing:</strong></p>
<ul>
<li><strong>BlazingText</strong> algorithm provides highly optimized implementations of the <strong>Word2vec</strong> and text classification algorithms.</li>
<li><strong>Sequence2sequence</strong> is a <strong>supervised</strong> learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens.</li>
<li><strong>Object2Vec</strong> generalizes the well-known Word2Vec embedding technique for words that are optimized in the Amazon SageMaker BlazingText algorithm.</li>
</ul>
<p><strong>There are Amazon SageMaker built-in algorithms for computer vision:</strong></p>
<ul>
<li>Image classification is a supervised learning algorithm used to classify images.</li>
<li>Object detection algorithm detects and classifies objects in images using a single deep neural network. It is a supervised learning algorithm that takes images as input and identifies all instances of objects within the image scene. The object is categorized into one of the classes in a specified collection with a confidence score that it belongs to the class. Its location and scale in the image are indicated by a rectangular bounding box.</li>
<li>Semantic segmentation algorithm tags every pixel in an image with a class label from a predefined set of classes.</li>
</ul>
<h4 id="other-options-for-training-algorithms"><strong>Other options for training algorithms</strong></h4>
<ul>
<li>Use Apache Spark with Amazon SageMaker</li>
<li>Submit custom code to train a model with a deep learning framework like TensorFlow or Apache MXNet</li>
<li>Use your own custom algorithm and put the code together as a Docker image</li>
<li>Subscribe to an algorithm from AWS Marketplace</li>
</ul>
<h3 id="33-train-ml-models"><strong>3.3: Train ML models</strong></h3>
<p>Cross Validation</p>
<p>Sage Maker</p>
<h3 id="34-perform-hyperparameter-optimization"><strong>3.4: Perform hyperparameter optimization</strong></h3>
<h4 id="what-are-hyperparameters"><strong>What are hyperparameters?</strong></h4>
<p>Hyperparameters are the knobs or settings that can be tuned before running a training job to control the behavior of an ML algorithm. They can have a big impact on model training as it relates to training time, model convergence, and model accuracy. Unlike model parameters that are derived from the training job, the values of hyperparameters do not change during the training.</p>
<h4 id="there-are-different-categories-of-hyperparameters"><strong>There are different categories of hyperparameters</strong></h4>
<h5 id="model-hyperparameters">Model hyperparameters</h5>
<p>Model hyperparameters define the model itself—Attributes of a neural network architecture like filter size, pooling, stride, padding</p>
<h5 id="optimizer-hyperparameters">Optimizer hyperparameters</h5>
<p>Optimizer hyperparameters, are related to how the model learn the patterns based on data and are used for a neural network model. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent, or even optimizers using momentum like Adam or initializing the parameter weights using methods like Xavier initialization or He initialization</p>
<h5 id="data-hyperparameters">Data hyperparameters</h5>
<p>Data hyperparameters are related to the attributes of the data, often used when you don’t have enough data or enough variation in data—Data augmentation techniques like cropping, resizing</p>
<h4 id="tuning-hyperparameters-can-be-very-labor-intensive"><strong>Tuning hyperparameters can be very labor-intensive</strong></h4>
<p>Traditionally, this was done <strong>manually</strong>: someone who has domain experience related to that hyperparameter and the use case would manually select the hyperparameters based on their intuition and experience. Then they would train the model and score it on the validation data. This process would be repeated over and over again until satisfactory results are achieved.</p>
<p><strong>A better way  is to use search methods to tune hyperparameters</strong>:</p>
<p><strong>Grid search and Random search</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124232631210.png?raw=true" alt="image-20210124232631210"></p>
<ul>
<li>Grid Search: you can set up a grid made of hyperparams and their different values. For each possible combination, a model is trained and a score is produced on the validation data. With this approach, every single combination of the given possible hyperparams values is tried. Therefore this method could be very inefficient.</li>
<li>Random Search: similar to grid search, but random combinations are selected. You can set the number of search iterations based on time and resource constrains.</li>
</ul>
<h4 id="aws-solution-sagemaker">AWS Solution: SageMaker</h4>
<p>Then there’s automated hyperparameter tuning, which uses methods like gradient descent, Bayesian optimization, and evolutionary algorithms to conduct a guided search for the best hyperparameter settings.</p>
<h3 id="35-evaluate-ml-models"><strong>3.5: Evaluate ML models</strong></h3>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<h3 id="metrics-for-classification-problems-">**Metrics for classification problems **</h3>
<p><strong>Accuracy is the ratio of correct predictions to total number of predictions</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124235038069.png?raw=true" alt="image-20210124235038069"></p>
<p><strong>Precision is the proportion of positive predictions that are actually correct</strong><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124235458004.png?raw=true" alt="image-20210124235458004"></p>
<p><strong>Recall is the proportion of correct sets that are identified as positive</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124235541375.png?raw=true" alt="image-20210124235541375"></p>
<p><strong>Additional metrics include:</strong></p>
<ul>
<li>F1 score:</li>
</ul>
<p>$$
(2<em>Precision</em>Recall)/(Precision+Recall)
$$</p>
<ul>
<li>AUC(Area under the Curve) - Receiver Operator Curve(ROC)</li>
</ul>
<h3 id="sample-questions-2">Sample Questions</h3>
<ol>
<li>
<p>An oil and natural gas company is using machine learning to discover prime locations for drilling. The company has <strong>chosen Amazon SageMaker as its service</strong> for creating machine learning models. The company’s data scientists are <strong>using notebook instances to develop those models</strong>. However, the data scientists spend a long time waiting for the training jobs to complete.</p>
<p>The company wants <strong>improve this idle time to more effectively iterate on the models</strong> with <strong>minimal</strong> change to the code to enable data scientists to <strong>quickly</strong> experiment with their models without having to wait for the training job to load the data and train the model.</p>
<p>What should the team of data scientists do to solve this issue?</p>
<ul>
<li>Use Amazon SageMaker in-built algorithms. ❌</li>
<li>Use Amazon SageMaker Estimators in local mode to train the models. ✅</li>
<li>Change the training job to use Pipe Mode to improve the time it takes to train the model. ❌</li>
<li>Create the models on local laptops. Then, port the code over to use Amazon SageMaker. ❌</li>
</ul>
<p><strong>Notes</strong>:</p>
<p>A &amp; C are for training algos, they can&rsquo;t help DS to quickly experiment the models.</p>
<p>B is to avoid the loading time for data.</p>
</li>
<li>
<p>A data scientist trained an XGBoost model to classify internal documents for further inquiry, and now wants to evaluate the <strong>model’s</strong> <strong>performance</strong> by looking at the results <strong>visually</strong>.</p>
<p>What technique should the data scientist use in this situation?</p>
<ul>
<li>Scatterplot to visualize the predicted labels versus the true label. ❌</li>
<li>Correlation matrix to visualize the predicted labels versus the true label. ❌</li>
<li>Confusion matrix to visualize the predicted labels ✅</li>
<li>Box plot to visualize the predicted labels (X axis) versus the true labels (Y axis). ❌</li>
</ul>
</li>
<li>
<p>A manufacturing company wants to increase the longevity of its factory machines by predicting when a machine part is about to stop working, jeopardizing the health of the machine. The company’s team of Data Scientists will build an ML model to accomplish this goal. The model will be trained on data made up of consumption metrics from similar factory machines, and will span a time frame from one hour before a machine part broke down to five minutes after the part degraded.</p>
<p>What kind of machine learning algorithm should the company use to build this model?</p>
<ul>
<li>Convolutional neural network (CNN) ❌</li>
<li>Amazon SageMaker DeepAR ✅</li>
<li>Scikit Learn Random Forest ❌</li>
<li>SciKit Learn Regression ❌</li>
</ul>
</li>
<li>
<p>A Data Scientist working for an autonomous vehicle company is building an ML model to detect and label people and various objects (for instance, cars and traffic signs) that may be encountered on a street. The Data Scientist has a dataset made up of labeled images, which will be used to train their machine learning model.</p>
<p>What kind of ML algorithm should be used?</p>
<ul>
<li>Instance segmentation ✅</li>
<li>Image localization ❌</li>
<li>Image classification ❌</li>
<li>Semantic segmentation ❌</li>
</ul>
</li>
<li>
<p>A Data Scientist is training a convolutional neural network model to detect incoming employees at the company’s front gate using a camera so that the system opens for them automatically. However, the model is taking too long to converge and the error oscillates for more than 10 epochs.</p>
<p>What should the Data Scientist do to improve upon this situation? (Select TWO.)</p>
<ul>
<li>Increase batch size ❌</li>
<li>Add more epochs ❌</li>
<li>Decrease weight decay ❌</li>
<li>Normalize the images before training ✅</li>
<li>Add batch normalization  ✅</li>
</ul>
</li>
</ol>
<h2 id="domain-4-ml-implementation-and-operations"><strong>Domain 4: ML Implementation and Operations</strong></h2>
<h3 id="41-build-ml-solutions-for-performance-availability-scalability-resiliency-and-fault-tolerance"><strong>4.1: Build ML solutions for performance, availability, scalability, resiliency, and fault tolerance</strong></h3>
<h4 id="high-availability-and-fault-tolerance"><strong>High availability and fault tolerance</strong></h4>
<p>At the heart of designing for failure are two concepts known as high availability and fault tolerance.</p>
<p>In a highly available solution, the system will continue to function even when any component of the architecture stops working. A key aspect of high availability is fault tolerance, which, when built into an architecture, ensures that applications will continue to function without degradation in performance, despite the complete failure of any component of the architecture.</p>
<p><!-- raw HTML omitted --></p>
<h4 id="one-method-of-achieving-high-availability-and-fault-tolerance-is-loose-coupling"><strong>One method of achieving high availability and fault tolerance is loose coupling</strong></h4>
<p>With a loosely coupled, distributed system, the failure of one component can be managed in between your application tiers so that the faults do not spread beyond that single point. Loose coupling is often <strong>achieved by making sure application components are independent of each other</strong>. For example, you should always decouple your storage layer with your compute layer because a training job only requires minimal time, but storing data is permanent. Decoupling helps turn off the compute resources when they are not needed.</p>
<p><strong>Loosely &amp; tightly coupled system:</strong></p>
<p>Tightly coupled:</p>
<ul>
<li>More interdependency</li>
<li>More coordination</li>
<li>More information</li>
</ul>
<p>⬇️</p>
<p>Loosely coupled:</p>
<ul>
<li>Less interdependency</li>
<li>Less coordination</li>
<li>Less information</li>
</ul>
<h4 id="queues-are-used-in-loose-coupling-to-pass-messages-between-components"><strong>Queues are used in loose coupling to pass messages between components</strong></h4>
<p>AWS SQS (queue service), AWS Step Function(workflow service)</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125160545179.png?raw=true" alt="image-20210125160545179"></p>
<h4 id="amazon-cloudwatch-helps-you-monitor-your-system"><strong>Amazon CloudWatch helps you monitor your system</strong></h4>
<h4 id="aws-cloudtrail-captures-api-calls-and-related-events"><strong>AWS CloudTrail captures API calls and related events</strong></h4>
<p>AWS CloudTrail captures API calls and related events made by or on behalf of your AWS account and delivers the log files to an Amazon S3 bucket that you specify. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p>
<h4 id="you-can-design-for-failure-of-any-individual-component-by-leveraging-key-aws-services-and-features-">**You can design for failure of any individual component by leveraging key AWS services and features **</h4>
<h5 id="aws-glue-and-emr">AWS GLUE and EMR</h5>
<p><!-- raw HTML omitted -->You should decouple your ETL process from the ML pipeline. The compute power needed for ML isn’t the same as what you’d need for an ETL process—they have very different requirements.<!-- raw HTML omitted --></p>
<ul>
<li>An ETL process needs to read in files from multiple formats, transform them as needed, and then write them back to a persistent storage. Keep in mind that reading and writing takes a lot of memory and disk I/O, so when you decouple your ETL process, <strong>use a framework like Apache Spark</strong>, which can handle large amounts of data easily for ETL.</li>
<li>Training, on the other hand, may require GPUs which are much more suited to handle the training requirements than CPUs. However, GPUs are less cost-effective to keep running when a model is not being trained. <strong>So you can make use of this decoupled architecture by simply using an ETL service like AWS Glue or Amazon EMR, which use Apache Spark for your ETL jobs and Amazon SageMaker to train, test, and deploy your models.</strong></li>
</ul>
<h5 id="ams-sagemaker-endpoints">AMS SageMaker Endpoints</h5>
<p>To ensure a highly available ML serving endpoint, deploy Amazon SageMaker endpoints backed by multiple instances across Availability Zones.</p>
<p><!-- raw HTML omitted --></p>
<h5 id="ams-sagemaker">AMS SageMaker</h5>
<p>Amazon SageMaker makes it easy to containerize ML models for both training and inference. In doing so, you can create ML models made up of loosely coupled, distributed services that can be placed on any number of platforms, or close to the data that the applications are analyzing.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125170039194.png?raw=true" alt="image-20210125170039194"></p>
<h5 id="aws-auto-scaling">AWS Auto Scaling</h5>
<p>Use AWS Auto Scaling <strong>to build scalable solutions by configuring automatic scaling</strong> for the AWS resources such as Amazon SageMaker endpoints that are part of your application in response to the changes in traffic to your application.</p>
<p>With AWS Auto Scaling, you configure and manage scaling for your resources through a scaling plan. The scaling plan uses dynamic scaling and predictive scaling to automatically scale your application’s resources.</p>
<p><strong>This ensures that you add the required computing power to handle the load</strong> on your application, and <strong>then remove it when it&rsquo;s no longer required</strong>. The scaling plan lets you choose scaling strategies to define how to optimize your resource utilization. You can optimize for availability, for cost, or a balance of both.</p>
<p>As you increase the number of concurrent prediction requests, at some point the endpoint responds more slowly and eventually errors out for some requests. Automatically scaling the endpoint avoids these problems and improves prediction throughput. When the endpoint scales out, Amazon SageMaker automatically spreads instances across multiple Availability Zones. This provides Availability Zone-level fault tolerance and protects from an individual instance failure.</p>
<p>If the endpoint has only a moderate load, you can run it on a single instance and still get good performance. Use automatic scaling to ensure high availability during traffic fluctuations without having to constantly provision for peak traffic. For production workloads, use at least two instances. Because Amazon SageMaker automatically spreads the endpoint instances across multiple Availability Zones, a minimum of two instances ensures high availability and provides individual fault tolerance.</p>
<p><strong>To determine the scaling policy for automatic scaling in Amazon SageMaker, test for how much load (RPS) the endpoint can sustain. Then configure automatic scaling and observe how the model behaves when it scales out. Expected behavior is lower latency and fewer or no errors with automatic scaling.</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125170232859.png?raw=true" alt="image-20210125170232859"></p>
<p>Remark:</p>
<h5 id="auto-scaling-group"><strong>Auto Scaling Group</strong></h5>
<p>What is ASG?</p>
<p>In real life, the load on your websites and app can change; in the cloud, you can create and get rid of servers very quickly.</p>
<p>ASG is used to:</p>
<ul>
<li>Scale out(which is add EC2 instances) to match an increased load.</li>
<li>Scale in (which is remove EC2 instances) to match a decreased load.</li>
<li>Ensure we have a min and max number of machines running</li>
<li>Automatically register new instances to a load balancer</li>
</ul>
<h3 id="42-recommend-and-implement-the-appropriate-ml-services-and-features-for-a-given-problem"><strong>4.2: Recommend and implement the appropriate ML services and features for a given problem</strong></h3>
<h4 id="the-stack-for-amazon-machine-learning-has-three-tiers"><strong>The stack for Amazon machine learning has three tiers</strong></h4>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125212450908.png?raw=true" alt="image-20210125212450908"></p>
<h5 id="ml-frameworks--infrastructure">ML frameworks + infrastructure</h5>
<p>The bottom tier of the stack is for expert ML practitioners who work at the framework level. To work with these frameworks, you are comfortable building, training, tuning, and deploying ML models on the metal, so to speak. ML frameworks are the foundation from which innovation in ML is designed. The focus here is on making it easier for you to connect more broadly to the AWS ecosystem, whether that’s about pulling in IoT data from AWS IOT Greengrass, accessing state-of-the art chips (P3), or leveraging elastic inference.</p>
<p><strong>The vast majority of deep learning and ML in the cloud is done on P3 instances in AWS</strong>. You can use whichever ML deep learning framework you like, but some popular options are TensorFlow, MXNet, and PyTorch, which are all supported on AWS.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125215339947.png?raw=true" alt="image-20210125215339947"></p>
<h5 id="ml-services">ML Services</h5>
<p>AWS SageMaker is the heart of this tier.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125220216005.png?raw=true" alt="image-20210125220216005"></p>
<h5 id="ai-services">AI Services</h5>
<p>AWS services in the top tier are for customers who really don&rsquo;t want to deal with building and training their ML models. All of that has been abstracted away, leaving you with easy-to-use services designed to help you deal with common ML problems in various domains, like computer vision, NLP, and time series forecasting.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125220258775.png?raw=true" alt="image-20210125220258775"></p>
<h5 id="amazon-ai-services">Amazon AI services</h5>
<ul>
<li>Amazon Translate</li>
<li>Amazon Lex</li>
<li>Amazon Polly</li>
<li>Amazon Transcribe</li>
<li>Amazon Rekognition</li>
<li>Amazon Comprehend</li>
</ul>
<h3 id="43-apply-basic-aws-security-practices-to-ml-solutions"><strong>4.3: Apply Basic AWS security practices to ML solutions</strong></h3>
<h4 id="below-is-a-summary-of-security-features-integrated-with-amazon-sagemaker"><strong>Below is a summary of security features integrated with Amazon SageMaker</strong></h4>
<ul>
<li>Authentication
<ul>
<li>IAM federation</li>
</ul>
</li>
<li>Gaining insight
<ul>
<li>Restrict access by IAM policy and condition keys</li>
</ul>
</li>
<li>Audit
<ul>
<li>API logs to AWS CloudTrail - exception of InvokeEndpoint</li>
</ul>
</li>
<li>Data Protection at rest
<ul>
<li>AWS KMS-based encryption for:
<ul>
<li>Notebooks</li>
<li>Training jobs</li>
<li>Amazon S3 location to store modelsEndpoint</li>
</ul>
</li>
</ul>
</li>
<li>Data Protection at motion
<ul>
<li>HTTPS for:
<ul>
<li>API/console</li>
<li>Notebooks</li>
<li>VPC-enabled</li>
<li>Interface endpoint</li>
<li>Limit by IPTraining jobs/endpoints</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="44-deploy-and-operationalize-ml-solutions"><strong>4.4: Deploy and operationalize ML solutions</strong></h3>
<h4 id="deploying-a-model-using-amazon-sagemaker-hosting-services-is-a-three-step-process"><strong>Deploying a model using Amazon SageMaker hosting services is a three-step process</strong></h4>
<h5 id="create-a-model-in-amazon-sagemaker">Create a model in Amazon SageMaker</h5>
<p>You need:</p>
<ul>
<li>The Amazon S3 path where the model artifacts are stored</li>
<li>The Docker registry path for the image that contains the inference code</li>
<li>A name that you can use for subsequent deployment steps</li>
</ul>
<h5 id="create-an-endpoint-configuration-for-an-https-endpoint">Create an endpoint configuration for an HTTPS endpoint</h5>
<p>You need:</p>
<ul>
<li>The name of one or more models in production variants</li>
<li>The ML compute instances that you want Amazon SageMaker to launch to host each production variant. When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, Amazon SageMaker launches them in multiple Availability Zones. This ensures continuous availability. Amazon SageMaker manages deploying the instances.</li>
</ul>
<h5 id="create-an-https-endpoint">Create an HTTPS endpoint</h5>
<p>You need to provide the endpoint configuration to Amazon SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration.</p>
<h4 id="you-may-want-to-pay-extra-attention-to-the-following-points-when-youre-delivering-an-ml-model-into-a-production-environment"><strong>You may want to pay extra attention to the following points when you’re delivering an ML model into a production environment:</strong></h4>
<ul>
<li>Apply software engineering disciplines. Add error recovery code and make sure that tests for unexpected data inputs exist. Perform the same kind of unit testing, quality assurance, and user acceptance testing that is performed for other systems. If the ML system has moved from the research stage to development, some of these expected software engineering practices might have been inconsistently applied. Automate this system using common DevOps tools like AWS CodeBuild and AWS CodeCommit.</li>
<li>Track, identify, and account for changes in data sources. The data might change over time. One change in data type in one source can break the whole pipeline. Changes in software that produces a data source can have flow-on effects.</li>
<li>Perform ongoing monitoring and evaluation of results. Evaluate the expectations versus the results of the ML system. Build methods to check the error rate and the classes of errors being made against project expectations. If the overall error rate is the same, are the same proportions of the different classes of errors still the same? Is model drift occurring?</li>
<li>Create methods to collect data from production inferences that can be used to improve future models.</li>
</ul>
<h4 id="amazon-sagemaker-supports-automatic-scaling-for-production-variants"><strong>Amazon SageMaker supports automatic scaling for production variants</strong></h4>
<p>Amazon SageMaker supports automatic scaling for production variants. Automatic scaling dynamically adjusts the number of instances provisioned for a production variant in response to changes in your workload. When the workload increases, automatic scaling brings more instances online. When the workload decreases, automatic scaling removes unnecessary instances so that you don&rsquo;t pay for provisioned variant instances that you aren&rsquo;t using.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210126003729710.png?raw=true" alt="image-20210126003729710"></p>
<h4 id="define-and-apply-a-scaling-policy-that-uses-amazon-cloudwatch-metrics"><strong>Define and apply a scaling policy that uses Amazon CloudWatch metrics</strong></h4>
<ul>
<li>Automatic scaling uses the policy to adjust the number of instances up or down in response to actual workloads</li>
<li>You can use the AWS Management Console to apply a scaling policy based on a predefined metric</li>
<li>A predefined metric is defined in an enumeration so you can specify it by name in code or use it in the console</li>
<li>Always <strong>load-test</strong> your automatic scaling configuration to ensure that it works correctly before using it to manage production traffic</li>
</ul>
<h3 id="sample-questions-3">Sample Questions:</h3>
<ol>
<li>
<p>A sports and betting company uses machine learning to predict the odds of winning during sporting events. It uses the Amazon SageMaker endpoint to serve its production model. The endpoint is on an m5.8xlarge instance.</p>
<p>What can the company do to ensure that this endpoint is highly available, while using the most cost-effective and easily managed solution?</p>
<ul>
<li>Create another endpoint. Put the two endpoints behind an Application Load Balancer. ❌</li>
<li>Increase the number of instances associated with the endpoint to more than one. ✅</li>
<li>Increase the instance size to m5.16 x-large. ❌</li>
<li>Add an elastic inference to the endpoint. ❌</li>
</ul>
</li>
<li>
<p>A healthcare company wants to deploy an ensemble of models behind a single endpoint with minimal management. The models include an XGBoost model trained on one of its structured datasets and a CNN model trained on an image dataset.</p>
<p>Which solution can the company use to reach this objective?</p>
<ul>
<li>
<p>Create an AWS Lambda function with Amazon API Gateway that preprocesses the incoming data. The function then creates the prediction from all the ensemble models. And finally, it returns the prediction. ✅</p>
</li>
<li>
<p>Create an AWS Deep Learning container that preprocesses the incoming data. The container then creates the prediction from all the ensemble models. And finally, it returns the prediction. ❌</p>
</li>
<li>
<p>Create an Amazon EC2 instance with the AWS Deep Learning AMI that preprocesses the incoming data. The instance then creates the prediction from all the ensemble models. And finally, it returns the prediction. ❌</p>
</li>
<li>
<p>Create an Amazon SageMaker endpoint that preprocesses the incoming data. The endpoint then creates the prediction from all the ensemble models. And finally, it returns the prediction. ❌</p>
</li>
</ul>
</li>
<li>
<p>A Machine Learning Engineer created a pipeline for training an ML model using an Amazon SageMaker training job. The training job began successfully, but then failed after running for five minutes.</p>
<p>How should the Engineer begin to debug this issue? (Select TWO.)</p>
<ul>
<li>Check AWS CloudTrail logs to check the error that caused the training to fail</li>
<li>Log into the Amazon SageMaker training job instance and check the job history</li>
<li>Go to Amazon CloudWatch logs and check the logs for the given training job ✅</li>
<li>Check the error in the given training job directly in the Amazon SageMaker console</li>
<li>Call the <strong>DescribeJob API</strong> to check the FailureReason option✅</li>
</ul>
</li>
<li>
<p>A news organization wants to extract metadata from its articles and blogs and index that metadata in Amazon Elasticsearch Service (Amazon ES) to enable faster searches.</p>
<p>What AWS service can the organization use to achieve this goal?</p>
<ul>
<li>Amazon Textract</li>
<li>Amazon Personalize</li>
<li>Amazon Rekognition Image</li>
<li>Amazon Comprehend ✅</li>
</ul>
</li>
<li>
<p>A news organization wants to extract metadata from its articles and blogs and index that metadata in Amazon Elasticsearch Service (Amazon ES) to enable faster searches.</p>
<p>What AWS service can the organization use to achieve this goal?</p>
<ul>
<li>Amazon Textract</li>
<li>Amazon Personalize</li>
<li>Amazon Rekognition Image</li>
<li>Amazon Comprehend ✅</li>
</ul>
</li>
<li>
<p>A machine translation company is deploying its language translation models behind an Amazon SageMaker endpoint. The company wants to deploy a solution directly on its website so that users can input text in one language and have it translated into a second language. The company wants to reach a solution with minimal maintenance and latency for spiky traffic times.</p>
<p>How should the company architect this solution?</p>
<ul>
<li>Create a function on an Amazon EC2 instance that uses CURL to call the InvokeEndpoint API. Call the Amazon EC2 instance from the website.</li>
<li>Use Lambda to call InvokeEndpoint. Use the Amazon API Gateway URL to call the AWS Lambda function. ✅</li>
<li>Use Amazon SageMaker InvokeEndpoint with API Gateway</li>
<li>Install the sagemaker-runtime library on the web server. Call InvokeEndpoint from the webserver.</li>
</ul>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Intro to Data Analytics Fundamental with AWS Solutions</title>
            <link>https://ffflora.cat/posts/2021/01/intro-to-data-analytics-fundamental-with-aws-solutions/</link>
            <pubDate>Thu, 14 Jan 2021 00:19:31 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/01/intro-to-data-analytics-fundamental-with-aws-solutions/</guid>
            <description>The challenges identified in many data analysis solutions can be summarized by five key challenges: volume, velocity, variety, veracity, and value.
Volume - Data Storage  Structured(10%) data is organized and stored in the form of values that are grouped into rows and columns of a table. Semistructured(10%) data is often stored in a series of key-value pairs that are grouped into elements within a file. Unstructured(80%) data is not structured in a consistent way.</description>
            <content type="html"><![CDATA[<p>The challenges identified in many data analysis solutions can be summarized by five key challenges: <strong>volume</strong>, <strong>velocity</strong>, <strong>variety</strong>, <strong>veracity</strong>, and <strong>value</strong>.</p>
<h2 id="volume---data-storage">Volume - Data Storage</h2>
<ul>
<li><strong>Structured</strong>(10%) data is organized and stored in the form of values that are grouped into rows and columns of a table.</li>
<li><strong>Semistructured(10%)</strong> data is often stored in a series of key-value pairs that are grouped into elements within a file.</li>
<li><strong>Unstructured</strong>(80%) data is not structured in a consistent way. Some data may have structure similar to semi-structured data but others may only contain metadata.</li>
</ul>
<p>When businesses have <strong>more data</strong> than they are able to <strong>process and analyze</strong>, they have a <strong>volume problem</strong>.</p>
<h3 id="amazon-s3">Amazon S3</h3>
<p>Advantages of using S3:</p>
<ul>
<li>Decoupling storage from processing</li>
<li>Parallelization, running process in parallel</li>
<li>Centralized location (reduce latency)</li>
</ul>
<h4 id="aws-s3-concepts">AWS S3 concepts</h4>
<p>First, Amazon S3 stores data as <strong>objects</strong> within <strong>buckets</strong>.</p>
<p>An <strong>object</strong> is composed of a file and any metadata that describes that file. To store an object in Amazon S3, you upload the file you want to store into a bucket. When you upload a file, you can set permissions on the object and add any metadata.</p>
<p><strong>Buckets</strong> are logical containers for objects. You can have one or more buckets in your account and can control access for each bucket individually. You control who can create, delete, and list objects in the bucket. You can also view access logs for the bucket and its objects and choose the geographical region where Amazon S3 will store the bucket and its contents.</p>
<h4 id="accessing-your-content"><strong>Accessing your content</strong></h4>
<p>Once objects have been stored in an Amazon S3 bucket, they are given an <strong>object key</strong>. Use this, along with the bucket name, to access the object.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201028180458759.png?raw=true" alt="image-20201028180458759"></p>
<p>An <strong>object key</strong> is the unique identifier for an object in a bucket. Because the combination of a bucket, key, and version ID uniquely identifies each object, you can think of Amazon S3 as a basic data map between &ldquo;bucket + key + version&rdquo; and the object itself. Every object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and (optionally) version.</p>
<h3 id="aws-data-lakes">AWS Data Lakes</h3>
<p>A data lake is a <strong>centralized repository</strong> that allows you to store <strong>structured</strong>, <strong>semistructured</strong>, and <strong>unstructured</strong> data at any scale.</p>
<h4 id="some-cautions-of-data-lakes">Some <strong>cautions</strong> of data lakes:</h4>
<ul>
<li>Single source of truth:	Be careful not to let your data lake become a swamp. Enforce proper organization and structure for all data entering the lake.</li>
<li>Store any type of data, regardless of structure: Be careful to ensure that data within the data lake is relevant and does not go unused. Train users on how to access the data, and set retention policies to ensure the data stays refreshed.</li>
<li>Can be analyzed using AI and machine learning: Be careful to learn how to use data in new ways. Don&rsquo;t limit analytics to typical data warehouse-style analytics. AI and machine learning offer significant insights.</li>
</ul>
<h4 id="benefits-of-a-data-lake-on-aws">Benefits of a data lake on AWS</h4>
<ul>
<li>Are a <strong>cost-effective data storage</strong> solution. You can durably store a nearly unlimited amount of data using Amazon S3.</li>
<li>Implement industry-leading <strong>security and compliance</strong>. AWS uses stringent data security, compliance, privacy, and protection mechanisms.</li>
<li>Allow you to take advantage of <strong>many different data collection and ingestion tools</strong> to ingest data into your data lake. These services include Amazon Kinesis for streaming data and AWS Snowball appliances for large volumes of on-premises data.</li>
<li>Help you to <strong>categorize and manage your data</strong> simply and efficiently. Use AWS Glue to understand the data within your data lake, prepare it, and load it reliably into data stores. Once AWS Glue catalogs your data, it is immediately searchable, can be queried, and is available for ETL processing.</li>
<li>Help you turn data into <strong>meaningful insights</strong>. Harness the power of purpose-built analytic services for a wide range of use cases, such as interactive analysis, data processing using Apache Spark and Apache Hadoop, data warehousing, real-time analytics, operational analytics, dashboards, and visualizations.</li>
</ul>
<h4 id="aws-lake-formation">AWS Lake Formation</h4>
<p>AWS Lake Formation makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and when prepared for analysis. A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions. AWS Lake Formation is in preview only.</p>
<p>AWS Lake Formation is a service that organizes and curates data within Amazon S3 data lakes. This service ensures the security and compliance of items within the lake as well as orchestrates transformation jobs utilizing the data lake and other AWS services.</p>
<h3 id="data-warehouse">Data Warehouse</h3>
<p>structured data.</p>
<p>Fast, centralized data retrieval.</p>
<p>Data lakes and daka warehouses are two different storage systems. Data lakes are not  a replacement for data warehouses.</p>
<p>A data warehouse is a <strong>central repository</strong> of <strong>structured</strong> data from <strong>many</strong> data sources. This data is <strong>transformed</strong>, <strong>aggregated</strong>, and <strong>prepared</strong> for business reporting and analysis.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030210016708.png?raw=true" alt="image-20201030210016708"></p>
<p>A data warehouse is a central repository of information coming from one or more data sources. Data flows into a data warehouse from transactional systems, relational databases, and other sources. These data sources can include structured, semistructured, and unstructured data. These data sources are transformed into structured data before they are stored in the data warehouse.</p>
<p>Data is stored within the data warehouse using a schema. A schema defines how data is stored within tables, columns, and rows. The schema enforces constraints on the data to ensure integrity of the data. The transformation process often involves the steps required to make the source data conform to the schema. Following the first successful ingestion of data into the data warehouse, the process of ingesting and transforming the data can continue at a regular cadence.</p>
<p>Business analysts, data scientists, and decision makers access the data through business intelligence (BI) tools, SQL clients, and other analytics applications. Businesses use reports, dashboards, and analytics tools to extract insights from their data, monitor business performance, and support decision making. These reports, dashboards, and analytics tools are powered by data warehouses, which store data efficiently to minimize I/O and deliver query results at blazing speeds to hundreds and thousands of users concurrently.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030233712765.png?raw=true" alt="image-20201030233712765"></p>
<h3 id="data-marts">Data Marts</h3>
<p>A subset of a data warehouse.</p>
<p>A subset of data from a data warehouse is called a <strong>data mart</strong>. Data marts only <strong>focus on one subject or functional area</strong>. A warehouse might contain all relevant sources for an enterprise, but a data mart might store <strong>only a single department’s sources</strong>. Because data marts are generally a copy of data already contained in a data warehouse, they are often <strong>fast and simple to implement.</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030231123696.png?raw=true" alt="image-20201030231123696"></p>
<h3 id="amazon-redshift-spectrum">Amazon Redshift Spectrum</h3>
<p>Amazon Redshift overcomes all of data lake&rsquo;s negatives by providing a <strong>cloud-based, scalable, secure environment</strong> for your data warehouse. Amazon Redshift is easy to set up, deploy, and manage and provides up to 10 times faster performance than other data warehousing solutions.</p>
<table>
<thead>
<tr>
<th align="center"><strong>Benefits of Amazon Redshift</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><strong>Faster performance</strong>  10x faster than other data warehouses</td>
</tr>
<tr>
<td align="center"><strong>Easy to set up</strong>, deploy, and manage</td>
</tr>
<tr>
<td align="center"><strong>Secure</strong></td>
</tr>
<tr>
<td align="center"><strong>Scales quickly</strong> to meet your needs</td>
</tr>
</tbody>
</table>
<h4 id="data-warehouse-vs-data-lakes"><strong>Data Warehouse vs Data Lakes</strong></h4>
<p>For analysis to be most effective, it should be performed on data that has been processed and cleansed. This often means implementing an ETL operation to collect, cleanse, and transform the data. This data is then placed in a data warehouse. It is very common for data from many different parts of the organization to be combined into a single data warehouse.</p>
<p>Amazon Redshift is a data warehousing solution specially designed for workloads of all sizes. Amazon Redshift Spectrum even provides the ability to query data that is housed in an Amazon S3 data lake.<img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030234922475.png?raw=true" alt="image-20201030234922475"></p>
<p><strong>Data lakes extend data warehouses</strong></p>
<p>Data lakes provide customers a means for including unstructured and semistructured data in their analytics. Analytic queries can be run over cataloged data within a data lake. This extends the reach of analytics beyond the confines of a single data warehouse.</p>
<p>Businesses can securely store data coming from applications and devices in its native format, with high availability, durability, at low cost, and at any scale. Businesses can easily access and analyze data in a variety of ways using the tools and frameworks of their choice in a high-performance, cost-effective way without having to move large amounts of data between storage and analytics systems.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030235045512.png?raw=true" alt="image-20201030235045512"></p>
<table>
<thead>
<tr>
<th>Characteristics</th>
<th>Data Warehouse</th>
<th>Data Lakes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data</strong></td>
<td>Relational from transactional systems, operational databases, and line of business applications</td>
<td>Non-relational and relational from IoT devices, websites, mobile apps, social media, and corporate applications</td>
</tr>
<tr>
<td><strong>Schema</strong></td>
<td>Designed prior to implementation (schema-on-write)</td>
<td>Written at the time of analysis  (schema-on-read)</td>
</tr>
<tr>
<td><strong>Price/ performance</strong></td>
<td>Fastest query results using higher cost storage</td>
<td>Query results getting faster using  low-cost storage</td>
</tr>
<tr>
<td><strong>Data quality</strong></td>
<td>Highly curated data that serves as the central version of the truth</td>
<td>Any data, which may or may not be curated (e.g., raw data)</td>
</tr>
<tr>
<td><strong>Users</strong></td>
<td>Business analysts</td>
<td>Data scientists, data developers, and business analysts (using curated data)</td>
</tr>
<tr>
<td><strong>Analytics</strong></td>
<td>Batch reporting, BI, and visualizations</td>
<td>Machine learning, predictive analytics, data discovery, and profiling.</td>
</tr>
</tbody>
</table>
<h3 id="data-storage-on-a-big-scale"><strong>Data storage on a BIG scale</strong></h3>
<p>We have discussed several recommendations for storing data:</p>
<ul>
<li>When storing <strong>individual objects or files</strong>, we recommend Amazon <strong>S3</strong>.</li>
<li>When storing <strong>massive volumes of data, both semistructured and unstructured</strong>, we recommend <strong>building a data lake on Amazon S3</strong>.</li>
<li>When storing massive amounts of <strong>structured data for complex analysis, we recommend storing your data in Amazon Redshift.</strong></li>
</ul>
<h3 id="apache-hadoop"><strong>Apache Hadoop</strong></h3>
<p>When many people think of working with a massive volume of fast-moving data, the first thing that comes to mind is Hadoop. Within AWS, Hadoop frameworks are implemented using Amazon <strong>EMR</strong> and AWS G<strong>l</strong>ue. These services implement the Hadoop framework to <strong>ingest, transform, analyze, and move results to analytical data stores.</strong></p>
<p>Hadoop uses a <strong>distributed processing architecture</strong>, in which a task is mapped to a cluster of commodity servers for processing. Each piece of work distributed to the cluster servers can be run or re-run on any of the servers. The cluster servers frequently use the <strong>Hadoop Distributed File System (HDFS)</strong> to store data locally for processing. The results of the computation performed by those servers are then reduced to a single output set. One node, designated as the master node, controls the distribution of tasks and can automatically handle server failures.</p>
<h4 id="benefits-of-using-apache-hadoop"><strong>Benefits of using Apache Hadoop</strong></h4>
<ul>
<li>
<p><strong>Handle uncertainty better</strong>: Hadoop facilitates data navigation, discovery, and one-time data analysis. With Hadoop, you can compensate for unexpected occurrences by analyzing large amounts of data quickly to form a response.</p>
</li>
<li>
<p><strong>Manage Data Variety</strong>: Hadoop can process structured, semistructured, or unstructured data. This includes virtually any data format currently available.</p>
<p>In addition to natively handling many types of data (such as XML, CSV, text, log files, objects, SQL, JSON, and binary), you can use Haddop to transform data into formats that allow better integration into your existing data sets. Also, you can store data with or without a schema and perform large-scale ETL operations to transform your data.</p>
</li>
<li>
<p><strong>Wide Selection of Solutions</strong>: Because Hadoop is open source, several ecosystem projects are available to help you analyze the multiple types of data Hadoop can process and analyze.</p>
<p>These projects give you tremendous flexibility when you are developing data analytics solutions. Hadoop’s programming frameworks (such as Hive and Pig) can support almost any data analytics use case for your applications.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031000543210.png?raw=true" alt="image-20201031000543210"></p>
</li>
<li>
<p><strong>Build for volume and velocity</strong>: Because of Hadoop’s distributed architecture, Hadoop clusters can handle tremendous amounts of data affordably. Adding additional data processing capability is as simple as adding additional servers to your cluster (horizontal scaling).</p>
</li>
</ul>
<h4 id="implementing-hadoop-with-amazon-emr"><strong>Implementing Hadoop with Amazon EMR</strong></h4>
<p>Amazon EMR is the AWS service that implements Hadoop frameworks. The service will ingest data from nearly any data source type at nearly any speed! Amazon EMR has the ability to implement two different file systems: <strong>HDFS</strong> or the <strong>Elastic MapReduce File System (EMRFS)</strong>. A file system is a set of organizational rules that govern how files are stored.</p>
<h5 id="hdfs"><strong>HDFS</strong></h5>
<p>To handle massive volumes of data rapidly, the processing system required a way to distribute the load of reading and writing files across tens or even hundreds of high-powered servers. HDFS is distributed storage allowing files to be read and written to clusters of servers in <strong>parallel</strong>. This dramatically reduces the overall length of each and every operation.</p>
<p>It is helpful to understand the inner workings of an HDFS cluster. An HDFS cluster primarily consists of a <em><strong>NameNode</strong></em>, <strong>which manages the file system metadata, and <em>DataNodes</em>, which store the actual data.</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001440898.png?raw=true" alt="image-20201031001440898"><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001458920.png?raw=true" alt="image-20201031001458920"><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001516185.png?raw=true" alt="image-20201031001516185"></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001546662.png?raw=true" alt="image-20201031001546662"><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001632817.png?raw=true" alt=""></p>
<p>Amazon EMR is the AWS service that implements Hadoop frameworks. An Amazon EMR process begins by ingesting data from one or more data sources and storing that data within a file system. If using HDFS, the file system is stored as an elastic block store volume. This storage volume is ephemeral meaning that the storage is of a temporary nature. Once the data has been copied into the HDFS volume, the transformation and analysis of the data is performed. The results are then sent to an analytical data store, such as an Amazon S3 data lake or Amazon Redshift data warehouse.</p>
<h5 id="amazon-emrfs"><strong>Amazon EMRFS</strong></h5>
<p>Amazon EMR provides an alternative to HDFS: the EMR File System (EMRFS). EMRFS can help ensure that there is a persistent &ldquo;source of truth&rdquo; for HDFS data stored in Amazon S3. When implementing EMRFS, there is no need to copy data into the cluster before transforming and analyzing the data as with HDFS. EMRFS can catalog data within a data lake on Amazon S3. The time that is saved by eliminating the copy step can dramatically improve performance of the cluster.</p>
<h2 id="velocity---data-processing">Velocity - Data Processing</h2>
<ol>
<li>Batch processing
<ol>
<li>scheduled batch processing</li>
<li>Periodic batch processing</li>
</ol>
</li>
<li>Streaming Processing
<ol>
<li>Near-real-time</li>
<li>Real-time</li>
</ol>
</li>
</ol>
<h3 id="characteristics-of-data-processing-velocity"><strong>Characteristics of data processing velocity</strong></h3>
<p><strong>Velocities on collecting data</strong></p>
<table>
<thead>
<tr>
<th>Data Processing</th>
<th>Velocities on Collecting Data</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch</strong>:</td>
<td>Velocity is very predictable with batch processing. It amounts to large bursts of data transfer at scheduled intervals.</td>
</tr>
<tr>
<td><strong>Periodic</strong>:</td>
<td>Velocity is less predictable with periodic processing. The loss of scheduled events can put a strain on systems and must be considered.</td>
</tr>
<tr>
<td><strong>Near real-time</strong>:</td>
<td>Velocity is a huge concern with near real-time processing. These systems require data to be processed within minutes of the initial collection of the data. This can put tremendous strain on the processing and analytics systems involved.</td>
</tr>
<tr>
<td><strong>Real-time:</strong></td>
<td>Velocity is the paramount concern for real-time processing systems. Information cannot take minutes to process. It must be processed in <strong>seconds</strong> to be valid and maintain its usefulness.</td>
</tr>
</tbody>
</table>
<p><strong>Velocities on processing data</strong></p>
<table>
<thead>
<tr>
<th>Data Processing</th>
<th>Velocities on Processing Data</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch and periodic</strong>:</td>
<td>Once the data has been collected, processing can be done in a controlled environment. There is time to plan for the appropriate resources.</td>
</tr>
<tr>
<td><strong>Near real-time and real-time</strong>:</td>
<td>Collection of the data leads to an immediate need for processing. Depending on the complexity of the processing (cleansing, scrubbing, curation), this can slow down the velocity of the solution significantly. Plan accordingly.</td>
</tr>
</tbody>
</table>
<h3 id="attributes-of-batch-and-stream-processing"><em><strong>*Attributes of batch and stream processing*</strong></em></h3>
<p>The table below highlights the difference between batch and stream processing:</p>
<p><img src="https://raw.githubusercontent.com/ffflora/data-science-notes/master/archived-pics/aws/image-20201015002429812.png" alt="image-20201015002429812"></p>
<h3 id="business-challenge">Business Challenge</h3>
<p>The slower collection of data followed by a rapid processing requirement is a common challenge.</p>
<p>The rapid collection of data followed by the rapid processing of data is a common signature of streaming analytics.</p>
<h3 id="batch-processing-architecture"><strong>Batch processing architecture</strong></h3>
<p>Batch processing can be performed in different ways using AWS services. i.e.,</p>
<p><strong>S3, Lambda, EMR, Glue, Redshift.</strong></p>
<p>Amazon <strong>EMR</strong> provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances.</p>
<p>Amazon <strong>EMR</strong> is a managed service for executing highly complex, massive batch workloads. This service also allows highly complex analytic operations.</p>
<p>AWS <strong>Glue</strong> is a fully managed extract, transform, and load (ETL) service that makes it easy for you to prepare and load your data for analytics.</p>
<p>Amazon <strong>Redshift</strong> is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake.</p>
<p>Amazon <strong>Redshift</strong> is a managed data warehouse service that stores large amounts of transactional data for the purpose of analytics.</p>
<p>AWS <strong>Lambda</strong> is a serverless compute service that can be used to trigger processing operations in a batch processing system.</p>
<h3 id="stream-data-processing">Stream Data Processing</h3>
<h4 id="aws-knesis">AWS Knesis</h4>
<p><strong>Amazon Kinesis Data Firehose</strong> is the easiest way to capture, transform, and load data streams into AWS data stores for near real-time analytics with existing business intelligence tools.</p>
<p><strong>Amazon Kinesis Data Analytics</strong> is the easiest way to process data streams in real time with SQL or Java without having to learn new programming languages or processing frameworks.</p>
<p><strong>&hellip;</strong></p>
<h5 id="other-stream-processing-architecture">Other stream processing architecture:</h5>
<p><strong>Amazon Athena</strong> is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>
<p><strong>Amazon QuickSight</strong> is a fast, cloud-powered business intelligence (BI) service that makes it easy for you to deliver insights to everyone in your organization.</p>
<h2 id="variety--data-structure-and-types">Variety – data structure and types</h2>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210109152122711.png?raw=true" alt="image-20210109152122711"></p>
<p>Surprisingly, the most common data types auch as <code>csv</code> and <code>JSON</code> are semistructured data.</p>
<h3 id="intro-to-structured-data-stores">Intro to Structured Data Stores</h3>
<h4 id="flat-file-data">Flat-file Data</h4>
<p>Flat-file data generally resides in a worksheet or spreadsheet.</p>
<h4 id="relational-databases"><strong>Relational databases</strong></h4>
<p>A process known as normalization helps a business take flat-file data and turn it into a relational database. Normalization is a set of rules that work together to reduce redundancy, increase reliability, and improve the consistency of data storage.</p>
<h4 id="types-of-information-systems"><strong>Types of information systems</strong></h4>
<p>There are two main ways—known as information systems—of organizing data within a relational database. The data can be organized to focus on the storage of transactions or the process of analyzing transactions.</p>
<p>Transactional databases are called online transaction processing (OLTP) databases. The data gathered by OLTP databases is often fed into another type of database that focuses on analyzing the transactional data. Online analytical processing (OLAP) databases gather data from OLTP systems for the purpose of organizing it for analytical operations.</p>
<h4 id="comparing-oltp-and-olap"><em><strong>*Comparing OLTP and OLAP*</strong></em></h4>
<table>
<thead>
<tr>
<th><strong>Characteristic</strong></th>
<th><strong>OLTP</strong></th>
<th><strong>OLAP</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nature</strong></td>
<td>Constant transactions (queries/updates)</td>
<td>Periodic large updates, complex queries</td>
</tr>
<tr>
<td><strong>Examples</strong></td>
<td>Accounting database, online retail transactions</td>
<td>Reporting, decision support</td>
</tr>
<tr>
<td><strong>Type</strong></td>
<td>Operational data</td>
<td>Consolidated data</td>
</tr>
<tr>
<td><strong>Data retention</strong></td>
<td>Short-term (2-6 months)</td>
<td>Long-term (2-5 years)</td>
</tr>
<tr>
<td><strong>Storage</strong></td>
<td>Gigabytes (GB)</td>
<td>Terabytes (TB)/petabytes (PB)</td>
</tr>
<tr>
<td><strong>Users</strong></td>
<td>Many</td>
<td>Few</td>
</tr>
<tr>
<td><strong>Protection</strong></td>
<td>Robust, constant data protection and fault tolerance</td>
<td>Periodic protection</td>
</tr>
</tbody>
</table>
<p>In an <strong>OLTP system</strong>, the most common queries are called lookup queries. These queries need to return several columns of data for each matching record. The filters on this data are generally based on the key columns in that table. In this type of system, you might query to get details for a specific order.</p>
<p>In an <strong>OLAP system</strong>, the most common queries are aggregate queries. These queries take large numbers of rows and reduce them to a single row by aggregating the values in one or more columns. In this type of system, you might query to find out the total number of items sold on a specific date.</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Row-based indexes</strong></th>
<th><strong>Columnar indexes</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Storage on disk</strong></td>
<td>Row by row</td>
<td>Column by column</td>
</tr>
<tr>
<td><strong>Read/write</strong></td>
<td>Best at random reads and writes</td>
<td>Best at sequential reads and writes</td>
</tr>
<tr>
<td><strong>Best for</strong></td>
<td>Returning full rows of data based on a key</td>
<td>Returning aggregations of column values</td>
</tr>
<tr>
<td><strong>Implementation</strong></td>
<td>Transactional systems</td>
<td>Analytical processing</td>
</tr>
<tr>
<td><strong>Data compression</strong></td>
<td>Low to medium compression can be achieved</td>
<td>High compression is the norm</td>
</tr>
</tbody>
</table>
<h5 id="aws-solution">AWS Solution:</h5>
<p>Within AWS, the <strong>Amazon Relational Database Service (Amazon RDS)</strong> provides the needs for many different relational database management systems. It supports the most popular database engines including Amazon Aurora, MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server.</p>
<p>**Amazon Redshift **is a fast, scalable data warehouse that makes it simple and cost effective to analyze all your data across your data warehouse and data lake. Amazon Redshift delivers 10 times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk. You can set up and deploy a new data warehouse in minutes and run queries across petabytes of data in your Amazon Redshift data warehouse and exabytes of data in your data lake built on Amazon S3.</p>
<p>Amazon Redshift implements columnar indexing to achieve the the right performance for analytical workloads.</p>
<h3 id="intro-to-semistructured-and-unstructured-data-stores">Intro to Semistructured and Unstructured Data Stores</h3>
<p>Semistructured and unstructured data are often stored in non-relational database systems, sometimes called NoSQL databases.</p>
<h4 id="data-schemas">Data Schemas</h4>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210110210251527.png?raw=true" alt="image-20210110210251527"></p>
<h5 id="aws-solutions"><strong>AWS Solutions:</strong></h5>
<p>Amazon <strong>DynamoDB</strong> is a key-value and document database that delivers single-digit millisecond performance at any scale. It&rsquo;s a fully managed, multiregion, multimaster database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second.</p>
<h3 id="graph-databases"><strong>Graph databases</strong></h3>
<p>Graph databases are purpose-built to store any type of data: structured, semistructured, or unstructured. The purpose for organization in a graph database is to navigate <strong>relationships</strong>. Data within the database is queried using specific languages associated with the software tool you have implemented.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210111235609272.png?raw=true" alt="image-20210111235609272"></p>
<h5 id="the-aws-solution">The AWS Solution:</h5>
<p><strong>Amazon Neptune</strong> is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected data sets.</p>
<p>The core of Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency.</p>
<table>
<thead>
<tr>
<th><strong>Characteristic</strong></th>
<th><strong>Relational</strong></th>
<th><strong>Non-relational</strong></th>
<th><strong>Graph</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Representation</strong></td>
<td>Multiple tables, each containing columns and rows</td>
<td>Collection of documents Single table with keys and values</td>
<td>Collections of nodes and relationships</td>
</tr>
<tr>
<td><strong>Data</strong> <strong>design</strong></td>
<td>Normalized relational or dimensional data warehouse.</td>
<td>Denormalized document, wide column or key value</td>
<td>Denormalized entity relationship</td>
</tr>
<tr>
<td><strong>Optimized</strong></td>
<td>Optimized for storage</td>
<td>Optimized for compute</td>
<td>Optimized for relationships</td>
</tr>
<tr>
<td><strong>Query</strong> <strong>style</strong></td>
<td>Language: SQL</td>
<td>Language: many Uses object querying</td>
<td>Language: many Uses object querying</td>
</tr>
<tr>
<td><strong>Scaling</strong></td>
<td>Scale vertically</td>
<td>Scale horizontally</td>
<td>Hybrid</td>
</tr>
<tr>
<td><strong>Implementation</strong></td>
<td>OLTP business systems, OLAP data warehouse</td>
<td>OLTP web/mobile apps</td>
<td>OLTP web/mobile apps</td>
</tr>
</tbody>
</table>
<p>A multidemensional data warehouse is best suited for a <strong>relational database</strong>.</p>
<p>Log files are generally produced in the form of XML or JSON files, which are very well suited for storage in a <strong>document database</strong>.</p>
<p>Data collected from online gaming websites is often very rapid in generation and temporary in nature. This data is well suited for a <strong>key-value database</strong>.</p>
<p>Transactional data from a social subscription service could be stored in a relational database, but due to the social component, it would be better suited to the advantages gained by using a <strong>graph database</strong>.</p>
<h5 id="remark-horizontal-vs-vertical-scaling">Remark: Horizontal vs Vertical Scaling</h5>
<p>Scalable Dimensions:</p>
<ul>
<li>
<p>concurrent Connections,</p>
</li>
<li>
<p>CPU high or low</p>
</li>
<li>
<p>Memory(Quantity + Speed)</p>
</li>
<li>
<p>Netword Interfaces</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210112001502324.png?raw=true" alt="image-20210112001502324"></p>
</li>
</ul>
<p>Scale Horizontally by adding more nodes and a load balancer.</p>
<p>Horizontal has more startup cost, but better efficiency and safer.</p>
<p>Scale Vertically by adding more powerful hardware.</p>
<p>Vertical has lower cost efficiency and a theoretical maximum.</p>
<h2 id="veracity---cleansing-and-transformation">Veracity - Cleansing and Transformation</h2>
<h3 id="definitions"><strong>Definitions</strong></h3>
<p><strong>Curation</strong> is the action or process of selecting, organizing, and looking after the items in a collection.
<strong>Data integrity</strong> is the maintenance and assurance of the accuracy and consistency of data over its entire lifecycle.
<strong>Data veracity</strong> is the degree to which data is accurate, precise, and trusted.</p>
<h3 id="understanding-database-consistency---acid-and-base"><strong>Understanding database consistency</strong> - ACID and BASE</h3>
<h4 id="acid"><strong>ACID</strong></h4>
<p><em><strong>ACID</strong> is an acronym for <strong>A</strong>tomicity, <strong>C</strong>onsistency, <strong>I</strong>solation, and <strong>D</strong>urability. It is a method for maintaining consistency and integrity in a structured database.</em></p>
<h4 id="base"><strong>BASE</strong></h4>
<p><strong>BASE</strong> is an acronym for <strong>B</strong>asically <strong>A</strong>vailable <strong>S</strong>oft state <strong>E</strong>ventually consistent. *It is a method for maintaining consistency and integrity in a structured or semistructured database.</p>
<table>
<thead>
<tr>
<th><strong>ACID compliance</strong></th>
<th><strong>BASE compliance</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Strong consistency</td>
<td>Weak consistency – stale data is OK</td>
</tr>
<tr>
<td>Isolation is key</td>
<td>Availability is key</td>
</tr>
<tr>
<td>Focus on committed results</td>
<td>Best effort results</td>
</tr>
<tr>
<td>Conservative (pessimistic) availability</td>
<td>Aggressive (optimistic) availability</td>
</tr>
</tbody>
</table>
<h4 id="aws-solution-1">AWS Solution</h4>
<p>When it comes to performing the data transformation component of ETL, there are two options within AWS: Amazon EMR and AWS Glue. These two services provide similar results but require different amounts of knowledge and time investment.</p>
<p><strong>Amazon EMR</strong> is a more hands-on approach to creating your data pipeline. This service provides a robust data collection and processing platform. Using this service requires you to have strong technical knowledge and know-how on your team. The upside of this is that you can create a more customized pipeline to fit your business needs. Additionally, your infrastructure costs may be lower than running the same workload on AWS Glue.</p>
<p><strong>AWS Glue</strong> is a serverless, managed ETL tool that provides a much more streamlined experience than Amazon EMR. This makes the service great for simple ETL tasks, but you will not have as much flexibility as with Amazon EMR. You can also use AWS Glue as a metastore for your final transformed data by using the AWS Glue Data Catalog. This catalog is a drop-in replacement for a Hive metastore.</p>
<h2 id="value---reporting-and-business-intelligence">Value - Reporting and Business Intelligence</h2>
<ul>
<li>
<p>What were total sales in April?</p>
<p>Questions relating to past events are answered using <strong>descriptive analytics.</strong></p>
</li>
<li>
<p>What is the year-over-year total sales for the Asia Pacific region?</p>
<p>Questions comparing current data sets to past data sets are answered using <strong>diagnostic analytics</strong>.</p>
</li>
<li>
<p>What is the projected growth for smoking-related hospitalizations next year?</p>
<p>Questions looking for predictions of future events are answered using <strong>predictive analytics</strong>.</p>
</li>
<li>
<p>What products should I buy if I like the Seattle Seahawks?</p>
<p>Questions looking for recommendations based on preferences or prior purchase history are answered using <strong>prescriptive analytics</strong>.</p>
</li>
<li>
<p>What is the average number of vehicles spotted by my video doorbell?</p>
<p>Questions that require analysis of video, images, and voice are answered using <strong>cognitive analytics.</strong></p>
</li>
</ul>
<h4 id="aws-solutions-1">AWS Solutions:</h4>
<p><strong>Amazon QuickSight</strong> is a fast, easy-to-use, cloud-powered business analytics service that makes it easy for all employees within an organization to build visualizations, perform one-time analyses, and quickly get business insights from their data, any time, on any device.</p>
<p>Interactive dashboards provide dashboard consumers with a self-service way to consume and slice and dice their data to answer questions without having to rely on a business intelligence team.</p>
]]></content>
        </item>
        
        <item>
            <title>How to Use Auto Completion in GCP</title>
            <link>https://ffflora.cat/posts/2019/08/how-to-use-auto-completion-in-gcp/</link>
            <pubDate>Wed, 28 Aug 2019 21:30:55 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2019/08/how-to-use-auto-completion-in-gcp/</guid>
            <description>Auto-completion gcloud interactive has auto prompting for commands and flags, and displays inline help snippets in the lower section as the command is typed.
Static information, like command and sub-command names, and flag names and enumerated flag values, are auto-completed using dropdown menus.
Install the beta components:
gcloud components install beta Enter the gcloud interactive mode:
gcloud beta interactive When using the interactive mode, click on the Tab key to complete file path and resource arguments.</description>
            <content type="html"><![CDATA[<h2 id="auto-completion">Auto-completion</h2>
<p><code>gcloud interactive</code> has auto prompting for commands and flags, and displays inline help snippets in the lower section as the command is typed.</p>
<p>Static information, like command and sub-command names, and flag names and enumerated flag values, are auto-completed using dropdown menus.</p>
<p>Install the beta components:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">gcloud components install beta
</code></pre></div><p>Enter the gcloud interactive mode:</p>
<pre><code>gcloud beta interactive
</code></pre><p>When using the interactive mode, click on the <strong>Tab</strong> key to complete file path and resource arguments. If a dropdown menu appears, use the <strong>Tab</strong> key to move through the list, and the <strong>Space bar</strong> to select your choice.</p>
<p>Try it out! Start typing the following command, using auto-complete to finish the command:</p>
<pre><code>gcloud compute instances describe &lt;your_vm&gt;
</code></pre><p>Across the bottom of Cloud Shell you can see the shortcut to toggles this feature. Try out the F2 toggle:</p>
<p><code>F2</code>: help: STATE Toggles the active help section, ON when enabled, OFF when disabled.</p>
]]></content>
        </item>
        
    </channel>
</rss>
