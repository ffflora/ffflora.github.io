<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Flora Wenyao Jiang</title>
        <link>https://ffflora.cat/posts/</link>
        <description>Recent content in Posts on Flora Wenyao Jiang</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Mon, 08 Nov 2021 19:41:07 -0800</lastBuildDate>
        <atom:link href="https://ffflora.cat/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>LeetCode - Constantly Updating </title>
            <link>https://ffflora.cat/posts/2021/11/leetcode-constantly-updating/</link>
            <pubDate>Mon, 08 Nov 2021 19:41:07 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/11/leetcode-constantly-updating/</guid>
            <description></description>
            <content type="html"><![CDATA[]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Specialty Cheatsheet(4)</title>
            <link>https://ffflora.cat/posts/2021/11/aws-machine-learning-specialty-cheatsheet4/</link>
            <pubDate>Wed, 03 Nov 2021 14:47:21 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/11/aws-machine-learning-specialty-cheatsheet4/</guid>
            <description>ML Implementation and Operation   Inference Pipeline can be considered as an Amazon SageMaker model that you can use to make either real-time predictions or to process batch transforms directly without any external preprocessing.
  You can use Inference Pipeline to package Spark and scikit-learn based preprocessors into containers:
https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-mleap-scikit-learn-containers.html
  An inference pipeline is a Amazon SageMaker model that is composed of a linear sequence of two to fifteen containers that process requests for inferences on data.</description>
            <content type="html"><![CDATA[<h1 id="ml-implementation-and-operation">ML Implementation and Operation</h1>
<ol>
<li>
<p><strong>Inference Pipeline</strong> can be considered as an Amazon SageMaker model that you can use to make either real-time predictions or to process batch transforms directly without any external preprocessing.</p>
</li>
<li>
<p>You can use <strong>Inference Pipeline</strong> to package <strong>Spark</strong> and <strong>scikit-learn</strong> based preprocessors into containers:</p>
<p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-mleap-scikit-learn-containers.html">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-mleap-scikit-learn-containers.html</a></p>
</li>
<li>
<p>An <strong>inference pipeline</strong> is a Amazon SageMaker model that is composed of a linear sequence of <strong>two to fifteen</strong> containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.</p>
</li>
<li>
<p>Within an <strong>inference pipeline</strong> model, Amazon SageMaker handles <strong>invocations</strong> as a sequence of <strong>HTTP</strong> requests.</p>
</li>
<li>
<p><strong>Neo</strong> currently supports image classification models exported as frozen graphs from <strong>TensorFlow, MXNet, or PyTorch, and XGBoost</strong> models.:</p>
<p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html">https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html</a></p>
</li>
<li>
<p>three advantages of using Amazon <strong>Neo</strong> with SageMaker models are:</p>
<p><strong>(i) run ML models with up to 2x better performance,</strong></p>
<p><strong>(ii) reduce framework size by 10x, and</strong></p>
<p><strong>(iii) run the same ML model on multiple hardware platforms.</strong></p>
</li>
<li>
<p>AWS <strong>Greengrass</strong> is not used for <strong>code optimization</strong>.</p>
</li>
<li>
<p><strong>version tag</strong> should be used in the <strong>Registry Paths</strong>:</p>
<p>“:1” is the correct version tag for <strong>production</strong> systems.</p>
</li>
<li>
<p><strong>SageMaker</strong> does not support <strong>resource based policies</strong>. You can create a role to <strong>delegate access</strong> or provide access via <strong>identity federation</strong>.</p>
</li>
<li>
<p>If you want Amazon <strong>SageMaker</strong> to replicate a subset of data on each ML compute instance that is launched for model training, specify <strong><code>ShardedByS3Key</code></strong> for <strong>S3DataDistributionType</strong> field.</p>
</li>
<li>
<p>There are no <strong>inter-node communications for batch processin</strong>g, so <strong>inter-node traffic encryption is not required</strong>. SSH and AWS-SSE are not used for inter-node traffic encryption.</p>
</li>
<li>
<p>If the value of the objective metric for the current training job is <strong>worse</strong> (higher when minimizing or lower when maximizing the objective metric) than the <strong>median</strong> value of running averages of the objective metric for previous training jobs up to the same epoch, Amazon SageMaker <strong>stops the current training job.</strong></p>
</li>
<li>
<p>By using Amazon <strong>Elastic Inference</strong> (EI), you can <strong>speed up the throughput and decrease the latency of getting real-time inferences</strong> from your <strong>deep learning models</strong> that are deployed as Amazon SageMaker hosted models, but <strong>at a fraction of the cost</strong> of using a <strong>GPU</strong> instance for your endpoint.</p>
</li>
<li>
<p><strong>Amazon Elastic Inference</strong> allows you to attach <strong>low-cost GPU-powered acceleration</strong> to <!-- raw HTML omitted -->Amazon EC2 and Sagemaker instances or Amazon ECS tasks<!-- raw HTML omitted -->, to <strong>reduce the cost of running deep learning <!-- raw HTML omitted -->inference<!-- raw HTML omitted --> by up to 75%</strong>. <!-- raw HTML omitted -->Amazon Elastic Inference supports <strong>TensorFlow, Apache MXNet, PyTorch, and ONNX models</strong>.<!-- raw HTML omitted --></p>
</li>
<li>
<p><strong>Network isolation</strong> is <strong>not supported</strong> by the following managed Amazon SageMaker containers as <strong>they require access to Amazon S3</strong>:</p>
<p><strong>Chainer</strong></p>
<p><strong>PyTorch</strong></p>
<p><strong>Scikit-learn</strong></p>
<p>Amazon SageMaker <strong>Reinforcement Learning</strong></p>
</li>
<li>
<p>The default <strong>Sagemaker</strong> IAM role gets permissions to access any bucket that has <code>sagemaker</code> in the name. If you add a policy to the role that grants the SageMaker service principal **<code>S3FullAccess</code> permission, the <em>name of the bucket does not need to contain <code>sagemaker</code></em>. ** Granting public access to S3 bucket is not recommended.</p>
</li>
<li>
<p><strong>IAM</strong> does <strong>not allow nesting or hierarchy of groups</strong>.(Can&rsquo;t be a child of other IAM groups.)</p>
</li>
<li>
<p>For the <strong>EC2 instance</strong>, <strong>IAM Role</strong> is the recommended mechanism for managing access. You can attach the required policy to the IAM Role for DynamoDB access.  <strong>DynamoDB does not support resource-based policy.</strong></p>
</li>
<li>
<p><strong>Group</strong> is <strong>not considered identity</strong>, and you <strong>cannot grant access to a group in a resource-based policy</strong>. With <strong>Resource-based</strong> policy, you can <strong>grant access to users, roles and other accounts</strong>. Resource-based policy is also called <strong>bucket-policy</strong>, as the policy is attached to the S3 buckets. Resource-based policy needs to have <strong>Principal</strong> tag.</p>
</li>
<li>
<p>There are two types of policies: <strong>Inline(Embeded) policy(not reusable)</strong> and <strong>Managed policy(reusable).</strong></p>
</li>
<li>
<p><strong>Attribute-Based Access Control (ABAC)</strong> is an authorization strategy that <strong>defines permissions based on attributes (or tags)</strong>. You can structure polices to allow operations when the <strong>principal&rsquo;s tag matches the resource tag</strong>. This approach is <strong>useful in an environment that is growing or changing rapidly.</strong> For example, you can check the cost center of an employee with that of the resource and allow access only if the cost center&rsquo;s match. <strong>RBAC</strong>(resource based), on the other hand, requires <strong>ongoing maintenance to update the resource list.</strong></p>
</li>
<li>
<p>In SageMaker, the <strong><code>iam:PassRole</code></strong> action is needed for the Amazon SageMaker action <strong><code>sagemaker:CreateModel</code></strong>. This allows the user to pass authorization to SageMaker to actually create models.</p>
</li>
<li>
<p><strong>XGBoost</strong> is a <strong>CPU-only</strong> algorithm, and won&rsquo;t benefit from the GPU&rsquo;s of a P3 or P2. It is also <strong>memory-bound</strong>, making M4 a better choice than C4.</p>
</li>
<li>
<p>With <strong>Pipe</strong> input mode, your data is fed on-the-fly into the algorithm container <strong><!-- raw HTML omitted -->without involving any disk I/O<!-- raw HTML omitted -->.</strong> This approach shortens the <strong>lengthy download process and dramatically reduces startup time</strong>. <!-- raw HTML omitted -->It also offers generally <strong>better read throughput than File input mode</strong>.<!-- raw HTML omitted --> This is because your data is fetched from Amazon S3 by a highly optimized <!-- raw HTML omitted -->multi-threaded<!-- raw HTML omitted --> background process. It also allows you to train on datasets that are much larger than the <strong>16 TB Amazon Elastic Block Store (EBS)</strong> volume size limit.</p>
<p><strong>Pipe mode enables the following:</strong></p>
<p>- <!-- raw HTML omitted --><strong>Shorter startup</strong><!-- raw HTML omitted --> times because the data is being streamed instead of being downloaded to your training instances.</p>
<p>- <strong>Higher I/O throughputs</strong> due to high-performance streaming agent.</p>
<p>- Virtually limitless data processing capacity.</p>
<p>With Pipe mode, the startup time is reduced significantly from <strong>11.5 minutes to 1.5 minutes</strong> in most experiments. Also, the overall <strong>I/O throughput is at least twice as fast as that of File mode</strong>. Both of these improvements made a positive impact on the total training time, which is reduced by up to 35%.</p>
<p><strong>File mode is the default mode for training a model in Amazon SageMaker.</strong></p>
</li>
<li>
<p>The algorithms that support <strong>Pipe input mode</strong> today when used with <strong>protobuf recordIO-encoded</strong> datasets are <!-- raw HTML omitted -->Principal Component Analysis (PCA), K-Means Clustering, Factorization Machines, Latent Dirichlet Allocation (LDA), Linear Learner (Classification and Regression), Neural Topic Modelling, and Random Cut Forest. AWS Mechanical Turk, Amazon Comprehend, and Amazon QuickSight are the fully managed AWS services for crowdsourcing tasks, Natural Language Processing (NLP), and Business Intelligence (BI)<!-- raw HTML omitted -->.</p>
</li>
<li>
<p>Amazon SageMaker&rsquo;s <strong>Pipe mode</strong> does <strong>not</strong> support Apache <strong>Parquet</strong> data format.</p>
</li>
<li>
<p><strong>EC2</strong> Storages:</p>
<ul>
<li><strong>Instace Store(Block)</strong>:
<ul>
<li>Store of host computer is <strong>assigned</strong> to <strong>EC2</strong> instance.</li>
<li><strong>Temporary</strong> Storage</li>
<li><strong>Highest</strong> Performance</li>
<li>Storage included as part of instance pricing</li>
<li><strong>Durability</strong>:
<ul>
<li>Data persists only for the <strong>lifetime</strong> of the instance</li>
<li><strong>Reboot</strong> - Data <strong>Persists</strong></li>
<li>Data is <strong>lost</strong> - when underlying hardware <strong>fails</strong>, instance <strong>stops</strong>, or instance <strong>terminates</strong>.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Elastic Block Store(EBS):</strong>
<ul>
<li>EBS is a managed <strong>block</strong> storage service</li>
<li>Storage volume is <strong>outside</strong> of host computer - <strong>long term</strong> persistance.</li>
<li>EC2 instance use EBS storage volume as a block device</li>
<li>You need to pay for allocated EBS storage</li>
<li><strong>Benefit:</strong>
<ul>
<li><strong>Stop-start</strong> EC2 instance</li>
<li><strong>Persist</strong> EBS volumes for <strong>terminated</strong> instances</li>
<li><strong>Detach and attach volume t</strong>o a different instance in the <strong>same availability</strong> zone</li>
<li>Built-in <strong>Snapshot</strong> capability for incremental backup to S3</li>
<li>Create <strong>AMI</strong> from snapshots to launch new EC2 instances</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>You can use <strong>Amazon CloudWatch API</strong> operations to send the <strong>training metrics to CloudWatch</strong>, and create a dashboard of those metrics. Lastly, use Amazon Simple Notification Service <strong>(Amazon SNS) to send a notification when the model is overfitting</strong>.</p>
</li>
<li>
<p><strong>CloudWatch Event</strong> does <strong>not</strong> capture events that occur <strong>during model training</strong></p>
</li>
<li>
<p><strong>CloudWatch Logs</strong> will typically contain the <strong>statistics</strong> as reported by the <strong>inference algorithm</strong>. In the case of <strong>Linear Learner</strong>, the <strong>predicted_label</strong> and the <strong>score</strong> is returned and stored in the <strong>CloudWatch Log entries</strong>.</p>
</li>
<li>
<p>The <strong><code>dropout</code></strong> hyperparameter refers to the dropout probability for network layers. <em>A</em> dropout is a form of regularization used in neural networks that <!-- raw HTML omitted -->reduces overfitting by trimming codependent neurons.<!-- raw HTML omitted --></p>
<p>This is an optional parameter in Amazon SageMaker <strong>Object2vec</strong>. <strong>Increasing the value of this parameter may say solve the overfitting of the model.</strong></p>
<p><strong>L1 regularization</strong> method is <strong>not</strong> available to Amazon SageMaker <strong>Object2vec</strong>. This is used for simple regression models like a Linear learner.</p>
</li>
<li>
<p>With the <strong>Lifecycle configuration</strong> feature in Amazon SageMaker, you can automate these customizations to be applied at different phases of the lifecycle of an instance. For example, you can write a script to install a list of libraries and, using the Lifecycle configuration feature, configure the scripts to <strong>automatically execute every time your notebook instance is started</strong>. Similarly, you can choose to <strong>automatically run the script only once when the notebook instance is created.</strong></p>
</li>
<li>
<p>The performance of deep learning neural networks often improves with the amount of data available.</p>
<p><strong>Data augmentatio</strong>n is a technique to artificially create new training data from existing training data. <!-- raw HTML omitted -->This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.<!-- raw HTML omitted --></p>
<p><strong>Image data augmentation</strong> is perhaps the most well-known type of data augmentation and involves <strong>creating transformed versions of images</strong> in the training dataset that belong to the same class as the original image.</p>
</li>
<li>
<p><strong>T-SNE</strong> is used to preprocess a dataset that contains highly correlated variables.</p>
</li>
<li>
<p><strong>Apache Spark</strong> is a <strong>fast and general engine for large-scale data processing</strong>. Amazon EMR features Amazon <strong>EMR runtime for Apache Spark</strong>, a performance-optimized runtime environment for Apache Spark that is active by default on Amazon EMR clusters.</p>
</li>
<li>
<p><strong>Apache Spark is a distributed processing framework and programming model that helps you do machine learning, stream processing, or graph analytics using Amazon EMR clusters.</strong></p>
<p><strong>Apache ZooKeeper</strong> is a <strong>centralized service for maintaining configuration information</strong> and <strong>providing distributed synchronization</strong> to distributed applications.</p>
<p><strong>Apache MXNet</strong> is an <strong>open-source, deep learning framework.</strong></p>
<p><strong>Apache Pig</strong> is more suitable for running <strong>big data analysis</strong>.</p>
</li>
<li>
<p>Integrating <strong>SageMaker and Spark:</strong></p>
<ul>
<li><strong>Pre-process data as normal with Spark</strong> - Generate DataFrames</li>
<li>Use <strong>SageMaker-spark</strong> library // <a href="https://github.com/aws/sagemaker-spark/blob/master/sagemaker-spark-sdk">sagemaker-spark-sdk</a></li>
<li><strong>SageMakerEstimator - K-means, PCA, XGBoost</strong></li>
<li>SageMakerModel</li>
</ul>
<h5 id="how-does-it-work">How does it work?</h5>
<ul>
<li>Connect notebook to a remote <strong>EMR cluster running Spark( or Zeppelin)</strong></li>
<li>Training df should have :
<ul>
<li>a feature col that is a vector of doubles</li>
<li>An optional labels col of doubles</li>
</ul>
</li>
<li>Call <strong>fit</strong> on your <strong>SageMakerEstimator</strong> to get a <strong>SageMakerModel</strong></li>
<li>Call <strong>transform</strong> on the SageMakerModel to make inferences</li>
<li>Works with Spark <strong>pipelines</strong> as well.</li>
</ul>
</li>
<li>
<p><strong>Spark MLLib</strong></p>
<ul>
<li>
<p>Classification: logistic regression, naive bayes</p>
</li>
<li>
<p>regression</p>
</li>
<li>
<p>decision tree</p>
</li>
<li>
<p>recommendation engine(ALS)</p>
</li>
<li>
<p>Clustering(k-means)</p>
</li>
<li>
<p>LDA</p>
</li>
<li>
<p>ML workflow utilities (pipeline, feature transformation, persistence)</p>
</li>
<li>
<p>SVD, PCA, statistics</p>
</li>
</ul>
</li>
<li>
<p>With Amazon <strong>Polly&rsquo;s</strong> custom <strong>lexicons</strong> or <strong>vocabularies</strong>, you can <strong>modify the pronunciation of particular words</strong>, such as company names, acronyms, foreign words, and neologisms (e.g., &ldquo;ROTFL&rdquo;, &ldquo;C’est la vie&rdquo; when spoken in a non-French voice). To customize these pronunciations, you upload an <strong>XML file with lexical entries</strong>. For example, you can customize the pronunciation of the Filipino word: &ldquo;Pilipinas&rdquo; by using the <code>phoneme</code> element in your input XML.</p>
</li>
<li>
<p>Lex can handle <strong>both speech-to-text and handling the chatbot logic</strong>. The output from Lex could be read back to the customer using Polly. Under the hood, more services would likely be needed as well to support Lex, such as Lambda and DynamoDB.</p>
</li>
<li>
<p><strong>Amazon SageMaker</strong> enables you to <strong>test multiple models or model versions</strong> behind the <strong>same endpoint</strong> using <strong>production</strong> variants. Each <strong><code>ProductionVariant</code></strong> identifies an ML model and the resources deployed for hosting the model. You can distribute endpoint invocation requests across multiple production variants by providing the traffic distribution for each variant or invoking a variant directly for each request.</p>
</li>
<li>
<p>SageMaker <strong>Debugger</strong>:</p>
<p>supported framework &amp;algos:</p>
<ul>
<li>tensorflow</li>
<li>pytorch</li>
<li>MXNet</li>
<li>XGBoost</li>
<li>SageMaker generic estimator(for use with custom training containers)</li>
</ul>
</li>
<li>
<p>you can create and run a custom ETL job in <strong>AWS Glue</strong> to <!-- raw HTML omitted --><strong>redact sensitive information</strong><!-- raw HTML omitted --> within a dataset stored in Amazon <strong>S3.</strong></p>
</li>
<li>
<p>The ML <strong>algorithm</strong> should be available in <strong>/opt/program</strong> directory of the Docker container. Three main files there are <strong>train, serve, and predictor.py.</strong></p>
<p><!-- raw HTML omitted -->***train/***<!-- raw HTML omitted --> contains the <!-- raw HTML omitted -->logic for training the model and storing the trained model<!-- raw HTML omitted -->.</p>
<p>If the train file runs successfully, it will save a model to <strong>/opt/ml/model</strong> directory. Deployment code for inference goes here.</p>
<p><strong>/opt/ml/code</strong> training script files goes here.</p>
<p><em><strong>serve/</strong></em> essentially runs the <strong>logic written in predictor.py.</strong></p>
<p>Two other files, nginx.config (NGINX configuration file) and wsgi.py (a simple wrapper file), usually can be left unchanged.</p>
</li>
<li>
<p>Amazon Sagemaker provides prebuilt Amazon SageMaker <strong>Docker Images for TensorFlow, MXNet, Chainer, and PyTorch, as well as Docker Images for Scikit-learn and Spark ML</strong>.</p>
</li>
<li>
<p>When using custom <strong>TensorFlow</strong> code, the Amazon SageMaker <strong>Python SDK</strong> supports <strong>script mode</strong> training scripts.</p>
</li>
<li>
<p>Your <strong>inference</strong> container responds to port <strong>8080</strong>, and must respond to ping requests in under <strong>2 seconds.</strong> Model artifacts need to be compressed in <strong>tar</strong> format, not zip.</p>
</li>
<li>
<p>Amazon SageMaker runs training jobs in an Amazon Virtual Private Cloud (Amazon <strong>VPC</strong>) to help keep your data secure. <!-- raw HTML omitted -->If you are using <strong>distributed ML frameworks and algorithms</strong>, and your algorithms transmit information that is directly related to the model (eg weights),<!-- raw HTML omitted --> you can provide an <strong>additional level of security by enabling inter-container traffic encryption</strong>. But note that turning on inter-container traffic encryption can <strong>increase training time</strong>, and therefore the <strong>cost</strong>.</p>
</li>
<li>
<p>VPC is within AWS customer account, SageMaker/EBS/&hellip; are within AWS service account.</p>
</li>
<li>
<p>Amazon <strong>Macie</strong> is a <strong>security service</strong> that uses <strong>machine learning to automatically discover, classify, and protect sensitive data in AWS</strong>. Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved. The fully managed service continuously monitors data access activity for anomalies and generates detailed alerts when it detects the risk of unauthorized access or inadvertent data leaks. Amazon Macie is available to protect data stored in Amazon S3.</p>
</li>
<li>
<p>Multi-factor authentication (<strong>MFA</strong>) could be used with <strong>each</strong> account for <strong>enhanced data security</strong>.</p>
</li>
<li>
<p>AWS managed SageMaker policies that can be attached to the users are <strong>AmazonSageMakerReadOnly, AmazonSageMakerFullAccess, AdministratorAccess, and DataScientist.</strong></p>
</li>
<li>
<p>The containers must be <strong>NVIDIA-Docker</strong> compatible and contain only <strong>CUDA</strong> toolkit, <strong>without NVIDIA Drivers</strong>. The toolkit includes a container runtime library and utilities to automatically configure containers to leverage NVIDIA GPUs.</p>
</li>
<li>
<p>When <strong>scaling</strong> responsiveness is not as fast as you would like, you should look at the <strong>cooldown</strong> period. The cooldown period is a duration when scale events will be ignored, allowing the new instances to become established and take on load. Decreasing this value will launch new variant instance faster.</p>
</li>
<li>
<p>Amazon <strong>ECR</strong> is <strong>integrated</strong> with Amazon <strong>ECS</strong> allowing you to easily store, run, and manage <strong>container images for applications running on Amazon ECS</strong>. All you need to do is specify the Amazon ECR repository in your task definition and Amazon ECS will retrieve the appropriate images for your applications.</p>
</li>
</ol>
<hr>
<h3 id="pca-on-sagemaker">PCA on SageMaker</h3>
<h4 id="two-modes">Two Modes</h4>
<ul>
<li>
<p><strong>Regular</strong> - Good for Sparse Data and Moderate sized datasets</p>
</li>
<li>
<p><strong>Random</strong> - Good for very large datasets – uses approximation algorithm</p>
</li>
</ul>
<h2 id="factorization-machine">Factorization Machine</h2>
<p>Factorization Machine algorithm is optimized for handling <strong>high dimensional sparse</strong> datasets.</p>
<p>Supports Regression and Classification.</p>
<p>Personalize Content - “predict” ratings/likeness</p>
<ul>
<li>Click Prediction for Ad-Placement</li>
<li>Product recommendation for user</li>
<li>Movie recommendation</li>
<li>News/Social Media Feed personalization for users</li>
</ul>
<h3 id="factorization-machine--data-format">Factorization Machine – Data Format</h3>
<p><strong>Input</strong>: recordio-protobuf (with Float32 values)</p>
<p>Inference: json recordio-protobuf</p>
<h2 id="random-cut-forest">Random Cut Forest</h2>
<ul>
<li>Unsupervised algorithm to detect outliers or anomalous data points</li>
<li>Tree based ensemble method</li>
<li>Support for Timeseries data</li>
<li>Assigns an anomaly score for each data point</li>
</ul>
<h3 id="rcf-uses">RCF Uses</h3>
<ul>
<li>Traffic spike due to rush hour or accident</li>
<li>DDoS attack detection</li>
<li>Unauthorized data transfer detection</li>
</ul>
<h2 id="kinesis-streams-vs-firehose">Kinesis Streams vs Firehose</h2>
<h3 id="kinesis-data-streams">Kinesis Data Streams</h3>
<ul>
<li>we can write custom code for the producer and the consume</li>
<li>It is going to be real time,  between 70 milliseconds and 200-millisecond latency,</li>
<li><strong>you must manage to scale yourself</strong> so if you want <strong>more throughpu</strong>t you need to think to do something called <strong>Shard splitting,</strong> which means <strong>adding Shards,</strong> and if you want <strong>less throughput</strong> you need to do <strong>Shard merging</strong>, which is <strong>removing</strong> Shards. And that is quite painful to do.</li>
<li>Data storage for your stream, between 1 to 7 days, and this allows  replay capability,  multiple consumer</li>
</ul>
<h3 id="firehose">Firehose</h3>
<ul>
<li>
<p>is a delivery service.</p>
</li>
<li>
<p>It&rsquo;s an ingestion service, so remember that word &ldquo;<strong>ingestion</strong>&rdquo;. And so it&rsquo;s <strong>fully</strong> <strong>managed</strong> and you can send it out to Amazon <strong>S3, Splunk, Redshift, and ElasticSearch</strong>.</p>
</li>
<li>
<p>It is fully serverless, that means we don&rsquo;t manage anything, <strong>and you can do also serverless data transformations using Lambda.</strong></p>
</li>
<li>
<p>It&rsquo;s going to be near real-time, that means that it&rsquo;s going to deliver data with the <strong>lowest buffer time of 1 minute into your targets,</strong></p>
</li>
<li>
<p>there is <strong>automated scaling</strong>, we don&rsquo;t need to provision capacity in advance,</p>
</li>
<li>
<p>and there is no data stored so there is no replay capability.</p>
</li>
</ul>
<h3 id="kinesis-summary">Kinesis Summary</h3>
<ul>
<li>Kinesis Data Stream: create real-time ML apps.</li>
<li>Kinesis Data Firehose: ingest massive data near real-time.</li>
<li>Kinesis Data Analytics: Real time ETL/ML algos on streams</li>
<li>Kinesis Video Stream: real-time video stream to create ML apps.</li>
<li>Kinesis Agent: is a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Streams and Kinesis Firehose. The agent continuously monitors a set of files and sends new data to your stream. The agent handles file rotation, checkpointing, and retry upon failures. It delivers all of your data in a reliable, timely, and simple manner. It also emits Amazon CloudWatch metrics to help you better monitor and troubleshoot the streaming process</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Specialty Cheatsheet(3)</title>
            <link>https://ffflora.cat/posts/2021/11/aws-machine-learning-specialty-cheatsheet3/</link>
            <pubDate>Wed, 03 Nov 2021 14:47:15 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/11/aws-machine-learning-specialty-cheatsheet3/</guid>
            <description>Modeling   Object2Vec can be used to find semantically similar objects such as questions. BlazingText Word2Vec can only find semantically similar words.
  mode is the mandatory hyperparameter for both the Word2Vec (unsupervised) and Text Classification (supervised) modes of the SageMaker BlazingText algorithm.
  Incremental Training in Amazon SageMaker
Over time, you might find that a model generates inference that are not as good as they were in the past.</description>
            <content type="html"><![CDATA[<h1 id="modeling">Modeling</h1>
<ol>
<li>
<p><em><strong>Object2Vec</strong></em> can be used to find semantically similar <strong>objects</strong> such as questions. <em><strong>BlazingText Word2Vec</strong></em> can only find semantically similar <strong>words</strong>.</p>
</li>
<li>
<p><strong>mode</strong> is the mandatory hyperparameter for both the <strong>Word2Vec</strong> (unsupervised) and <strong>Text Classification</strong> (supervised) modes of the SageMaker <strong>BlazingText</strong> algorithm.</p>
</li>
<li>
<p><strong>Incremental Training</strong> in Amazon SageMaker</p>
<p>Over time, you might find that a model generates inference that are not as good as they were in the past. With incremental training, you can use the artifacts from <strong>an existing model and use an expanded dataset</strong> to train a new model. Incremental training <strong>saves both time and resources</strong>.</p>
<p>Use incremental training to:</p>
<ul>
<li>Train a new model using an expanded dataset that contains an underlying pattern that was not accounted for in the previous training and which resulted in poor model performance.</li>
<li>Use the model artifacts or a portion of the model artifacts from a popular publicly available model in a training job. You don&rsquo;t need to train a new model from scratch.</li>
<li>Resume a training job that was stopped.</li>
<li>Train several variants of a model, either with different hyperparameter settings or using different datasets.</li>
</ul>
</li>
<li>
<p><strong>Rekognition</strong> <strong>doesn&rsquo;t</strong> support <strong>Incremental training</strong>.</p>
</li>
<li>
<p><strong>Amazon Rekognition Content Moderation</strong> enables you to streamline or automate your image and video <strong>moderation</strong> workflows using machine learning. Using fully managed image and video moderation APIs, you can proactively detect inappropriate, unwanted, or offensive content containing nudity, suggestiveness, violence, and other such categories.</p>
</li>
<li>
<p>Only <strong>three</strong> built-in algorithms currently <em><strong>support</strong></em> <strong>incremental training</strong>: Object <strong>Detection Algorithm, Image Classification Algorithm, and Semantic Segmentation Algorithm.</strong></p>
</li>
<li>
<p><strong>BlazingText</strong> algorithm can be used in both <strong>supervised and unsupervised</strong> learning modes.</p>
</li>
<li>
<p>SageMaker <strong>DeepAR</strong> algorithm specializes in <strong>forecasting</strong> new product performance.</p>
</li>
<li>
<p><strong>LDA</strong>: <strong>Observations</strong> are referred to as documents. The <strong>feature set</strong> is referred to as <strong>vocabulary</strong>. A feature is referred to as a <strong>word</strong>. And the <strong>resulting categories</strong> are referred to as topics.</p>
</li>
<li>
<p><strong>LDA</strong> is a <strong>&ldquo;bag-of-words&rdquo;</strong> model, which means that the <strong>order</strong> of words does <strong>not matter</strong>.</p>
</li>
<li>
<p><strong>Factorization Machines</strong> algorithm specializes in building <strong>recommendation systems.</strong></p>
</li>
<li>
<p><strong>Factorization Machine</strong> can be used to <strong>capture click patterns</strong> for a click prediction system.</p>
</li>
<li>
<p><strong>Image Classification</strong> is used to <strong>classify</strong> images into multiple classes such as cat vs dog. <strong>Object Detection</strong> is used to <strong>detect</strong> objects in an image. Semantic Segmentation is used for pixel level analysis of an image and it can be used in this computer vision system to <strong>detect misalignment</strong>.</p>
</li>
<li>
<p><em><strong>Object Detection</strong></em> : is the technology that is related to computer vision and image processing. It&rsquo;s aim? detect objects in an image.</p>
<p><em><strong>Semantic Segmentation</strong></em> : is a technique that detects , for each pixel , the object category it belongs to , all object categories ( labels ) must be known to the model.</p>
<p><em><strong>Instance Segmentation</strong></em> : same as Semantic Segmentation, but dives a bit deeper, it identifies , for each pixel, the object instance it belongs to. The main difference is that differentiates two objects with the same labels in comparison to semantic segmentation.</p>
</li>
<li>
<p><code>feature_dim</code> and <code>k</code> are the required hyperparameters for the SageMaker <strong>K-means</strong> algorithm.</p>
</li>
<li>
<p>When you use <strong>automatic model tuning</strong>, the <strong>linear learner</strong> internal tuning mechanism is turned <strong>off</strong> automatically. <strong>This sets the number of parallel models, num_models, to 1.</strong></p>
</li>
<li>
<p>You can think of <strong>L1</strong> as reducing the number of features in the model altogether. <strong>L2</strong> <strong>“regulates” the feature weight</strong> instead of just dropping them. Please review the concept of L1 and L2 regularization in more detail:</p>
<p><a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c</a></p>
</li>
<li>
<p>The <strong>residuals</strong> plot would indicate any trend of underestimation or overestimation. Ideally, <a href="https://www.statisticshowto.com/residual/">residual values</a> should be equally and randomly spaced around the horizontal axis. Both <strong>Mean Absolute Error</strong> and <strong>RMSE</strong> would only give the error magnitude.</p>
</li>
<li>
<p><strong>AUC/ROC</strong>  is the best evaluation metric for a <strong>binary</strong> classification model. This metric does <strong>not</strong> require you to set a classification <strong>threshold</strong>.</p>
<p>For <strong>imbalanced</strong> datasets, you are better off using another metric called - <strong>PR AUC</strong> - that is also used in production systems for a highly imbalanced dataset, where the fraction of <strong>positive class is small</strong>, such as in case of credit card fraud detection.</p>
</li>
<li>
<p>A <strong>&ldquo;vanishing gradient&rdquo;</strong> results from multiplying together many small derivates of the <strong>sigmoid</strong> activation function in multiple layers. <strong>ReLU</strong> does not have a small derivative, and avoids this problem.</p>
</li>
<li>
<p>Fixing the <strong>&ldquo;vanishing gradient&rdquo;</strong>:</p>
<ul>
<li>Multi-level heirarchy: break up levels into their own sub-networks trained individually</li>
<li>Long short-term memory(LSTM)</li>
<li>Residual Networks
<ul>
<li>Resnet</li>
<li>Ensemble of shorter networks</li>
</ul>
</li>
<li>Better choice of activation function
<ul>
<li>ReLu</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Transfer learning</strong> generally involves using an existing model, or adding additional layers on top of one.</p>
</li>
<li>
<p>A <strong>learning rate</strong> that is <strong>too large</strong> may <strong>overshoot the true minima</strong>, while a learning rate that is <strong>too small will slow down convergence</strong>.</p>
</li>
<li>
<p><strong>Learning rat</strong>e affects the <strong>speed</strong> at which the algorithm reaches (<strong>converges</strong> to) the optimal weights. The SGD algorithm makes updates to the weights of the linear model for every data example it sees. The size of these updates is controlled by the learning rate.<img src="https://cs231n.github.io/assets/nn3/learningrates.jpeg" alt="img"></p>
</li>
<li>
<p><strong>Music</strong> is fundamentally a <!-- raw HTML omitted -->timeseries<!-- raw HTML omitted --> problem, which <strong>RNN&rsquo;s</strong> (recurrent neural networks) are best suited for. You might see the term <strong>LSTM</strong> used as well, which is a specific kind of RNN.</p>
</li>
<li>
<p><strong>RNN</strong> is good for:</p>
<ul>
<li><strong>Timeseries</strong> data(predict future based on past; logs; where to drive the <strong>self-driving car</strong> based on past trajectories)</li>
<li>Data that consist of sequence of arbitrary length
<ul>
<li>machine translation</li>
<li>image captions</li>
<li>Machine-generated music</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Custom entity recognition</strong> extends the capability of <strong>Amazon Comprehend</strong> by enabling you to identify new entity types not supported as one of the preset generic entity types. This means that in addition to identifying entity types such as LOCATION, DATE, PERSON, and so on, you can <strong>analyze documents and extract entities</strong> like product codes or business-specific entities that fit your particular needs.</p>
</li>
<li>
<p>To <strong>get inferences for an entire dataset</strong>, use <strong>batch transform</strong>. With batch transform, you create a batch transform job using <strong>a trained model and the dataset</strong>, which must be stored in Amazon <strong>S3</strong>. Amazon SageMaker saves the inferences in an S3 bucket that you specify when you create the batch transform job.</p>
<p>You can use Amazon SageMaker Batch Transform to <!-- raw HTML omitted --><strong>exclude attributes</strong><!-- raw HTML omitted --> before running predictions. You can also join the prediction results with partial or entire input data attributes when using data that is in <strong>CSV, text, or JSON</strong> format. This eliminates the need for any additional pre-processing or post-processing and accelerates the overall ML process.</p>
</li>
<li>
<p>Two methods of deploying a model for <strong>inference</strong>:</p>
<ul>
<li>Amazon SageMaker <strong>Hosting Services</strong>
<ul>
<li>Provides a persistent <strong>HTTPS</strong> endpoint for getting predictions one at a time.</li>
<li>Suited for web applications that need <strong>sub-second latency response.</strong></li>
</ul>
</li>
<li>Amazon SageMaker <strong>Batch Transform</strong>
<ul>
<li>Doesn’t need a persistent endpoint</li>
<li>Get inferences for an <strong>entire</strong> dataset</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>IP Insights</strong> algorithm supports <strong>only CSV</strong> file type as training data.</p>
</li>
<li>
<p>In <strong>XGBoost</strong>,<code>subsample</code> prevents overfitting.</p>
<p><code>eta</code> step size shrinkage, prevent overfitting</p>
<p><code>gamma</code>: minimum loss reduction to create a partition; larger = more conservation</p>
<p><code>alpha</code> L1 regularization = more conservation</p>
<p><code>lambda</code> L2 regularization= more conservation</p>
<p><code>eval_metric</code> optimize on AUC, errer, rmse&hellip;</p>
<p><code>scale_pos_weight</code> Adust balance of positive and negative weights; helpful for unbalanced classes; might set to sum(negative class)/sum(positive classes)</p>
<p><code>max_depth</code> maximum depth of the tree; too high and you may overfit</p>
<p>Other XGBoost hyperparameters: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html</a></p>
</li>
<li>
<p><strong>XGBoost Regularization:</strong></p>
<p><strong><code>alpha</code>:</strong> L1 regularization. Default 0;</p>
<p><strong><code>lambda</code>:</strong> L2 regularization, default 1.</p>
</li>
<li>
<p><strong>Boosting</strong> generally <strong>yields better accuracy</strong></p>
<p><strong>Bagging</strong> avoids <strong>overfitting</strong>, and <strong>easier to parallize</strong></p>
</li>
<li>
<p><strong>Bullseyes</strong> desmonstrates</p>
<p><img src="https://www.researchgate.net/profile/Anni-Helena-Ruotsala/publication/304674901/figure/fig6/AS:668649476067338@1536429866393/Precision-versus-accuracy-The-bullseye-represents-the-true-value-eg-the-true.ppm" alt="Precision versus accuracy. The bullseye represents the true value, e.g., the true location of the object, while black dots represent measurements, e.g., the estimated 3D locations of the object based on the 2D images. Source: http://www.antarcticglaciers.org/glacial-geology/dating-glacial-sediments2/precision-and-accuracy-glacial-geology/. Accessed 7.4.2016."></p>
<p>Dart-throwing Demo</p>
<p><img src="https://www.researchgate.net/profile/Syed-Akhter-6/publication/330761010/figure/fig2/AS:721145984720896@1548946009061/Bias-and-variance-in-dart-throwing.ppm" alt="2: Bias and variance in dart-throwing."></p>
</li>
<li>
<p><strong>Batch Normalization</strong> should <strong>not</strong> be a <strong>method</strong> of <strong>regularization</strong> because the main purpose of it is to <strong>speed up the training by selecting a batch and forcing the weight to be distributed near 0</strong>, <strong>not</strong> too large, <strong>not</strong> too small.</p>
</li>
<li>
<p>Amazon SageMaker <strong>NTM</strong> is an <strong>unsupervised</strong> learning algorithm that is used to <strong>organize a corpus of documents into <em>topics</em> that contain word groupings based on their statistical distribution.</strong> Documents that contain frequent occurrences of words such as &ldquo;bike&rdquo;, &ldquo;car&rdquo;, &ldquo;train&rdquo;, &ldquo;mileage&rdquo;, and &ldquo;speed&rdquo; are likely to share a topic on &ldquo;transportation&rdquo; for example. Topic modeling can be used to classify or summarize documents based on the topics detected or to retrieve information or recommend content based on topic similarities. The topics from documents that NTM learns are characterized as a <em><strong>latent representation</strong></em> because the topics are inferred from the observed word distributions in the corpus. The semantics of topics are usually inferred by examining the top ranking words they contain. Because the method is unsupervised, <strong>only the number of topics</strong>, not the topics themselves, are <strong>prespecified</strong>. In addition, the topics are not guaranteed to align with how a human might naturally categorize documents.</p>
</li>
<li>
<p><strong>1×1 convolutions</strong> are called <strong>bottleneck</strong> structure in <strong>CNN</strong>, which can:</p>
<ul>
<li>
<p>It suffers <strong>less overfitting</strong> due to <strong>small kernel size</strong></p>
</li>
<li>
<p>It can be used for <strong>feature pooling</strong></p>
</li>
<li>
<p>can help in <strong>dementionality reduction</strong></p>
</li>
</ul>
</li>
<li>
<p><strong>CNN</strong> is called &ldquo;<strong>feature-location invariant</strong>&rdquo;</p>
</li>
<li>
<p><strong>CNN typical usage:</strong></p>
<p>Conv2D -&gt; Maxpooling2D -&gt; Dropout -&gt; Flatten -&gt; Dense -&gt; Dropout -&gt; Softmax</p>
</li>
<li>
<p><strong>Softmax</strong> usually used as the last layer of the <strong>multiple classification</strong> problem. It can&rsquo;t product more than one label(sigmoid can)</p>
</li>
<li>
<p><strong>Logistic activation, Sigmoid, or Soft Step</strong> all represent the same function: Logistic (Sigmoid or Soft Step): f(x)=sigma(x)=1/(1-exp(-x))</p>
</li>
<li>
<p><strong>Entropy</strong> is a measure of the <strong>uncertainty associated with a given distribution</strong> &ndash; <strong>it measures how much information is required, on average, to identify random samples from that distribution.</strong> <strong>Cross entropy</strong> can be used to define a <strong>loss function in machine learning and optimization</strong>. Cross entropy is related to <strong>log-likelihood</strong>: <strong>maximizing the likelihood is the same as minimizing the cross-entropy.</strong></p>
</li>
<li>
<p><strong>BLEU</strong> (bilingual evaluation understudy) is an algorithm for <strong>evaluating the quality of the text which has been machine-translated from one natural language to another.</strong> The range of this metric is from 0.0 (a perfect translation mismatch) to 1.0 (a perfect translation match).</p>
</li>
<li>
<p>For <strong>stochastic gradient descent</strong>, the batch size equals <strong>1</strong>.</p>
<p>For <strong>batch gradient descen</strong>t, the <strong>batch size equals the size of the training set</strong>.</p>
<p>And, for <strong>mini-batch gradient descent,</strong> the batch size is greater than 1 but less than the size of the training set.</p>
</li>
<li>
<p>The small <strong>batch size</strong> can result:</p>
<pre><code>  1) **Faster updates in the model weights**

  2) Noise and oscillations in the training process, which **might be able to escape the local minima**
</code></pre>
</li>
<li>
<p>For missing data, <strong>Deep learning</strong> is better suited to the imputation of categorical data. Square footage is <strong>numerical, which is better served by kNN.</strong></p>
</li>
<li>
<p>In classifications tasks with <strong>imbalanced</strong> class distributions, we should prefer <strong>StratifiedKFold</strong> over <strong>KFold</strong>.</p>
</li>
</ol>
<h2 id="sagemaker-service-overview">SageMaker Service Overview</h2>
<h3 id="data-copy-from-s3-to-training-instance">Data Copy from S3 to Training Instance</h3>
<p><strong>File Mode:</strong></p>
<ul>
<li>Training job copies entire dataset from S3 to training instance beforehand</li>
<li>Space Needed: Entire data set + Final model artifacts</li>
<li>slower than Pipe mode</li>
<li>used for incremental training</li>
</ul>
<p><strong>Pipe Mode</strong>:</p>
<ul>
<li>Training job streams data from S3 to training instance</li>
<li>Faster start time and Better Throughput</li>
<li>Space Needed: Final model artifacts</li>
<li>You <strong>MUST</strong> use protobuf RecordIO as your training data format before you can take advantage of the Pipe mode.</li>
</ul>
<h3 id="data-format-in-sagemaker">Data Format in SageMaker</h3>
<h4 id="training-data-format">Training Data Format</h4>
<p>CSV</p>
<p>RecordIO: Data types needs to be int32, float 32, float 64</p>
<p>Algorithm Specipic formats( LibSVM, JSON, Parquet)</p>
<p>Data needs to be stored in S3</p>
<table>
<thead>
<tr>
<th align="left">ContentTypes for Built-in Algorithms</th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">ContentType</td>
<td align="left">Algorithm</td>
</tr>
<tr>
<td align="left">application/x-image</td>
<td align="left">Object Detection Algorithm, Semantic Segmentation</td>
</tr>
<tr>
<td align="left">application/x-recordio</td>
<td align="left">Object Detection Algorithm</td>
</tr>
<tr>
<td align="left">application/x-recordio-protobuf</td>
<td align="left">Factorization Machines, K-Means, k-NN, Latent Dirichlet Allocation, Linear Learner, NTM, PCA, RCF, Sequence-to-Sequence</td>
</tr>
<tr>
<td align="left">application/<strong>jsonlines</strong></td>
<td align="left">BlazingText, DeepAR</td>
</tr>
<tr>
<td align="left">image/<strong>jpeg</strong></td>
<td align="left">Object Detection Algorithm, Semantic Segmentation</td>
</tr>
<tr>
<td align="left">image/<strong>png</strong></td>
<td align="left">Object Detection Algorithm, Semantic Segmentation</td>
</tr>
<tr>
<td align="left">text/<strong>csv</strong></td>
<td align="left">IP Insights, K-Means, k-NN, Latent Dirichlet Allocation, Linear Learner, NTM, PCA, RCF, XGBoost</td>
</tr>
<tr>
<td align="left">text/<strong>libsvm</strong></td>
<td align="left">XGBoost</td>
</tr>
</tbody>
</table>
<h4 id="infetence-format">Infetence Format</h4>
<p>CSV</p>
<p>JSON</p>
<p>RecordIO</p>

    <img src="/1.png"  class="center"  style="border-radius: 8px;"  />


<h2 id="sagemaker-build-in-algos">SageMaker Build-in Algos</h2>
<h4 id="blazingtext"><strong>BlazingText</strong></h4>
<p>Unsupervised -&gt; <strong>word2vec</strong></p>
<ul>
<li>called <em>word embeding</em></li>
<li>has multiple modes:
<ul>
<li>Chow(continuous bag of words)</li>
<li>Skip-gram</li>
<li>Batch-skin-gram</li>
</ul>
</li>
</ul>
<p><strong>Supervised</strong> -&gt; multiclass, multilebel classification</p>
<ul>
<li>It&rsquo;a useful for NLP, but it is not a NLP algo.</li>
</ul>
<h5 id="data-type">Data Type</h5>
<ul>
<li>
<p><strong>Supervised</strong> mode(text classification): one sentence per line; first word is the string <code>_label_</code> followed by the label.</p>
</li>
<li>
<p>word2vec wants a text file with <strong>one training sentence per line.</strong></p>
</li>
<li>
<h6 id="training-and-validation-data-format-for-the-word2vec-algorithm">Training and Validation Data Format for the Word2Vec Algorithm</h6>
<p>For <strong>Word2Vec</strong> training, upload the file under the <em><strong>train</strong></em> channel. No other channels are supported. The <strong>file should contain a training sentence per line.</strong></p>
</li>
<li>
<h6 id="training-and-validation-data-format-for-the-text-classification-algorithm">Training and Validation Data Format for the Text Classification Algorithm</h6>
<p>For supervised mode, you can train with file mode or with the augmented manifest text format.</p>
</li>
<li>
<p>Train with File Mode</p>
<p>For <code>supervised</code> mode, the training/validation file should contain a training sentence per line along with the labels. Labels are words that are prefixed by the string <em><strong>label</strong></em>. Here is an example of a training/validation file:</p>
</li>
</ul>
<pre><code>__label__4  linux ready for prime time , intel says , despite all the linux hype , the open-source movement has yet to make a huge splash in the desktop market . that may be about to change , thanks to chipmaking giant intel corp .

__label__2  bowled by the slower one again , kolkata , november 14 the past caught up with sourav ganguly as the indian skippers return to international cricket was short lived .
</code></pre><p>​	<strong>Note</strong></p>
<p>​	The order of labels within the sentence doesn&rsquo;t matter.</p>
<p>​	Upload the training file under the train channel, and optionally upload the validation file under the 	validation channel.</p>
<h6 id="train-with-augmented-manifest-text-format">Train with Augmented Manifest Text Format</h6>
<p>The supervised mode also supports the augmented manifest format, which enables you to do training in <strong>pipe</strong> mode without needing to create RecordIO files. While using the format, an S3 manifest file needs to be generated that contains the list of sentences and their corresponding labels. The manifest file format should be in <a href="http://jsonlines.org/">JSON Lines</a> format in which each line represents one sample. The sentences are specified using the <code>source</code> tag and the label can be specified using the <code>label</code> tag. Both <code>source</code> and <code>label</code> tags should be provided under the <code>AttributeNames</code> parameter value as specified in the request.</p>
<pre><code>{&quot;source&quot;:&quot;linux ready for prime time , intel says , despite all the linux hype&quot;, &quot;label&quot;:1}
{&quot;source&quot;:&quot;bowled by the slower one again , kolkata , november 14 the past caught up with sourav ganguly&quot;, &quot;label&quot;:2}
</code></pre><p>Multi-label training is also supported by specifying a JSON array of labels.</p>
<pre><code>{&quot;source&quot;:&quot;linux ready for prime time , intel says , despite all the linux hype&quot;, &quot;label&quot;: [1, 3]}
{&quot;source&quot;:&quot;bowled by the slower one again , kolkata , november 14 the past caught up with sourav ganguly&quot;, &quot;label&quot;: [2, 4, 5]}
</code></pre><h5 id="hyperparameters">Hyperparameters</h5>
<ul>
<li>Word2vec:
<ul>
<li><code>mode</code>( <code>batch_skipgram</code>, <code>skipgram</code>, <code>cbow</code>)</li>
<li><code>learning_rate</code></li>
<li><code>window_size</code></li>
<li><code>vector_dim</code></li>
<li><code>negative_samples</code></li>
</ul>
</li>
<li>Text Classification
<ul>
<li><code>epochs</code></li>
<li><code>learning_rate</code></li>
<li><code>word_ngrams</code></li>
<li><code>vector_dim</code></li>
</ul>
</li>
</ul>
<h5 id="instance-types">Instance Types</h5>
<ul>
<li>For cbow and skip gram: any single CPU or GPU works (single ml.p3.2xlarge)</li>
<li>For batch_skipgram, can use <strong>single or multi</strong> CPU</li>
<li>For text classification: C5 if less than 2GB training data. For larger datasets, use a single GPU</li>
</ul>
<h4 id="object2vec"><strong>Object2Vec</strong></h4>
<p>Supervised -&gt; Classification, Regression</p>
<p>Unsupervised</p>
<h5 id="data-types">Data Types</h5>
<p>data must be tokenized into integers; training data consists of pairs/sequence of tokens: sentence - sentence, labels - sentences, customer - customer, product - product, user - item</p>
<h5 id="hyperparameters-1">Hyperparameters</h5>
<ul>
<li>Usual ones</li>
<li><code>enc1-network</code>, <code>enc2_network</code> - choose hcnn, bilstm, pooled_embedding&hellip;</li>
</ul>
<h5 id="instance-type">Instance Type</h5>
<ul>
<li>can only train on a single machine( GPU or CPU)</li>
<li>Inference: use GPUs(ml.p2.2xlarge). Use <code>INFERENCE_PREDDERED_MODE</code>to optimize for encoder embeddings rather than clssification or regression</li>
</ul>
<h4 id="factorization-machines"><strong>Factorization Machines</strong></h4>
<p>Supervised -&gt; Classification, Regression</p>
<p>Dealing with sparse data</p>
<ul>
<li>Click prediction</li>
<li>Item recommendations</li>
</ul>
<p>Limited tp pair-wise interactions: user -&gt; items for example</p>
<p>Can use to predict following things gavin a matrix representing some pairs of things( users &amp; items )</p>
<ul>
<li>classification( click or not? Purchase or not? )</li>
<li>Value(predicted rating)</li>
</ul>
<p>Usually used in the context of recommender system</p>
<h5 id="data-types-1">Data Types</h5>
<ul>
<li>recordIO-protobuf with float32</li>
<li>sparse data means CSV <strong>isn&rsquo;t</strong> practical</li>
</ul>
<h5 id="instance-type-1">Instance Type</h5>
<ul>
<li>GPU or CPU</li>
<li>CPU recommended</li>
<li>GPU only works with dense data</li>
</ul>
<h4 id="heading"></h4>
<h4 id="k-nearest-neighbors-knn"><strong>K-Nearest Neighbors, KNN</strong></h4>
<p>Supervised -&gt; Classification, Regression</p>
<h5 id="data-types-2">Data Types</h5>
<p>recordIO-protobuf or CSV, first column is label</p>
<p>File or Pipe mode on either</p>
<h5 id="hyperparameters-2">Hyperparameters</h5>
<ul>
<li>K!</li>
<li><code>sample_size</code></li>
</ul>
<h5 id="instance-type-2">Instance Type</h5>
<ul>
<li>Training on CPU or GPU</li>
<li>Inference:
<ul>
<li>CPU for lower latency</li>
<li>GPU for higher throughput on large batches</li>
</ul>
</li>
</ul>
<h4 id="linear-learner"><strong>Linear Learner</strong></h4>
<p>Supervised -&gt; Classification, Regression</p>
<h5 id="data-types-3">Data Types</h5>
<p>recordio-protobuf float32, csv</p>
<h5 id="processsing">Processsing</h5>
<ul>
<li>Training data must be <em>normalized</em>(all features weighted the same),</li>
<li>input data should be <em>shuffled</em></li>
</ul>
<h5 id="training">Training</h5>
<ul>
<li>uses stochastic gradient descent</li>
<li>Choose an optimization algo (Adam, Adagrad, SGD,&hellip;)</li>
<li>Multiple models are optimized in parallel</li>
<li>Tune L1, L2 regularization</li>
</ul>
<h5 id="validation">Validation</h5>
<ul>
<li>most optimal model is selected</li>
</ul>
<h5 id="hyperparameters-3">Hyperparameters</h5>
<ul>
<li><code>Balance_mulyiclass_weights</code>
<ul>
<li>Gives each class equal importance in loss functions</li>
</ul>
</li>
<li><code>Learning_rate</code>, <code>mini_batch_size</code></li>
<li><code>L1</code></li>
<li><code>wd</code>
<ul>
<li>Weight decay (L2 Regularization)</li>
</ul>
</li>
</ul>
<h5 id="instance-types-1">Instance Types</h5>
<ul>
<li>single or multiple-machine CPU or GPU</li>
</ul>
<h4 id="xgboost"><strong>XGBoost</strong></h4>
<p>Supervised -&gt; Classification, Regression</p>
<h5 id="data-types-4">Data Types</h5>
<p>Csv, libsvm, recordIO-protobuf, parquet</p>
<h5 id="instance-types-2">Instance Types</h5>
<ul>
<li>Use <strong>CPUs only for multiple</strong> instance training; memory-bound, not compute bound; M5 is a good choice.</li>
<li>Use GPUs for single-instance training; like P3; must set <code>tree_method</code> to <code>gpu_hist</code></li>
</ul>
<h4 id="deepar"><strong>DeepAR</strong></h4>
<p>Supervised -&gt; Timeseries Forecasting</p>
<p>Uses RNN&rsquo;s</p>
<p>Allows you to train the same model over several related timeseries.</p>
<p>Find frequencies and seasonality.</p>
<h5 id="data-types-5">Data Types</h5>
<ul>
<li>JSON lines format(GZIP or Parquet)</li>
<li>Each record must contain:
<ul>
<li>start: the starting timestamp</li>
<li>target: the timeseries values</li>
</ul>
</li>
<li>Each record can contain:
<ul>
<li><code>Dynamic_feat</code>: dynamic features</li>
<li><code>ᓚᘏᗢ</code> categorical features</li>
</ul>
</li>
</ul>
<h5 id="hyperparameters-4">Hyperparameters</h5>
<ul>
<li><code>Contaxt_length</code> number of time points the model sees before making a prediction; can be smaller than seasonalities -  the model will lag one year anyhow.</li>
<li><code>epoches</code></li>
<li><code>mini_batch_size</code></li>
<li><code>learning_rate</code></li>
<li><code>num_cells</code></li>
</ul>
<h5 id="instance-types-3">Instance Types</h5>
<ul>
<li>Can use CPU or GPU, single or multi machine</li>
<li>CPU only for inference</li>
</ul>
<h4 id="object-detection"><strong>Object Detection</strong></h4>
<p>Supervised -&gt; Classification</p>
<p>Takes in images, output all instances of objects with categories and confidence scores</p>
<p>Transfer learning mode/ incremental training.</p>
<h5 id="data-types-6">Data Types</h5>
<ul>
<li>RecordIO or image format (jpg, png)</li>
<li>with image format, supply a JSON file for annotation data for each image</li>
</ul>
<h5 id="hyperparameters-5">Hyperparameters</h5>
<ul>
<li>Usual ones</li>
<li>Optimizer ( sgd, adam, rmsprop, adadelta&hellip;)</li>
</ul>
<h5 id="instance-type-3">Instance Type</h5>
<ul>
<li>Use GPU for training</li>
<li>Use CPU or GPU for inference</li>
</ul>
<h4 id="image-classification"><strong>Image Classification</strong></h4>
<p>Supervised -&gt; Classification</p>
<p><strong>What objects are in the image</strong></p>
<p>Resnet CNN under the hood.</p>
<p>Full training mode: network initialized with random weights</p>
<p><strong>Transfer learning</strong> mode: initialized with pre-trained weights; the top fully-connected layer is initialized with random weights</p>
<h5 id="data-types-7">Data Types</h5>
<ul>
<li>Apache MXNet RecordIO (Not PRotobuf)</li>
<li>Raw jpg or png</li>
<li>image format requires <code>.lst</code> files to associate image index, class label and path to the image</li>
<li>Augmented Manifest Image Format enables <strong>Pipe</strong> Mode</li>
</ul>
<h5 id="hyperparameters-6">Hyperparameters</h5>
<ul>
<li>Usual ones</li>
<li>Optimizer ( weight decay, beta1, beta2, eps, gamma&hellip;)</li>
</ul>
<h5 id="instance-type-4">Instance Type</h5>
<ul>
<li>Use GPU for training</li>
<li>Use CPU or GPU for inference</li>
</ul>
<h4 id="semantic-segmentation"><strong>Semantic Segmentation</strong></h4>
<p>Supervised -&gt; Classification</p>
<p>it can detect objects in an image, shape of each object along with location and <strong>pixels</strong> that are part of the object.</p>
<p>Useful for self-driving vehicles.</p>
<h5 id="data-types-8">Data Types</h5>
<ul>
<li>jpg images and png annotations</li>
<li>jpg images accepted for inference</li>
<li>Label maps to describe annotations</li>
<li>Augmented Manifest Image Format enables <strong>Pipe</strong> Mode</li>
</ul>
<h5 id="hyperparameters-7">Hyperparameters</h5>
<ul>
<li>Usual ones</li>
<li>blackbone</li>
</ul>
<h5 id="instance-type-5">Instance Type</h5>
<ul>
<li>Only <strong>GPU</strong> for training on a <strong>single</strong> machine only</li>
<li>Use CPU or GPU for <strong>inference</strong></li>
</ul>
<h4 id="seq2seq"><strong>Seq2Seq</strong></h4>
<p>Supervised -&gt; Convert seq of tokens to another seq to tokens</p>
<p>Seq2Seq algorithm is used for text summarization – It accepts a series of tokens as input and outputs another sequence of tokens.</p>
<h5 id="data-types-9">Data Types</h5>
<p>RecordIO-protobuf(<strong>must be int</strong>), start with tokenized text files</p>
<p>Must provide training, validation and vocabulary data.</p>
<h5 id="hyperparameters-8">Hyperparameters</h5>
<ul>
<li><code>batch_size</code></li>
<li><code>optimizer_type</code>(Adam,sgd, rmsprop)</li>
<li><code>learning_rate</code></li>
<li><code>num_layers_encoder</code>, <code>num_layers_decoder</code></li>
<li>Can optimize on:
<ul>
<li>accuracy(vs provided validation dataset)</li>
<li><strong>BLEU score</strong>(compares against multiple reference translations)</li>
<li><strong>Perplexit</strong>y(cross-entropy)</li>
</ul>
</li>
</ul>
<h5 id="instance-types-4">Instance Types</h5>
<ul>
<li>
<p>Can only use GPU instance types.</p>
</li>
<li>
<p>Can only use a <strong>single</strong> machine for training, bur can use multi-GPS&rsquo;s on one machine</p>
</li>
</ul>
<h4 id="k-means"><strong>K-Means</strong></h4>
<p>Unsupervised -&gt; clsutering</p>
<h5 id="data-types-10">Data Types</h5>
<ul>
<li>Two data channels: <code>train</code> is required, <code>test</code> optional
<ul>
<li>Train <code>sharedByS3Key</code>, test <code>FullyReplicated</code></li>
</ul>
</li>
<li>recordIO-protobuf or CSV</li>
<li>File or Pipe on either</li>
</ul>
<h5 id="hyperparameters-9">Hyperparameters</h5>
<ul>
<li><code>K!</code>
<ul>
<li>Plot within-cluster sum of squares as function of K</li>
<li>Use <code>elbow method</code></li>
<li>optimize for tightness of clusters</li>
</ul>
</li>
<li><code>mini_batch_size</code></li>
<li><code>extra_center_factor</code></li>
<li><code>init_method</code></li>
</ul>
<h5 id="instance-type-6">Instance Type</h5>
<ul>
<li>CPU recommended</li>
</ul>
<h4 id="lda"><strong>LDA</strong></h4>
<p>Unsupervised -&gt; Topic Modeling (Document level)</p>
<p>need to Define how many topics you want CPU based</p>
<h5 id="data-types-11">Data Types</h5>
<ul>
<li>Two data channels: <code>train</code> is required, <code>test</code> optional</li>
<li>recordIO-protobuf or CSV</li>
<li>words must be tokenized into int.
<ul>
<li>every document must contain a count for every word in the vocab in csv</li>
</ul>
</li>
<li>Pipe mode only supported with recordIO</li>
</ul>
<h5 id="hyperparameters-10">Hyperparameters</h5>
<ul>
<li><code>num_topics</code></li>
<li><code>alpha0</code>
<ul>
<li>initial guess for concentration parameter</li>
<li>smaller values generate sparse topic mixtures</li>
</ul>
</li>
</ul>
<h5 id="instance-type-7">Instance Type</h5>
<ul>
<li>Single instance CPU training</li>
</ul>
<h4 id="neural-topic-modelntm"><strong>Neural Topic Model(NTM)</strong></h4>
<p>Unsupervised -&gt; Topic modeling, similiar to LDA</p>
<p>need to Define how many topics you want</p>
<h5 id="data-types-12">Data Types</h5>
<ul>
<li>Four data channels: <code>train</code> is required, <code>validation</code>, <code>test</code>, <code>auxiliary</code> optional</li>
<li>recordIO-protobuf or CSV</li>
<li>words must be tokenized into int.
<ul>
<li>every document must contain a count for every word in the vocab in csv</li>
<li>the <code>auxiliary</code> channel is for the vocab.</li>
</ul>
</li>
<li>File or Pipe mode</li>
</ul>
<h5 id="hyperparameters-11">Hyperparameters</h5>
<ul>
<li><strong>lowering <code>mini_batch_size</code> and <code>learning_rate</code></strong> can reduce validation loss  - at expense of training time</li>
<li><code>num_topics</code></li>
</ul>
<h5 id="instance-type-8">Instance Type</h5>
<p>GPU or CPU</p>
<ul>
<li>GPU recommended for training</li>
<li>CPU ok for inference</li>
</ul>
<h4 id="pca-principal-component-analysis"><strong>PCA: Principal Component Analysis</strong></h4>
<p>Unsupervised -&gt; Dimensioality reduction</p>
<h5 id="data-types-13">Data Types</h5>
<ul>
<li>recordIO-protobuf or CSV</li>
<li>File or Pipe mode on either</li>
</ul>
<h5 id="hyperparameters-12">Hyperparameters</h5>
<ul>
<li><code>Algorithm_mode</code></li>
<li><code>subtract_mean</code></li>
</ul>
<h5 id="instance-type-9">Instance Type</h5>
<ul>
<li>GPU or CPU</li>
</ul>
<h4 id="random-cut-forest"><strong>Random Cut Forest</strong></h4>
<p>Unsupervised -&gt; <strong>anomaly detection</strong></p>
<p>Data is sampled randomly</p>
<p>shows up in Kinesis Analytics as well, it works on streaming data too.</p>
<h5 id="data-types-14">Data Types</h5>
<ul>
<li>RecordIO-protobuf or CSV</li>
<li>Can use File or Pipe mode on either</li>
</ul>
<h5 id="hyperparameters-13">Hyperparameters</h5>
<ul>
<li><code>num_trees</code>: increasing reduces noise</li>
<li><code>num_samples_per_tree</code>: should be chosen such that 1/num_samples_per_tree appox the ratio of anomalous to normal data</li>
</ul>
<h5 id="instance-type-10">Instance Type</h5>
<ul>
<li><strong>CPU</strong> for training</li>
<li>Use CPU <strong>inference</strong> ( ml.c5.xl)</li>
</ul>
<h4 id="ip-insights"><strong>IP Insights</strong></h4>
<p>Unsupervised -&gt; Detect unusual network activity</p>
<p>automatically generates negative samples during training by random pairing entites and IPs</p>
<h5 id="data-types-15">Data Types</h5>
<ul>
<li>CSV only - entity and IP</li>
</ul>
<h5 id="hyperparameters-14">Hyperparameters</h5>
<ul>
<li><code>num_entity_vectors</code>:
<ul>
<li>Hash size</li>
<li>set to twice the number of unique entity identifiers</li>
</ul>
</li>
<li><code>vectoe_dim</code>:
<ul>
<li>size of embedding vectors</li>
<li>Scales model size</li>
<li>too large results in overfitting</li>
</ul>
</li>
<li>Epochs learning_rate, batch_size, &hellip;</li>
</ul>
<h5 id="instance-type-11">Instance Type</h5>
<ul>
<li>CPU or GPU</li>
<li>GPU recommended</li>
<li>can use multiple GPUs</li>
<li>size of CPU instance depends on <code>vector_dim</code> and <code>num_entity_vectors</code></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Specialty Cheatsheet(2)</title>
            <link>https://ffflora.cat/posts/2021/11/aws-machine-learning-specialty-cheatsheet2/</link>
            <pubDate>Wed, 03 Nov 2021 14:34:57 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/11/aws-machine-learning-specialty-cheatsheet2/</guid>
            <description>Exploratory Data Analysis   In case of a binary classification model with strongly unbalanced classes, we need to over-sample from the minority class, collect more training data for the minority class and create more samples using algorithms such as SMOTE which effectively uses a k-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. Over-sampling from the positive class or collecting more training data for the positive class would further aggravate the situation.</description>
            <content type="html"><![CDATA[<h1 id="exploratory-data-analysis">Exploratory Data Analysis</h1>
<ol>
<li>
<p>In case of a binary classification model with strongly unbalanced classes, we need to over-sample from the minority class, collect more training data for the minority class and create more samples using algorithms such as SMOTE which effectively uses a k-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. Over-sampling from the positive class or collecting more training data for the positive class would further aggravate the situation.</p>
<p><a href="http://www.svds.com/learning-imbalanced-classes/">http://www.svds.com/learning-imbalanced-classes/</a></p>
<p><a href="https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes">https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes</a></p>
</li>
<li>
<p>A data <strong>warehouse</strong> can only store <strong>structured</strong> data whereas a data lake can store structured, semi-structured and unstructured data.</p>
<p><strong><!-- raw HTML omitted -->Data lakes<!-- raw HTML omitted --></strong> provides schema on <strong>read</strong> access, whereas <!-- raw HTML omitted --><strong>data warehouse</strong><!-- raw HTML omitted --> provides schema on <strong>write</strong>.</p>
</li>
<li>
<p>Great reference for the most common probability distributions: <a href="https://medium.com/@srowen/common-probability-distributions-347e6b945ce4">Common Probability Distributions: The Data Scientist’s Crib Sheet</a></p>
<p>The <strong>Rademacher</strong> distribution takes value 1 with probability 1/2 and value −1 with probability 1/2. The <strong>degenerate</strong> distribution is localized at a point x0, where x is certain to take the value x_0. The probability mass function equals 1 at this point and 0 elsewhere.</p>
</li>
<li>
<p>Tf-idf is a statistical technique frequently used in Machine Learning areas such as text-summarization and classification. Tf-idf measures the <strong>relevance of a word in a document compared to the entire corpus of documents</strong>. You have a corpus (D) containing the following documents:</p>
<p>Document 1 (d1) : “A quick brown fox jumps over the lazy dog. What a fox!”</p>
<p>Document 2 (d2) : “A quick brown fox jumps over the lazy fox. What a fox!”</p>
<p>Which of the following statements is correct:</p>
<ul>
<li>Using tf-idf, the word “fox” is equally relevant for both document d1 and document d2</li>
</ul>
<p><strong>tf</strong> is the frequency of any &ldquo;term&rdquo; in a given &ldquo;document&rdquo;. Using this definition, we can compute the following:</p>
<p>tf(“fox”, d1) = 2/12 , as the word &ldquo;fox&rdquo; appears twice in the first document which has a total of 12 words</p>
<p>tf(“fox”, d2) = 3/12 , as the word &ldquo;fox&rdquo; appears thrice in the second document which has a total of 12 words</p>
<p>An idf is constant per corpus (in this case, the corpus consists of 2 documents) , and accounts for the ratio of documents that include that specific &ldquo;term&rdquo;. Using this definition, we can compute the following:</p>
<p>idf(“fox”, D) = log(2/2) = 0 , as the word &ldquo;fox&rdquo; appears in both the documents in the corpus</p>
<p>Now,</p>
<p>tf-idf(“fox”, d1, D) = tf(“fox”, d1) * idf(“fox”, D) = (2/12) * 0 = 0</p>
<p>tf-idf(“fox”, d2, D) = tf(“fox”, d2) * idf(“fox”, D) = (3/12) * 0 = 0</p>
<p>Using tf-idf, the word “fox” is equally relevant (or just irrelevant!) for both document d1 and document d2</p>
</li>
<li>
<p>TF-IDF: Three different inverse document frequency functions are <strong>standard, smooth, probabilistic:</strong> Standard: log(N)/n_t, Smooth: log(N)/((1+n_t) +1), Probabilistic: log(N-N_t)/n_t, where N is the total number of documents in the corpus, and n_t is a number of documents where the term t appears.</p>
</li>
<li>
<p><strong>Logarithm transformation</strong> and <strong>Standardization</strong> are the correct techniques to address outliers in data. Please review this reference link:</p>
<p><a href="https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114">https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114</a></p>
</li>
<li>
<p>ElasticSearch, EMR and EC2 are <strong>not</strong> “serverless”.</p>
</li>
<li>
<p>The best way to engineer the <strong>cyclical</strong> features is to represent these as (x,y) coordinates on a circle using sin and cos functions. Please review this technique in more detail here -</p>
<p><a href="http://blog.davidkaleko.com/feature-engineering-cyclical-features.html">http://blog.davidkaleko.com/feature-engineering-cyclical-features.html</a></p>
</li>
<li>
<p>Q1 = 1/4 x (N+1)th term</p>
<p>Q3 = 3/4 x (N+1)th term</p>
<p>Interquartile Range (IQR) = Q3-Q1</p>
<p>Minimum outlier cutoff = Q1 - 1.5 * IQR</p>
<p>Maximum outlier cutoff = Q3 + 1.5 * IQR</p>
<p>More details on the box plot statistical characteristics:</p>
<p><a href="https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51">https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51</a></p>
</li>
<li>
<p>The <strong>Box Plot and Violin Plot</strong> are used to summarize multivariate distributions. They are a standardized way of displaying the data distributions based on a <strong>five-number summary (minimum, first quartile (Q1), median, third quartile (Q3), and maximum).</strong> The plots show symmetry, tightness of the groups, skewness, and any outliers present.</p>
</li>
<li>
<p>The Multiple Imputations by Chained Equations (<strong>MICE</strong>) algorithm is a robust, informative method of dealing with <strong>missing data</strong> in your datasets. This procedure imputes or &lsquo;fills in&rsquo; the missing data in a dataset through an iterative series of predictive models. Each specified variable in the dataset is imputed in each iteration using the other variables in the dataset. These iterations will be run continuously until convergence has been met. In General, MICE is a better imputation method than naive approaches (filling missing values with 0, dropping columns).</p>
</li>
<li>
<p>QuickSight supports UTF-8 file encoding, <strong>but not UTF-8 (with BOM)</strong>.</p>
</li>
<li>
<p>Quicksight supports the following file formats only: <strong>CSV/TSV, ELF/CLF, JSON, XLSX</strong>.</p>
</li>
<li>
<p>For QuickSight, AWS Enterprise Edition Authors (create &amp; publish) pay $18/month with an annual subscription, while Readers (getting secure access to interactive dashboards) pay $0.30/session up to $5/month&rsquo;.</p>
</li>
<li>
<p><strong>F0.5-Measure</strong> (beta=0.5): More weight on precision, less weight on recall.</p>
<p><strong>F1-Measure</strong> (beta=1.0): Balance the weight on precision and recall.</p>
<p><strong>F2-Measure</strong> (beta=2.0): Less weight on precision, more weight on recall</p>
<p><img src="https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-12-06_07-55-05-6516021657c5e962ac6ca87cdd939d92.jpg" alt="img"></p>
</li>
<li>
<p>The <!-- raw HTML omitted --><em>Recall</em><!-- raw HTML omitted --> is also called <strong>Sensitivity, Hit Rate, and True Positive Rate.</strong></p>
<p><strong>Positive Predictive Value (PPV)</strong> is the same as <!-- raw HTML omitted --><em>Precision</em><!-- raw HTML omitted -->.</p>
</li>
<li>
<p>The Receiver Operating Characteristic - <strong>ROC</strong>, <strong>true positive rate &amp; false positive rate</strong> - determines the ability of a binary classification model, as its discrimination threshold is varied.</p>
</li>
<li>
<p><strong>False positive Rate</strong> = 1 - TNR(True negative rate) = 1 - specifiticy</p>
</li>
<li>
<p><strong>Specificity</strong> = TN/(TN+FP)</p>
</li>
<li>
<p>If the model has a high specificity, it implies that all false positives (think of it as false alarms) have been weeded out. In other words, the <strong>specificity</strong> of a test refers to how well the test identifies those who have not indulged in substance abuse.</p>
<p>Please read this excellent reference article for more details:</p>
<p><a href="https://www.statisticshowto.datasciencecentral.com/sensitivity-vs-specificity-statistics/">https://www.statisticshowto.datasciencecentral.com/sensitivity-vs-specificity-statistics/</a></p>
</li>
<li>
<p>As per sklearn, <strong>the minority class is considered as the positive class.</strong> Hence, in cases with <strong>fraudulent</strong> data, a fraud transaction is considered as a positve class. Similary, in diagnostics, a disease detected is considered positive.</p>
</li>
<li>
<p><strong>Type 2</strong> error is also known as <strong>False Negative.</strong></p>
<p>A Null hypothesis assumes positive for no-change/default (No Fraud, Healthy, Not Guilty), and a negative for change/non-default (Not Healthy, Fraud, Guilty) outcome.</p>
<p>A type 2 error occurs when the null hypothesis is false but is falsely accepted. This corresponds to the False-negative in classification, where a negative is considered for no-change/default (No Fraud, Healthy, Non-Guilty), etc.</p>
<p>A type 1 error occurs when the null hypothesis is true but is falsely rejected.</p>
</li>
<li>
<p><strong>Miss Rate is also known as the False Negative Rate</strong>. It is given as FN/FN+TP (FN =False Negative, TP = True Positive).</p>
<p>As the False Negatives are undesired and should be reduced to zero for an ideal model, the value of the miss rate in the ideal case will approach zero.</p>
</li>
<li>
<p>If there are <!-- raw HTML omitted -->no outliers,<!-- raw HTML omitted --> <strong>MAE</strong> (Mean Absolute Error) will be more suitable  for comparison of performances of various models, as the error remains linear in this case. And if there <!-- raw HTML omitted -->are outliers<!-- raw HTML omitted --> <strong>RMSE</strong> will be preferred.</p>
</li>
<li>
<p><strong>Ground truth</strong> provides built-in five data labelling tasks</p>
<pre><code>```
Bounding Boxes
Image classification
Semantic segmentation
Text classification
Named Entity Recognition.
```
</code></pre>
</li>
<li>
<p>The <em>p</em> value represents the level of probability that an apparently significant relationship between variables was really just due to chance. <!-- raw HTML omitted -->If <em>p</em> is set at 0.01, this means that we would expect such a result in only 1 in 100 cases<!-- raw HTML omitted -->. This is a very stringent level, and while it means that the researcher can be more confident about a significant result if they find one, <strong>it also increases the chance of making a Type II error: confirming the null hypothesis when it should be rejected.</strong></p>
</li>
<li>
<p>Adoptive (or q-quantile) binning helps in partitioning a numeric attribute into &lsquo;q&rsquo; equal partitions.  Adoptive binning leads to discrete-valued categorical features transforming <strong>numerical data into ordinal data.</strong></p>
</li>
<li>
<p>95% of the measurements fall between +/- 2 standard deviations around the mean.</p>
</li>
</ol>
<h2 id="model-performance-evaluation">Model Performance Evaluation</h2>
<h3 id="regression-model">Regression Model</h3>
<ol>
<li>MSE</li>
<li>RMSE</li>
</ol>
<p><strong>Residual</strong> is Actual - Predicted.</p>
<p><strong>MSE</strong> is the mean value of (sumation of each residual^2)</p>
<p><strong>RMSE</strong> is rooted MSE</p>
<h3 id="binary-model-and-multiclass-classifier">Binary Model and Multiclass Classifier</h3>
<p><!-- raw HTML omitted --><!-- raw HTML omitted -->The actual output of many binary classification algorithms is a prediction score. The score indicates the system’s certainty that the given observation belongs to the positive class<!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p>To convert this raw score to a positive or negative class, we need to specify a <strong>cut-off</strong>. A sample with score greater than the cut-off is classified as positive class and a sample with score less than the cut-off is classified as negative class.</p>
<p>Instead of manually performing this step, we can compute &ldquo;AUC&rdquo; metric.  AUC refers to Area Under Curve.  <strong>The curve here refers to the plot that has Probability of False Alarm (False Positive Rate) in X-Axis and Probability of Detection (Recall) in Y-Axis.</strong> By plotting False Alarm vs Recall at different cut-off thresholds, we can form a curve.  It measures the ability of the model to predict a higher score for positive examples as compared to negative examples. Since AUC is independent of the selected threshold, you can get a sense of the prediction performance of your model from the AUC metric without picking a threshold.</p>
<p>Common Techniques for evaluating performance:</p>
<ol>
<li>
<p>Visually observe raw score using Plots</p>
</li>
<li>
<p>Evaluate Area Under Curve (AUC) Metric</p>
</li>
<li>
<p>Confusion Matrix</p>
</li>
</ol>
<h2 id="some-terms">Some Terms</h2>
<p><strong>Early stopping</strong>: the model trains until it stops improving. Early stopping is a <strong>form of regularization used to avoid overfitting when training a learner with an</strong> iterative method</p>
<p><strong>Bias:</strong> does not match the reality.</p>
<p><strong>High bias:</strong> The model doesn&rsquo;t learn from data, and it translate to large training and validation errors. In other words, the model is <strong>underfitting.</strong> Should <strong>decrease</strong> regularizations.</p>
<p><strong>Low bias:</strong> Overfitting.</p>
<p><strong>Variance:</strong> Measures how well the algorithm generalizes from the data, it&rsquo;s the difference between the validation data and training data.</p>
<p><strong>High Variance:</strong> Validation error is high but training error is low: overfitting. Should <strong>increase</strong> regularizations.</p>
<p><strong>Regularization</strong>: tone down the overdependence of specific features.</p>
<p>**L1 Regularization: ** Algorithm aggresively eliminates features that are not important. Useful in large demension dataset - reduce the number of features. L1 gives you sparse estimates.</p>
<p><strong>L2 Regularization:</strong> Algorithm simply reuces the weight of features. It allows other features to influence outcome. L2 gives you dense estimates.</p>
<p><strong>Inference</strong>: The process of using the trained model to make <strong>predictions</strong>.</p>
<p><strong>Normalization:</strong> The normalization transformer normalizes numeric variables to have a mean of zero and variance of one.</p>
]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Specialty Cheatsheet(1)</title>
            <link>https://ffflora.cat/posts/2021/04/aws-machine-learning-specialty-cheatsheet1/</link>
            <pubDate>Tue, 13 Apr 2021 23:26:49 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/04/aws-machine-learning-specialty-cheatsheet1/</guid>
            <description>Data Engineering   In Kinesis Data Stream, number_of_shards = max (incoming_write_bandwidth_in_KB/1000, outgoing_read_bandwidth_in_KB/2000)
where
incoming_write_bandwidth_in_KB = average_data_size_in_KB multiplied by the number_of_records_per_seconds.
outgoing_read_bandwidth_in_KB = incoming_write_bandwidth_in_KB multiplied by the number_of_consumers.
  Glue cannotwrite the output in RecordIO-Protobuf format. Lambda is not suited for long-running processes such as the task of transforming 1TB data into RecordIO-Protobuf format. Kinesis Firehose is not meant to be used for batch processing use cases and it cannot write data in RecorIO-Protobuf format.</description>
            <content type="html"><![CDATA[<h1 id="data-engineering">Data Engineering</h1>
<ol>
<li>
<p>In <!-- raw HTML omitted -->Kinesis Data Stream<!-- raw HTML omitted -->, number_of_shards = max (incoming_write_bandwidth_in_KB/<strong>1000</strong>, outgoing_read_bandwidth_in_KB/<strong>2000</strong>)</p>
<p>where</p>
<p>incoming_write_bandwidth_in_KB = average_data_size_in_KB multiplied by the number_of_records_per_seconds.</p>
<p>outgoing_read_bandwidth_in_KB = incoming_write_bandwidth_in_KB multiplied by the number_of_consumers.</p>
</li>
<li>
<p><strong>Glue <!-- raw HTML omitted -->cannot<!-- raw HTML omitted --> write the output in RecordIO-Protobuf forma</strong>t. Lambda is not suited for long-running processes such as the task of transforming 1TB data into RecordIO-Protobuf format. Kinesis <strong>Firehose</strong> is not meant to be used for batch processing use cases and <strong>it cannot write data in RecorIO-Protobuf format</strong>. Apache Spark (running on the EMR cluster in this use-case) can write the output in RecorIO-Protobuf format.</p>
</li>
<li>
<p>Glue&rsquo;s <strong>FindMatches</strong> feature is a new way to perform de-duplication as part of Glue ETL, and is a simple, server-less solution to the problem.</p>
</li>
<li>
<p>AWS Glue does <strong>not</strong> support <em><strong>timestream</strong></em> as the source input type.</p>
</li>
<li>
<p>Converting the data to <strong>recordIO-protobuf</strong> file type can significantly improve the training time with a marginal increase in cost to store the recordIO-protobuf data on <strong>S3</strong>. Spinning up EMR clusters would be costly and require complex infrastructure maintenance.</p>
</li>
<li>
<p>For those SageMaker supervised learning algorithms which require the training data to be in <strong>CSV</strong> format, the target variable should be in the <strong>first column</strong> and it <strong>should not have a header</strong> record.</p>
</li>
<li>
<p>Kinesis Firehose can transform data to <strong>Parquet</strong> format and store it on S3 without provisioning any servers. Also this transformed data can be read into an Athena Table <!-- raw HTML omitted -->via a Glue Crawler<!-- raw HTML omitted --> and then the underlying data is readily available for ad-hoc analysis. Although Glue ETL Job can transform the source data to Parquet format, it is best suited for batch ETL use cases and it’s not meant to process streaming data. EMR cluster is not an option as the company does not want to manage the underlying infrastructure.</p>
</li>
<li>
<p><strong>Athena</strong> performs much more efficiently and at lower cost when using columnar formats such as <strong>Parquet or ORC</strong>, and that Kinesis Firehose has the ability to convert <strong>JSON</strong> <!-- raw HTML omitted -->data to Parquet or ORC format on the fly.<!-- raw HTML omitted --></p>
</li>
<li>
<p><strong>Athena</strong> supports a wide variety of data formats such as <strong>CSV, JSON, ORC, Avro, or Parquet.</strong></p>
</li>
<li>
<p><strong>Athena</strong> automatically executes queries in <strong>parallel</strong>, so that you get query results in seconds, even on large dataset;  Athena leverages <em><strong>Hive</strong></em> for <strong>partitioning</strong> data. There are two types of <strong>cost controls</strong> in Amazon Athena:</p>
<p>– <strong>per-query limit</strong></p>
<p><strong>– per-workgroup limit</strong></p>
<p>If the query in Athena <strong>exceeded the limit</strong>, all the succeeding queries will be canceled. Therefore, the type of cost control you need to use is the <strong>per-query limit or the per query data usage control</strong>. The per-query control limit <strong>specifies the total amount of data scanned per query.</strong> You can create only one per-query control limit in a workgroup, and it applies to each query that runs in it.</p>
</li>
<li>
<p><strong>Athena</strong> integrates with Amazon <strong>QuickSight</strong> for easy data visualization.</p>
</li>
<li>
<p>Kinesis Data Firehose needs the following three elements to convert the input data format to Parquet/ORC format:</p>
<ol>
<li>
<p><strong>Deserializer to read JSON data</strong> (Input must be in JSON only, If not use a lambda to convert to JSON),</p>
</li>
<li>
<p>Schema using <strong>AWS Glue to interpret the input data,</strong></p>
</li>
<li>
<p>Serializer to convert the data to target columnar storage format (Parquet/ORC).</p>
</li>
</ol>
</li>
<li>
<p><strong>A Glue crawler connects to the data store that can be Amazon S3, RDS, Redshift, DynamoDB, or JDBC (Java Database Connectivity Interface).</strong></p>
<p>Amazon ElastiCache is in-memory data stores in the cloud that cannot be connected to Glue. (A caveat: a user can write custom Scala or Python code and import custom libraries and Jar files into Glue ETL jobs to access data sources not natively supported by AWS Glue, like ElastiCache). When you are creating a new Job on the console, you can <strong>specify</strong> one or more <strong>library</strong> <em><strong><code>.zip</code></strong></em> files by choosing <strong>Script Libraries</strong> and job parameters (optional) and <strong>entering the full Amazon S3</strong> library path(s) in the same way you would when creating a development endpoint.</p>
<p><a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-libraries.html">https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-libraries.html</a></p>
</li>
<li>
<p><strong>Kinesis Data Firehose is used for streaming data scenarios</strong>. <strong>AWS Glue ML Transforms job can perform deduplication</strong> in a serverless fashion.</p>
</li>
<li>
<p><strong>Amazon Kinesis Data Firehose</strong> <strong><!-- raw HTML omitted --><em>buffers incoming streaming</em><!-- raw HTML omitted --></strong> data to a certain size or for a certain period of time before delivering it to destinations. You can configure the buffer size and buffer interval while <!-- raw HTML omitted -->creating your <strong>delivery stream</strong>.<!-- raw HTML omitted --></p>
<p>Buffer size is in MBs and ranges from <!-- raw HTML omitted -->1MB to 128MB<!-- raw HTML omitted --> for Amazon S3 destination and <!-- raw HTML omitted --><strong>1MB to 100MB</strong><!-- raw HTML omitted --> for Amazon Elasticsearch Service destination. Buffer interval is in seconds and ranges from <strong>60 seconds to 900 seconds</strong>. Please note that in circumstances where data delivery to destination is falling behind data writing to a delivery stream, Firehose raises buffer size dynamically to catch up and make sure that all data is delivered to the destination.</p>
<p><img src="https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-12-04_16-41-13-3dcfe5855fa5c240d34bedca038dcb9b.JPG" alt="img"></p>
</li>
<li>
<p>Kinesis Firehose Data <strong>Delivery Failure Handing</strong></p>
<p><img src="https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-12-05_02-32-25-005129db5966b762cc65018c53796c0e.JPG" alt=""></p>
</li>
<li>
<p>Firehose can deliver data to <strong>S3, Elastic Search, Splunk, Redshift, and HTTP Endpoints.</strong> Athena is not a database, and Firehose <strong>doesn&rsquo;t</strong> deliver data to DynamoDB.</p>
</li>
<li>
<p>There is no such thing as a <del>LOAD</del> command for <strong>Redshift</strong>. <em><strong>COPY</strong></em> is much faster than <em><strong>INSERT</strong></em>. <em>UNLOAD</em> is used to write the results of a Redshift query to one or more text files on Amazon S3.</p>
</li>
<li>
<p>There are two <strong>in-place query</strong> with S3: <strong>Athena and Redshift Spectrum.</strong></p>
</li>
<li>
<ul>
<li><strong>Athena</strong>: <strong>ad-hoc</strong> data discovery and <strong>SQL</strong> querying.</li>
<li><strong>Redshift Spectrum</strong>: more <strong>complex</strong> queries and <strong>large</strong> number of <strong>uses</strong>.</li>
</ul>
</li>
<li>
<p>Only <strong>GZIP</strong> is supported if the data is loaded to Amazon <strong>Redshift</strong>.</p>
</li>
<li>
<p>Amazon SageMaker <strong>models</strong> are stored as <strong><code>model.tar.gz</code></strong> in the S3 bucket specified in <strong><code>OutputDataConfig</code> <code>S3OutputPath</code></strong> parameter of the <strong><code>create_training_job</code></strong> call. You can specify most of these model artifacts when creating a hosting model. You can also open and review them in your notebook instance. When <code>model.tar.gz</code> is untarred, it contains <code>model_algo-1</code>, which is a serialized Apache MXNet object.</p>
</li>
<li>
<p><strong>Workload management (WLM)</strong> is only available in Amazon <strong>Redshift</strong>.</p>
</li>
<li>
<p>If you want Amazon SageMaker to replicate a subset of data on each ML compute instance that is launched for model training, specify <em><strong><code>ShardedByS3Key</code></strong></em> for <!-- raw HTML omitted --><strong>S3DataDistributionType</strong><!-- raw HTML omitted --> field.</p>
</li>
<li>
<p><strong>KCL</strong> helps you consume and process data from a <strong>Kinesis data stream</strong> by taking care of many of the complex tasks associated with distributed computing. These include load balancing across multiple consumer application instances, responding to consumer application instance failures, checkpointing processed records, and reacting to resharding. The KCL takes care of all of these subtasks so that you can focus your efforts on <strong>writing your custom</strong> record-processing logic.</p>
</li>
<li>
<p>Amazon <strong>KPL</strong> is only used for <!-- raw HTML omitted -->streaming data into Amazon Kinesis Data Streams<!-- raw HTML omitted --> and not for reading data from it.</p>
</li>
<li>
<p><strong>Kinesis Product Library</strong>(KPL) provides built-in performance benefits and ease of use advantages.</p>
<ul>
<li>
<p><strong>Performance Benefits</strong></p>
<p>The KPL can help build high-performance producers.</p>
</li>
<li>
<p><strong>Consumer-Side Ease of Use</strong></p>
<p>For consumer-side developers using the KCL in Java, the KPL integrates without additional effort.</p>
<p>For consumer-side developers who do not use the KCL but instead use the API operation <code>GetRecords</code> directly, a KPL Java library is available to extract the individual user records before returning them to the user.</p>
</li>
<li>
<p><strong>Producer Monitoring</strong></p>
<p>You can collect, monitor, and analyze your Kinesis Data Streams producers using Amazon CloudWatch and the KPL.</p>
</li>
<li>
<p><strong>Asynchronous Architecture</strong></p>
</li>
</ul>
<p><a href="https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html#developing-producers-with-kpl-advantage">https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html#developing-producers-with-kpl-advantage</a></p>
</li>
<li>
<p>Hyperparameters should be tuned against the <strong>Validation</strong> Set.</p>
</li>
<li>
<p>Kinesis cheatsheet <a href="https://tutorialsdojo.com/amazon-kinesis/">https://tutorialsdojo.com/amazon-kinesis/</a></p>
</li>
<li>
<p>Amazon <strong>FSx</strong> for <strong>Lustre</strong> <!-- raw HTML omitted -->speeds up your <strong>File mode</strong> training jobs by serving your Amazon S3 data to Amazon SageMaker at high speeds<!-- raw HTML omitted -->. The first time you run a training job, Amazon FSx for Lustre automatically copies data from Amazon S3 and makes it available to Amazon SageMaker. Additionally, the same Amazon FSx file system can be used for subsequent iterations of training jobs on Amazon SageMaker, preventing repeated downloads of common Amazon S3 objects. Because of this, Amazon FSx has the most benefit to training jobs that <strong>have training sets in Amazon S3 and in workflows where training jobs must be run several times using different training algorithms or parameters to see which gives the best result.</strong></p>
</li>
<li>
<p>Data storage for machine learning modeling could be from <strong>S3</strong>, <strong>Amazon Fsx for Lustre, and Amazon EFS.</strong></p>
</li>
<li>
<p>Amazon <strong>FSx</strong> has two modes:</p>
<ol>
<li><strong>Standalone</strong> File share</li>
<li>Link to <strong>S3 Bucket</strong> and access S3 as a file share</li>
</ol>
</li>
<li>
<p><strong>AWS Database Migration Service (AWS DMS)</strong> is a cloud service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to migrate your data into the AWS Cloud, between on-premises instances (through an AWS Cloud setup), or between combinations of cloud and on-premises setups. With AWS DMS, you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync.</p>
<ul>
<li>continuous Data Replication</li>
<li>No data transformation (<strong>ETL jobs needs to be done later somewhere else</strong>.)</li>
<li>once the data is in AWS, you can use <!-- raw HTML omitted -->Glue<!-- raw HTML omitted --> to transform it.</li>
<li>You can use Database Migration Service for one-time data migration into RDS and EC2-based databases.</li>
<li>You can also continuously replicate your data with high availability (enable multi-AZ) and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</li>
<li>Replication between on-premises to on-premises databases is not supported.</li>
<li>The service provides an end-to-end view of the data replication process, including diagnostic and performance data for each point in the replication pipeline.</li>
<li>You only pay for the compute resources used during the migration process and any additional log storage. Each database migration instance includes storage sufficient for swap space, replication logs, and data cache for most replications and inbound data transfer is free.</li>
</ul>
</li>
<li>
<p>The <strong>S3 Standard-IA</strong> storage class is designed for long-lived and infrequently accessed data. (IA stands for <em>infrequent access</em>.) S3 Standard-IA is available for <strong>millisecond access</strong> (same as the S3 Standard storage class). Amazon S3 charges a retrieval fee for these objects, so they are most suitable for infrequently accessed data.</p>
<p>Amazon S3 <strong>Glacier Deep Archive</strong> does not provide immediate access. Expedited retrievals may still take <strong>hours</strong> to complete.</p>
</li>
<li>
<p><strong>S3</strong> related:</p>
<ul>
<li>By default, you can create up to <strong>100</strong> buckets in <strong>each</strong> of your AWS accounts.</li>
<li>You <strong>can’t change its Region</strong> after creation.</li>
<li>You can host <strong>static</strong> websites by configuring your bucket for website hosting.</li>
<li>You <strong>can’t delete</strong> an S3 bucket using the Amazon S3 console if the bucket contains <strong>100,000</strong> or more objects. You <strong>can’t</strong> delete an S3 bucket using the <strong>AWS CLI if versioning is enabled</strong>.</li>
</ul>
</li>
<li>
<p><strong>S3 Data Consistency Model</strong></p>
<ul>
<li>read-after-write consistency for PUTS of new objects in your S3 bucket in all regions</li>
<li>strong consistency for read-after-write HEAD or GET requests</li>
<li>strong consistency for overwrite PUTS and DELETES in all regions</li>
<li>strong read-after-write consistency for any storage request</li>
<li>eventual consistency for listing all buckets after deleting a bucket (deleted bucket might still show up)</li>
<li>eventual consistency on propagation of enabling versioning on a bucket for the first time.</li>
</ul>
</li>
<li>
<p><strong>S3 Select</strong></p>
<ul>
<li>S3 Select is an Amazon S3 capability designed to <strong>pull out only the data you need</strong> from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in S3.</li>
<li>Amazon S3 Select works on objects stored in <strong>CSV and JSON format, Apache Parquet format, JSON Arrays, and BZIP2 compression for CSV and JSON objects.</strong></li>
<li><strong>CloudWatch</strong> Metrics for S3 Select lets you monitor S3 Select usage for your applications. These metrics are available at <strong>1-minute</strong> intervals and lets you quickly identify and act on operational issues.</li>
</ul>
</li>
<li>
<p><strong>Encryption</strong> in S3:</p>
<ul>
<li>
<ul>
<li><strong>Server-side</strong> Encryption using
<ul>
<li><strong>Amazon S3-Managed Keys (SSE-S3)</strong></li>
<li><strong>AWS KMS-Managed Keys (SSE-KMS)</strong></li>
<li><strong>Customer-Provided Keys (SSE-C)</strong></li>
</ul>
</li>
<li><strong>Client-side</strong> Encryption using
<ul>
<li>AWS KMS-managed customer master key</li>
<li>client-side master key</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Customer Master Key</strong> provides additional layer of security and control.</p>
</li>
<li>
<p><strong>S3 Server-side encryption</strong> can enable encryption settings as part of Bucket configuration, and all objects are automatically encrypted. <strong>Client-Side encryption</strong> is useful for scenarios where you want to encrypt and decrypt at the <strong>client</strong> and store the encrypted object in S3.</p>
</li>
<li>
<p><strong>Availability Zones</strong> give customers the ability to operate production applications and databases that are more highly available, fault-tolerant, and scalable than would be possible from a single data center. AWS maintains multiple AZs around the world and more zones are added at a fast pace. Each AZ can be multiple data centers (typically 3), and at full scale can be hundreds of thousands of servers. They are fully isolated partitions of the AWS Global Infrastructure. With their own power infrastructure, the AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles of each other).</p>
<p>All AZs are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. The network performance is sufficient to accomplish synchronous replication between AZs. AWS Availability Zones are also powerful tools for helping build highly available applications. AZs make partitioning applications about as easy as it can be. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.</p>
</li>
<li>
<p><strong>Durability</strong> – S3 <strong>replicates</strong> data across <strong>multiple availability zones and multiple devices in each availability zone</strong>. <strong>Cross-Region</strong> Replication is used for <strong>disaster</strong> <strong>recovery</strong> by keeping a copy of your bucket in another region. <strong>Versioning</strong> protects from accidental and malicious deletes. <strong>Server-Side encryption</strong> is used for storing the data encrypted <strong>at rest</strong>.</p>
</li>
<li>
<p><strong>Amazon Athena</strong> is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is <strong>serverless</strong>, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>
<p>Athena can run queries to analyze unstructured, semi-structured, and structured data stored in S3.</p>
</li>
<li>
<p><strong>Kinesis</strong> and <strong>Athena</strong> can feed data into <strong>MapReduce</strong> jobs.</p>
</li>
<li>
<p>In Kinesis Data Stream, the minimum value of a stream’s time period is 24 hours, but this can be increased up to 8760 hours (365 days).</p>
</li>
<li>
<p><strong>AWS Lake formation</strong> integrates with the following services which also accepts the lake formation permissions:</p>
<ol>
<li>
<p>AWS Glue,</p>
</li>
<li>
<p>Amazon Athena,</p>
</li>
<li>
<p>Amazon Redshift Spectrum,</p>
</li>
<li>
<p>Amazon Quicksight,</p>
</li>
<li>
<p>Amazon EMR and</p>
</li>
<li>
<p>AWS Glue DataBrew.</p>
</li>
</ol>
</li>
<li>
<p><strong>An EMR cluster has three nodes:</strong></p>
<ol>
<li>
<p><strong>Master</strong> node - Runs and manages the master components of distributed applications.</p>
</li>
<li>
<p><strong>Core</strong> node - Coordinates the data storage as part of the HDFS file system with the task nodes.</p>
</li>
<li>
<p><strong>Task</strong> nodes - Optional nodes, and can be added to perform parallel computation tasks on data.</p>
</li>
</ol>
<p>Task nodes can be used with <!-- raw HTML omitted -->spot EC2 instances, as they can be used and removed whenever required, and this does not affect the working of other nodes.<!-- raw HTML omitted --></p>
<p>Using spot instances can provide a huge cost saving compared to the on-demand and reserved instances, as this spares the EC2 instances once the job is done. The master and core nodes can&rsquo;t be used with the spot instances, as their availability is a must requirement for the EMR cluster.</p>
</li>
<li>
<p>While you can use Spot instances on any node type, a Spot interruption on the Master node requires terminating the entire cluster, and on a Core node, it can lead to HDFS data loss.</p>
</li>
<li>
<p><strong>EMR Storage</strong></p>
<ul>
<li>HDFS</li>
<li>EMRFS: access to S3 as if it were HDFS</li>
<li>Local file system</li>
<li>EBS for HDFS</li>
</ul>
</li>
<li>
<p><strong><!-- raw HTML omitted -->Kinesis Data Streams PutRecord API<!-- raw HTML omitted --></strong> uses <em><strong>name of the stream</strong></em>, <em><strong>a partition key</strong></em> and the <em><strong>data blob</strong></em> whereas <!-- raw HTML omitted --><strong>Kinesis Data Firehose PutRecord API</strong><!-- raw HTML omitted --> uses the <em><strong>name of the delivery stream</strong></em> and the <em><strong>data record</strong></em>.</p>
</li>
<li>
<p><img src="https://img-c.udemycdn.com/redactor/raw/test_question_description/2020-12-06_05-31-48-e1d0b84a350d082ddd023606a22f2432.jpg" alt="img"></p>
<p>Kinesis Data Firehose requires only Stream Data Name and the Data Record, whereas the Kinesis Data Stream needs ShardId, Partition Key, and the Data Record.</p>
<p>Also, Kinesis Firehose scales automatically depending on the input data stream, without any need to manually increase the ingestion size.</p>
</li>
<li>
<p>Using SQS would require implementing substantial functionality on top of SQS, and AWS <strong>Batch is only designed for scheduling and allocating the resources needed for batch processing.</strong></p>
</li>
<li>
<p><strong>AWS Data Pipeline</strong> provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline <strong>launches compute resources</strong> in your account <strong>allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters.</strong></p>
</li>
<li>
<p><strong>Preventative</strong> controls encompass the AWS <strong>CAF</strong>(Cloud Adoption Framework) Security Perspective capabilities of IAM, infrastructure security, and data protection. They are <strong>(i) IAM, (ii) Infrastructure security, and (iii) Data protection (encryption and tokenization).</strong></p>
<p><strong>Detective</strong> controls detect changes to configurations or undesirable actions and enable timely incident response. They are <strong>(i) Detection of unauthorized traffic, (ii) Configuration drift, (iii) Fine-grained audits.</strong></p>
</li>
<li>
<p>AWS Step Functions lets you coordinate multiple AWS services into serverless workflows so you can build and update apps quickly. Using Step Functions, you can design and run workflows that stitch together services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into feature-rich applications. Workflows are made up of a series of steps, <strong>with the output of one step acting as input into the next</strong>. They are perfect to run multiple ETL jobs that hand off the input to ML workflows. <strong>(The keyword in the question is &lsquo;workflow&rsquo;.)</strong></p>
</li>
<li>
<p><strong>Kinesis Data Analytics</strong> can use <strong>lambda to convert GZIP</strong> and can run SQL on the converted data. A specialist can simply select an AWS Lambda function from the Kinesis Analytics application source page in the AWS Management console. Kinesis Analytics application will automatically process raw data records using the Lambda function and send transformed data to SQL code for further processing.</p>
</li>
<li>
<p><strong>Object tags</strong> are useful for <strong>security and lifecycle management.</strong> One can use tags to identify an object with, say PII (Personally identifiable information), or use S3 for one-dimensional object categorization. One can also add another dimension using bucket prefixes.</p>
</li>
<li>
<p><strong>Collaborative filtering</strong> is based on (user, item, rating) tuples. So, unlike content-based filtering, it leverages other users’ experiences. The main concept behind collaborative filtering is that users with similar tastes (based on observed user-item interactions) are more likely to have similar interactions with items they haven&rsquo;t seen before.</p>
<p>Compared to content-based filtering, collaborative filtering provides better results for <strong>diversity</strong> (how dissimilar recommended items are); <strong>serendipity</strong> (a measure of how surprising the successful or relevant recommendations are); and <strong>novelty</strong> (how unknown recommended items are to a user).</p>
</li>
<li>
<p>Amazon EMR offers you two options to work with Jupyter notebooks: (i) EMR Notebook, and (ii) JupyterHub. An EMR notebook is a &lsquo;serverless&rsquo; Jupyter notebook. Unlike a traditional notebook, the contents of an EMR notebook itself — the equations, visualizations, queries, models, code, and narrative text — are saved in Amazon S3 separately from the cluster that runs the code.</p>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>React.js Related Notes (1)</title>
            <link>https://ffflora.cat/posts/2021/03/react.js-related-notes-1/</link>
            <pubDate>Wed, 24 Mar 2021 00:08:29 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/03/react.js-related-notes-1/</guid>
            <description>This series of posts summaries the problems/issues I met during work, and the solutions of how I solved these problems.
  How do I parse a .json data file to pass into this DevExtreme component and render out a series of area charts?
Data looks like:
type Data = { period_index: number, breakdown: { sessions: number, conversions: number, avg_page_views: number, avg_time_on_page: number, new_users: number, } }[] parseData = (data: Data | any) =&amp;gt; { if (data.</description>
            <content type="html"><![CDATA[<p>This series of posts summaries the problems/issues I met during work, and the solutions of how I solved these problems.</p>
<ol>
<li>
<p>How do I parse a <code>.json</code> data file to pass into <a href="https://js.devexpress.com/Demos/WidgetsGallery/Demo/Charts/Area/React/Light/">this</a> DevExtreme component and render out a series of area charts?</p>
<p>Data looks like:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-typescript" data-lang="typescript"><span style="color:#a6e22e">type</span> <span style="color:#a6e22e">Data</span> <span style="color:#f92672">=</span> {
  <span style="color:#a6e22e">period_index</span>: <span style="color:#66d9ef">number</span>,
  <span style="color:#a6e22e">breakdown</span><span style="color:#f92672">:</span> {
    <span style="color:#a6e22e">sessions</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">conversions</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">avg_page_views</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">avg_time_on_page</span>: <span style="color:#66d9ef">number</span>,
    <span style="color:#a6e22e">new_users</span>: <span style="color:#66d9ef">number</span>,
  }
}[]
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-react" data-lang="react"><span style="color:#a6e22e">parseData</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">data</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">Data</span> <span style="color:#f92672">|</span> <span style="color:#a6e22e">any</span>) =&gt; {
  <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">length</span> <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>) {
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">sampleData</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">data</span>[<span style="color:#ae81ff">0</span>];
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">keys</span> <span style="color:#f92672">=</span> Object.<span style="color:#a6e22e">keys</span>(<span style="color:#a6e22e">sampleData</span>.<span style="color:#a6e22e">breakdown</span>);
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">resultWithMonth</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">data</span>.<span style="color:#a6e22e">map</span>((<span style="color:#a6e22e">d</span><span style="color:#f92672">:</span> { <span style="color:#a6e22e">period_index</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>; <span style="color:#a6e22e">month</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>; <span style="color:#a6e22e">date</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>; }) =&gt; {
   
      <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">month</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">period_index</span> <span style="color:#f92672">%</span> <span style="color:#ae81ff">12</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
      <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">date</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">moment</span>(<span style="color:#e6db74">`</span><span style="color:#e6db74">${</span><span style="color:#a6e22e">month</span><span style="color:#e6db74">}</span><span style="color:#e6db74">`</span>, <span style="color:#e6db74">&#34;M&#34;</span>).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;MMM&#39;</span>)
      <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">month</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">month</span>
      <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">date</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">date</span>
      <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">d</span>
    })
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">result</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">keys</span>.<span style="color:#a6e22e">map</span>(<span style="color:#a6e22e">key</span> =&gt; {
      <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">dataSource</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">resultWithMonth</span>.<span style="color:#a6e22e">map</span>((<span style="color:#a6e22e">d</span><span style="color:#f92672">:</span> { <span style="color:#a6e22e">period_index</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">any</span>; <span style="color:#a6e22e">breakdown</span><span style="color:#f92672">:</span> { [<span style="color:#a6e22e">x</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>]<span style="color:#f92672">:</span> <span style="color:#a6e22e">any</span>; }; <span style="color:#a6e22e">date</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">any</span>; }) =&gt; {
        <span style="color:#66d9ef">return</span> {
          <span style="color:#a6e22e">period_index</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">period_index</span>,
          [<span style="color:#a6e22e">key</span>]<span style="color:#f92672">:</span> <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">breakdown</span>[<span style="color:#a6e22e">key</span>],
          <span style="color:#a6e22e">monthString</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">d</span>.<span style="color:#a6e22e">date</span>
        }
      })
   
      <span style="color:#66d9ef">return</span> {
   
        <span style="color:#a6e22e">title</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">key</span>,
        <span style="color:#a6e22e">subtitle</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">numeral</span>(<span style="color:#a6e22e">dataSource</span>[<span style="color:#a6e22e">dataSource</span>.<span style="color:#a6e22e">length</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>][<span style="color:#a6e22e">key</span>]).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;0,0&#39;</span>),
        <span style="color:#a6e22e">dataSource</span>
      }
    })
   
    <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">result</span>
  }
  <span style="color:#66d9ef">return</span> []
}
<span style="color:#a6e22e">parseMonthData</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">period</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>) =&gt; {
   
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">month</span> <span style="color:#f92672">=</span> (parseInt(<span style="color:#a6e22e">period</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">12</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">year</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">1900</span> <span style="color:#f92672">+</span> (parseInt(<span style="color:#a6e22e">period</span>) <span style="color:#f92672">-</span> (parseInt(<span style="color:#a6e22e">period</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">12</span>)) <span style="color:#f92672">/</span> <span style="color:#ae81ff">12</span>
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">date</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">moment</span>(<span style="color:#e6db74">`</span><span style="color:#e6db74">${</span><span style="color:#a6e22e">month</span><span style="color:#e6db74">}</span><span style="color:#e6db74">`</span>, <span style="color:#e6db74">&#34;M&#34;</span>).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;MMMM&#39;</span>)
  <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">date</span>
}
</code></pre></div><p>Chart looks like:</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/blog/1.png?raw=true" alt="image-20210324014548148"></p>
</li>
<li>
<p>How to parse data and get color ranges so that the colors in the table are dynamically changed based on the value in the cell?</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-react" data-lang="react"><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">COLUMNS</span> <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;value&#39;</span>, <span style="color:#e6db74">&#39;count&#39;</span>, <span style="color:#e6db74">&#39;Clicks&#39;</span>, <span style="color:#e6db74">&#39;Conversions&#39;</span>, <span style="color:#e6db74">&#39;Cost&#39;</span>, <span style="color:#e6db74">&#39;avg_cpm&#39;</span>, <span style="color:#e6db74">&#39;avg_cpc&#39;</span>, <span style="color:#e6db74">&#39;avg_cpa&#39;</span>]
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">COLOR</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#1c558e&#39;</span>
   
   
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">getColorRanges</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">data</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">DataType</span>[] <span style="color:#f92672">|</span> <span style="color:#a6e22e">any</span>)<span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>[][] =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorRanges</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">COLUMNS</span>.<span style="color:#a6e22e">map</span>(<span style="color:#a6e22e">col</span> =&gt; {
    <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">col</span> <span style="color:#f92672">===</span> <span style="color:#e6db74">&#39;value&#39;</span>) <span style="color:#66d9ef">return</span> [] <span style="color:#75715e">//keywords, do nothing. 
</span><span style="color:#75715e"></span>   
    <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">sorted</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">data</span>
      .<span style="color:#a6e22e">map</span>((<span style="color:#a6e22e">x</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">DataType</span>) =&gt; <span style="color:#a6e22e">x</span>[<span style="color:#a6e22e">col</span>])
      .<span style="color:#a6e22e">sort</span>((<span style="color:#a6e22e">a</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>, <span style="color:#a6e22e">b</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>) =&gt; <span style="color:#a6e22e">a</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">b</span>);
   
    <span style="color:#66d9ef">return</span> [<span style="color:#a6e22e">sorted</span>[<span style="color:#ae81ff">0</span>], <span style="color:#a6e22e">sorted</span>[<span style="color:#a6e22e">sorted</span>.<span style="color:#a6e22e">length</span> <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]];
  })
   
  <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">colorRanges</span>
}
   
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorRanges</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">getColorRanges</span>(<span style="color:#a6e22e">data</span>);
   
<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">getBackgroundColor</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">column</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">string</span>, <span style="color:#a6e22e">value</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">number</span>) =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">columnIndex</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">COLUMNS</span>.<span style="color:#a6e22e">indexOf</span>(<span style="color:#a6e22e">column</span>);
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorRange</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">colorRanges</span>[<span style="color:#a6e22e">columnIndex</span>];
  <span style="color:#66d9ef">const</span> [<span style="color:#a6e22e">min</span>, <span style="color:#a6e22e">max</span>] <span style="color:#f92672">=</span> <span style="color:#a6e22e">colorRange</span>;
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">colorWeight</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">value</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">min</span>) <span style="color:#f92672">/</span> (<span style="color:#a6e22e">max</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">min</span>);
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">color</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">COLOR</span> <span style="color:#f92672">+</span> Math.<span style="color:#a6e22e">ceil</span>(<span style="color:#ae81ff">128</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">colorWeight</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">127</span>).<span style="color:#a6e22e">toString</span>(<span style="color:#ae81ff">16</span>);
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">opacity</span> <span style="color:#f92672">=</span> (<span style="color:#ae81ff">128</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">colorWeight</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">127</span>) <span style="color:#f92672">/</span> <span style="color:#ae81ff">255</span>
  <span style="color:#66d9ef">return</span> [<span style="color:#a6e22e">color</span>, <span style="color:#a6e22e">opacity</span>];
}
<span style="color:#75715e">// inside the devExpress data Grid component, there&#39;s a config is called `cellRender`, where you can adjust the style and/or render config for cells.
</span><span style="color:#75715e"></span><span style="color:#a6e22e">cellRender</span><span style="color:#f92672">=</span>{<span style="color:#a6e22e">cell</span> =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">columnWithDollar</span> <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;Cost&#39;</span>, <span style="color:#e6db74">&#39;CPC&#39;</span>, <span style="color:#e6db74">&#39;CPM&#39;</span>, <span style="color:#e6db74">&#39;CPA&#39;</span>]
  <span style="color:#66d9ef">const</span> { <span style="color:#a6e22e">caption</span> } <span style="color:#f92672">=</span> <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">column</span>
  <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">col</span> <span style="color:#f92672">!==</span> <span style="color:#e6db74">&#39;value&#39;</span>) {
          <span style="color:#66d9ef">const</span> [<span style="color:#a6e22e">backgroundColor</span>, <span style="color:#a6e22e">opacity</span>] <span style="color:#f92672">=</span> <span style="color:#a6e22e">getBackgroundColor</span>(<span style="color:#a6e22e">col</span>, <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]);
         <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">cellElement</span>.<span style="color:#a6e22e">style</span>.<span style="color:#a6e22e">backgroundColor</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">backgroundColor</span>
         <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">opacity</span> <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.9</span>) {
           <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">cellElement</span>.<span style="color:#a6e22e">style</span>.<span style="color:#a6e22e">color</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;#ddd&#39;</span>
  }
  <span style="color:#66d9ef">return</span> (<span style="color:#a6e22e">columnWithDollar</span>.<span style="color:#a6e22e">includes</span>(<span style="color:#a6e22e">caption</span>) <span style="color:#f92672">?</span> 		(<span style="color:#a6e22e">numeral</span>(<span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;$0,0.00&#39;</span>)) <span style="color:#f92672">:</span> (<span style="color:#a6e22e">numeral</span>(<span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]).<span style="color:#a6e22e">format</span>(<span style="color:#e6db74">&#39;0,0&#39;</span>)));
  }
  <span style="color:#66d9ef">else</span> <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">cell</span>.<span style="color:#a6e22e">data</span>[<span style="color:#a6e22e">col</span>]
  }}
</code></pre></div><p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/blog/2.png?raw=true" alt="image-20210330155336896"></p>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>AWS SageMaker Deep Dive</title>
            <link>https://ffflora.cat/posts/2021/02/aws-sagemaker-deep-dive/</link>
            <pubDate>Sat, 06 Feb 2021 17:02:04 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/02/aws-sagemaker-deep-dive/</guid>
            <description>This post consist of the notes that are based on the series of AWS SageMaker videos provided by Amazon Web Services Amazon SageMaker Technical Deep Dive Series.
Fully-Managed Notebook Instances with Amazon SageMaker Create your Notebook Instance:
 Pick the right family:  t - tiny &amp;lt; m, c - computed optimized, p - GPU   Pick the right size:  From medium to very very large.   Pick the right version:  ml.</description>
            <content type="html"><![CDATA[<p>This post consist of the notes that are based on the series of AWS SageMaker videos provided by Amazon Web Services <a href="https://www.youtube.com/playlist?list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz">Amazon SageMaker Technical Deep Dive Series</a>.</p>
<h1 id="fully-managed-notebook-instances-with-amazon-sagemaker">Fully-Managed Notebook Instances with Amazon SageMaker</h1>
<p>Create your Notebook Instance:</p>
<ol>
<li>Pick the right family:
<ul>
<li>t - tiny &lt; m,   c - computed optimized, p - GPU</li>
</ul>
</li>
<li>Pick the right size:
<ul>
<li>From medium to very very large.</li>
</ul>
</li>
<li>Pick the right version:
<ul>
<li>ml.t<strong>3</strong>.medium: 3 means the latest version in the T family</li>
</ul>
</li>
</ol>
<p>There are ~200 example notebooks in SageMaker Notebook, definitely try them out.</p>
<h1 id="built-in-machine-learning-algorithms-with-amazon-sagemaker">Built-in Machine Learning Algorithms with Amazon SageMaker</h1>
<p>** = Distributed Training*</p>
<p><em>&lt;&gt; = incremental training</em></p>
<table>
<thead>
<tr>
<th>Type of Problem</th>
<th>Algorithms</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classification</td>
<td>Linear Learner *<!-- raw HTML omitted -->XGBoost<!-- raw HTML omitted -->KNN<!-- raw HTML omitted -->Fatorization Machines</td>
</tr>
<tr>
<td>Computer Vision</td>
<td>Image Classification &lt;&gt;<!-- raw HTML omitted -->Object Detection<!-- raw HTML omitted -->Semantic Segmentation</td>
</tr>
<tr>
<td>Topic Modeling</td>
<td>LDA<!-- raw HTML omitted -->NTM</td>
</tr>
<tr>
<td>Working with Text</td>
<td>Blazing Text<!-- raw HTML omitted -->- Supervised<!-- raw HTML omitted -->- Unsupervised *</td>
</tr>
<tr>
<td>Recommendation</td>
<td>Fatorization Machines * (+ KNN)</td>
</tr>
<tr>
<td>Forecasting</td>
<td>DeepAR *</td>
</tr>
<tr>
<td>Anomaly Detection</td>
<td>Random Cut Forests *<!-- raw HTML omitted -->IP Insights *</td>
</tr>
<tr>
<td>Clustering</td>
<td>KMeans *<!-- raw HTML omitted -->KNN</td>
</tr>
<tr>
<td>Sequence Translation</td>
<td>Seq2Seq *</td>
</tr>
<tr>
<td>Regression</td>
<td>Linear Learner<!-- raw HTML omitted -->XGBoost<!-- raw HTML omitted -->KNN</td>
</tr>
<tr>
<td>Feature Reduction</td>
<td>PCA <!-- raw HTML omitted -->Object2Vec</td>
</tr>
</tbody>
</table>
<h2 id="heading"></h2>
<h2 id="heading-1"></h2>
<h2 id="heading-2"></h2>
]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Specialty Prep List</title>
            <link>https://ffflora.cat/posts/2021/01/aws-machine-learning-specialty-prep-list/</link>
            <pubDate>Sun, 31 Jan 2021 19:29:26 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/01/aws-machine-learning-specialty-prep-list/</guid>
            <description>Here are some resources I collected for better preparing the AWS Machine Learning Specialty Exam.
Practical experience aws/amazon-sagemaker-examples
Object Detection with Amazon Sagemaker
AWS Training offers digital courses on machine learning (ML).   Data Analytics Fundamentals
  Exam Readiness: AWS Certified Machine Learning - Specialty
  Deep Learning on AWS
  Elements of Data Science
  Math for Machine Learning
  Linear and Logistic Regression</description>
            <content type="html"><![CDATA[<p>Here are some resources I collected for better preparing the AWS Machine Learning Specialty Exam.</p>
<h3 id="practical-experience">Practical experience</h3>
<p><a href="https://github.com/aws">aws</a>/<strong><a href="https://github.com/aws/amazon-sagemaker-examples">amazon-sagemaker-examples</a></strong></p>
<p><a href="https://www.coursera.org/projects/object-detection-sagemaker">Object Detection with Amazon Sagemaker</a></p>
<h3 id="aws-training-offers-digital-courses-on-machine-learning-ml">AWS Training offers digital courses on machine learning (ML).</h3>
<ul>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=35364">Data Analytics Fundamentals</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=42183">Exam Readiness: AWS Certified Machine Learning - Specialty</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/training/course-descriptions/deep-learning/">Deep Learning on AWS</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=26598">Elements of Data Science</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=26597">Math for Machine Learning</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=26599">Linear and Logistic Regression</a></p>
</li>
<li>
<p><input disabled="" type="checkbox"> <a href="https://www.aws.training/Details/eLearning?id=12531">Machine Learning Use Case</a></p>
</li>
</ul>
<h3 id="aws-whitepapers">AWS whitepapers</h3>
<blockquote>
<p>The following are whitepapers that have been written about ML and deep learning that you will want to read as part of your exam study:</p>
</blockquote>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/whitepapers/Deep_Learning_on_AWS.pdf?did=wp_card&amp;trk=wp_card">Deep Learning on AWS</a></li>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/whitepapers/aws-power-ml-at-scale.pdf?did=wp_card&amp;trk=wp_card">Power machine learning at scale</a></li>
</ul>
<h3 id="aws-documentation">AWS documentation</h3>
<blockquote>
<p>Review the following AWS service documentation as part of your study:</p>
</blockquote>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/sagemaker/?id=docs_gateway">Amazon SageMaker</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/dlami/?id=docs_gateway">Deep Learning AMI</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/polly/?id=docs_gateway">Amazon Polly</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/lex/?id=docs_gateway">Amazon Lex</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/transcribe/?id=docs_gateway">Amazon Transcribe</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/translate/?id=docs_gateway">Amazon Translate</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/rekognition/?id=docs_gateway">Amazon Rekognition</a></li>
<li><input disabled="" type="checkbox"> <a href="https://docs.aws.amazon.com/s3/?id=docs_gateway">Amazon S3</a></li>
</ul>
<h3 id="service-faqs">Service FAQs</h3>
<blockquote>
<p>Here are links to service FAQs that have some helpful information when looking to better understand ML-related AWS services:</p>
</blockquote>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/sagemaker/faqs/">AWS SageMaker FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/comprehend/faqs/">AWS Comprehend FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/lex/faqs/">AWS Lex FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/polly/faqs/">AWS Polly FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/rekognition/faqs/">AWS Rekognition FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/translate/faqs/">AWS Translate FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/transcribe/faqs/">AWS Transcribe FAQ</a></li>
<li><input disabled="" type="checkbox"> <a href="https://aws.amazon.com/deeplens/faqs/">AWS DeepLens FAQ</a></li>
</ul>
<h3 id="aws-blogs">AWS blogs</h3>
<blockquote>
<p>There is an <a href="https://aws.amazon.com/blogs/machine-learning/">AWS machine learning blog</a> that you should read as part of your exam study.</p>
</blockquote>
<h3 id="official-exam-guide-sample-exam-questions">Official Exam Guide, sample exam questions</h3>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/training-and-certification/docs-ml/AWS-Certified-Machine-Learning-Specialty_Exam-Guide.pdf">Download the exam guide</a></li>
<li><input disabled="" type="checkbox"> <a href="https://d1.awsstatic.com/training-and-certification/docs-ml/AWS-Certified-Machine-Learning-Specialty_Sample-Questions.pdf">Download the sample questions</a></li>
<li><input disabled="" type="checkbox"> <a href="https://www.udemy.com/course/aws-certified-machine-learning-specialty-full-practice-exams/">Udemy - AWS Certified Machine Learning Specialty: 3 PRACTICE EXAMS</a></li>
<li><input disabled="" type="checkbox"> <a href="https://amazonmr.au1.qualtrics.com/reports/RC/public/YW1hem9ubXItNWQ0YWQzYTViZjczMDIwMDBmNTc4ZWY3LVVSX2RvRFVNUlZZS1NtQjUyWg==">Exam Readiness - Sample Questions</a></li>
</ul>
<h3 id="other-videos">Other Videos</h3>
<ul>
<li><input disabled="" type="checkbox"> <a href="https://www.youtube.com/playlist?list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz">Amazon SageMaker Deep Dive Series</a></li>
</ul>
<h1 id="-٩-ω-و-">＼＼\ ٩( &lsquo;ω&rsquo; )و //／／</h1>
]]></content>
        </item>
        
        <item>
            <title>AWS Machine Learning Exam Readiness with Sample Questions</title>
            <link>https://ffflora.cat/posts/2021/01/aws-machine-learning-exam-readiness-with-sample-questions/</link>
            <pubDate>Sat, 30 Jan 2021 00:20:32 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/01/aws-machine-learning-exam-readiness-with-sample-questions/</guid>
            <description>These are my study notes directly from AWS Training cource - AWS Machine Learning Exam Readiness.
Domain 1: Data Engineering Create Data Repo for Machine Learning Identify and implement a data ingestion solution To use the data for ML, you need to ingest it into a service like Amazon S3
Batch and stream processing are two kinds of data ingestion.
Batch processing Batch processing periodically collects and groups source data.With batch processing, the ingestion layer periodically collects and groups source data and sends it to a destination like Amazon S3.</description>
            <content type="html"><![CDATA[<p>These are my study notes directly from AWS Training cource - <em>AWS Machine Learning Exam Readiness</em>.</p>
<h2 id="domain-1-data-engineering">Domain 1: Data Engineering</h2>
<h3 id="create-data-repo-for-machine-learning">Create Data Repo for Machine Learning</h3>
<h3 id="identify-and-implement-a-data-ingestion-solution"><strong>Identify and implement a data ingestion solution</strong></h3>
<p>To use the data for ML, you need to ingest it into a service like Amazon S3</p>
<p><strong>Batch</strong> and <strong>stream</strong> processing are two kinds of data ingestion.</p>
<h4 id="batch-processing">Batch processing</h4>
<p><!-- raw HTML omitted -->Batch processing periodically collects and groups source data.<!-- raw HTML omitted --></p>
<p>With batch processing, the ingestion layer periodically collects and groups source data and sends it to a destination like Amazon S3. Batch processing is typically used when there is no real need for real-time or near-real-time data, because it is generally easier and more affordably implemented than other ingestion options.</p>
<h4 id="aws-solutions">AWS Solutions:</h4>
<p>AWS <strong>Glue</strong>(ETL tool), AWS <strong>Database Migration</strong>, AWS <strong>Step Functions</strong>.</p>
<p>(You can automate various ETL tasks that involve complex workflows by using AWS Step Functions.)</p>
<h4 id="stream-processing">Stream Processing</h4>
<p><!-- raw HTML omitted -->Stream processing manipulates and loads data as it’s recognized in real time.<!-- raw HTML omitted --></p>
<h4 id="aws-solutions-1">AWS Solutions:</h4>
<p>**Amazon Kinesis **</p>
<h3 id="identify-and-implement-a-data-transformation-solution"><strong>Identify and implement a data transformation solution</strong></h3>
<h5 id="some-solutions">Some Solutions:</h5>
<ul>
<li>Using Apache Spark on Amazon EMR provides a managed framework</li>
</ul>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114223729670.png?raw=true" alt="image-20210114223729670"></p>
<ul>
<li>
<p>If your datasets or computations are not optimally compatible with SQL, you can use AWS Glue to seamlessly run Spark jobs (Scala and Python support) on data stored in your Amazon S3 buckets.</p>
</li>
<li>
<p>You can store a single source of data in Amazon S3 and perform ad hoc analysis.</p>
<p>This reference architecture shows how AWS services for big data and ML can help build a scalable analytical layer for healthcare data. Customers can store a single source of data in Amazon S3 and perform ad hoc analysis with Athena, integrate with a data warehouse on Amazon Redshift, build a visual dashboard for metrics using Amazon QuickSight, and build an ML model to predict readmissions using Amazon SageMaker. By not moving the data around and connecting to it using different services, customers avoid building redundant copies of the same data.<img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114224855633.png?raw=true" alt="image-20210114224855633"></p>
</li>
</ul>
<h3 id="sample-questions">Sample Questions</h3>
<ol>
<li>
<p>A data engineer needs to create a <strong>cost-effective</strong> data pipeline solution that ingests <strong>unstructured data from various sources</strong> and stores it for downstream analytics applications and ML. The solution should include a data store <strong>where the processed data is highly available for at least one year</strong>, so that data analysts and data scientists can run analytics and ML workloads on the most recent data. For compliance reasons, the solution should include both processed and raw data. The raw data does not need to be accessed regularly, but when needed, should be <strong>accessible within 24 hours.</strong></p>
<p><strong>ANS</strong>:</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114225713429.png?raw=true" alt="image-20210114225713429"></p>
</li>
<li>
<p>An ad-tech company has hired a data engineer to create and maintain a machine learning pipeline for its clickstream data. The data will be gathered <strong>from various sources,</strong> including on premises, and will need to be <strong>streamed</strong> to the company’s <strong>Amazon EMR instances</strong> for further processing.</p>
<p><strong>ANS</strong>:</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210114230019872.png?raw=true" alt="image-20210114230019872"></p>
</li>
<li>
<p>A transportation company currently uses Amazon EMR with Apache Spark for some of its data transformation workloads. It transforms columns of geographical data (like latitudes and longitudes) and adds columns to segment the data into different clusters per city to attain additional features for the k-nearest neighbors algorithm being used.</p>
<p>The company wants less operational overhead for their transformation pipeline. They want a new solution that does not make significant changes to the current pipeline and only requires minimal management.</p>
<p>What AWS services should the company use to build this new pipeline?</p>
<ul>
<li><strong>Use AWS Glue to transform files. Use Amazon S3 as the destination.</strong> ✅</li>
<li>Use AWS Glue to transform files. Use Amazon EMR HDFS as the destination. ❌</li>
<li>Use Amazon EMR to transform files. Use Amazon S3 as the destination. ❌</li>
<li>Use Lambda to transform files. Use Amazon EMR HDFS as the destination. ❌</li>
</ul>
</li>
</ol>
<h2 id="domain-2-eda">Domain 2: EDA</h2>
<h3 id="sample-questions-1">Sample Questions:</h3>
<ol>
<li>
<p>A team of data scientists in a company focusing on security and smart home devices created an ML model that can classify guest types at a front door using a video doorbell. The team is getting an accuracy of 96.23% on the validation dataset.</p>
<p>However, when the team tested this model in production, images were classified with a much lower accuracy. That was due to weather: The changing seasons had an impact on the quality of production images.</p>
<p>What can the team do to improve their model?</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210118003049500.png?raw=true" alt="image-20210118003049500"></p>
</li>
<li>
<p>A team of data scientists in a financial company wants to predict the risk for their incoming customer loan applications. The team has decided to do this by applying the <strong>XGBoost</strong> algorithm, which will predict the probability that a customer will default on a loan. In order to create this solution, the team wants to first <strong>merge the customer application data with demographic and location data</strong> before feeding it into the model.</p>
<p>However, the <strong>dimension of this data is really large</strong>, and the team wants to <strong>keep only those features that are the most relevant</strong> to the prediction.</p>
<p>What techniques can the team use to reach the goal? (Select TWO.)</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210118003924933.png?raw=true" alt="image-20210118003924933"></p>
<ul>
<li>AWS glue doesn&rsquo;t have that shit.</li>
<li>clustering doesn&rsquo;t help with dimensionality reduction.</li>
<li><strong>A</strong> helps the dimensionality problem, but doesn&rsquo;t help the feature selection problem.</li>
</ul>
</li>
<li>
<p>A social networking organization wants to analyze all the comments and likes from its users to flag offensive language on the site. The organization’s data science team wants to use a Long Short-term Memory (LSTM) architecture to classify the raw sentences from the comments into one of two categories: offensive and non-offensive.</p>
<p>What should the team do to prepare the data for the LSTM?</p>
<ul>
<li>Vectorize the sentences. Transform them into numerical sequences. Use the sentences as the input. ❌</li>
<li>Convert the individual sentences into numerical sequences starting from the number 1 for each word in a sentence. Use the sentences as the input. ❌</li>
<li>Convert the individual sentences into sequences of words. Use those as the input. ❌</li>
<li><strong>Vectorize the sentences. Transform them into numerical sequences with a padding. Use the sentences as the input. ✅</strong></li>
</ul>
</li>
</ol>
<h2 id="domain-3-modeling">Domain 3: Modeling</h2>
<h3 id="31-frame-business-problems-as-ml-problems">3.1 Frame Business Problems as ML problems</h3>
<p>Regression problem(prediction) vs classification problem</p>
<h3 id="32-select-the-appropriate-models-for-an-ml-problem">3.2 <strong>Select the appropriate model(s) for an ML problem</strong></h3>
<p>Amazon <strong>SageMaker</strong> provides a few built-in algorithms that work for <strong>classification</strong> situations: <strong>Linear Learner, XGBoost, and K-Nearest Neighbors.</strong> <strong>XGBoost</strong>, for instance, is an open-source implementation of the gradient-boosted trees algorithm. <strong>Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler, weaker models.</strong></p>
<p>In terms of the built-in Amazon <strong>SageMaker</strong> algorithms you could choose for <strong>regression problems</strong>, it’s pretty similar. Again, you could choose <strong>Linear Learner and XGBoost.</strong> <!-- raw HTML omitted -->The difference is that you set the hyperparameters to direct these algorithms to produce quantitative results.<!-- raw HTML omitted --></p>
<p><strong>There are Amazon SageMaker built-in algorithms for natural language processing:</strong></p>
<ul>
<li><strong>BlazingText</strong> algorithm provides highly optimized implementations of the <strong>Word2vec</strong> and text classification algorithms.</li>
<li><strong>Sequence2sequence</strong> is a <strong>supervised</strong> learning algorithm where the input is a sequence of tokens (for example, text, audio) and the output generated is another sequence of tokens.</li>
<li><strong>Object2Vec</strong> generalizes the well-known Word2Vec embedding technique for words that are optimized in the Amazon SageMaker BlazingText algorithm.</li>
</ul>
<p><strong>There are Amazon SageMaker built-in algorithms for computer vision:</strong></p>
<ul>
<li>Image classification is a supervised learning algorithm used to classify images.</li>
<li>Object detection algorithm detects and classifies objects in images using a single deep neural network. It is a supervised learning algorithm that takes images as input and identifies all instances of objects within the image scene. The object is categorized into one of the classes in a specified collection with a confidence score that it belongs to the class. Its location and scale in the image are indicated by a rectangular bounding box.</li>
<li>Semantic segmentation algorithm tags every pixel in an image with a class label from a predefined set of classes.</li>
</ul>
<h4 id="other-options-for-training-algorithms"><strong>Other options for training algorithms</strong></h4>
<ul>
<li>Use Apache Spark with Amazon SageMaker</li>
<li>Submit custom code to train a model with a deep learning framework like TensorFlow or Apache MXNet</li>
<li>Use your own custom algorithm and put the code together as a Docker image</li>
<li>Subscribe to an algorithm from AWS Marketplace</li>
</ul>
<h3 id="33-train-ml-models"><strong>3.3: Train ML models</strong></h3>
<p>Cross Validation</p>
<p>Sage Maker</p>
<h3 id="34-perform-hyperparameter-optimization"><strong>3.4: Perform hyperparameter optimization</strong></h3>
<h4 id="what-are-hyperparameters"><strong>What are hyperparameters?</strong></h4>
<p>Hyperparameters are the knobs or settings that can be tuned before running a training job to control the behavior of an ML algorithm. They can have a big impact on model training as it relates to training time, model convergence, and model accuracy. Unlike model parameters that are derived from the training job, the values of hyperparameters do not change during the training.</p>
<h4 id="there-are-different-categories-of-hyperparameters"><strong>There are different categories of hyperparameters</strong></h4>
<h5 id="model-hyperparameters">Model hyperparameters</h5>
<p>Model hyperparameters define the model itself—Attributes of a neural network architecture like filter size, pooling, stride, padding</p>
<h5 id="optimizer-hyperparameters">Optimizer hyperparameters</h5>
<p>Optimizer hyperparameters, are related to how the model learn the patterns based on data and are used for a neural network model. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent, or even optimizers using momentum like Adam or initializing the parameter weights using methods like Xavier initialization or He initialization</p>
<h5 id="data-hyperparameters">Data hyperparameters</h5>
<p>Data hyperparameters are related to the attributes of the data, often used when you don’t have enough data or enough variation in data—Data augmentation techniques like cropping, resizing</p>
<h4 id="tuning-hyperparameters-can-be-very-labor-intensive"><strong>Tuning hyperparameters can be very labor-intensive</strong></h4>
<p>Traditionally, this was done <strong>manually</strong>: someone who has domain experience related to that hyperparameter and the use case would manually select the hyperparameters based on their intuition and experience. Then they would train the model and score it on the validation data. This process would be repeated over and over again until satisfactory results are achieved.</p>
<p><strong>A better way  is to use search methods to tune hyperparameters</strong>:</p>
<p><strong>Grid search and Random search</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124232631210.png?raw=true" alt="image-20210124232631210"></p>
<ul>
<li>Grid Search: you can set up a grid made of hyperparams and their different values. For each possible combination, a model is trained and a score is produced on the validation data. With this approach, every single combination of the given possible hyperparams values is tried. Therefore this method could be very inefficient.</li>
<li>Random Search: similar to grid search, but random combinations are selected. You can set the number of search iterations based on time and resource constrains.</li>
</ul>
<h4 id="aws-solution-sagemaker">AWS Solution: SageMaker</h4>
<p>Then there’s automated hyperparameter tuning, which uses methods like gradient descent, Bayesian optimization, and evolutionary algorithms to conduct a guided search for the best hyperparameter settings.</p>
<h3 id="35-evaluate-ml-models"><strong>3.5: Evaluate ML models</strong></h3>
<h4 id="confusion-matrix">Confusion Matrix</h4>
<h3 id="metrics-for-classification-problems-">**Metrics for classification problems **</h3>
<p><strong>Accuracy is the ratio of correct predictions to total number of predictions</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124235038069.png?raw=true" alt="image-20210124235038069"></p>
<p><strong>Precision is the proportion of positive predictions that are actually correct</strong><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124235458004.png?raw=true" alt="image-20210124235458004"></p>
<p><strong>Recall is the proportion of correct sets that are identified as positive</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210124235541375.png?raw=true" alt="image-20210124235541375"></p>
<p><strong>Additional metrics include:</strong></p>
<ul>
<li>F1 score:</li>
</ul>
<p>$$
(2<em>Precision</em>Recall)/(Precision+Recall)
$$</p>
<ul>
<li>AUC(Area under the Curve) - Receiver Operator Curve(ROC)</li>
</ul>
<h3 id="sample-questions-2">Sample Questions</h3>
<ol>
<li>
<p>An oil and natural gas company is using machine learning to discover prime locations for drilling. The company has <strong>chosen Amazon SageMaker as its service</strong> for creating machine learning models. The company’s data scientists are <strong>using notebook instances to develop those models</strong>. However, the data scientists spend a long time waiting for the training jobs to complete.</p>
<p>The company wants <strong>improve this idle time to more effectively iterate on the models</strong> with <strong>minimal</strong> change to the code to enable data scientists to <strong>quickly</strong> experiment with their models without having to wait for the training job to load the data and train the model.</p>
<p>What should the team of data scientists do to solve this issue?</p>
<ul>
<li>Use Amazon SageMaker in-built algorithms. ❌</li>
<li>Use Amazon SageMaker Estimators in local mode to train the models. ✅</li>
<li>Change the training job to use Pipe Mode to improve the time it takes to train the model. ❌</li>
<li>Create the models on local laptops. Then, port the code over to use Amazon SageMaker. ❌</li>
</ul>
<p><strong>Notes</strong>:</p>
<p>A &amp; C are for training algos, they can&rsquo;t help DS to quickly experiment the models.</p>
<p>B is to avoid the loading time for data.</p>
</li>
<li>
<p>A data scientist trained an XGBoost model to classify internal documents for further inquiry, and now wants to evaluate the <strong>model’s</strong> <strong>performance</strong> by looking at the results <strong>visually</strong>.</p>
<p>What technique should the data scientist use in this situation?</p>
<ul>
<li>Scatterplot to visualize the predicted labels versus the true label. ❌</li>
<li>Correlation matrix to visualize the predicted labels versus the true label. ❌</li>
<li>Confusion matrix to visualize the predicted labels ✅</li>
<li>Box plot to visualize the predicted labels (X axis) versus the true labels (Y axis). ❌</li>
</ul>
</li>
<li>
<p>A manufacturing company wants to increase the longevity of its factory machines by predicting when a machine part is about to stop working, jeopardizing the health of the machine. The company’s team of Data Scientists will build an ML model to accomplish this goal. The model will be trained on data made up of consumption metrics from similar factory machines, and will span a time frame from one hour before a machine part broke down to five minutes after the part degraded.</p>
<p>What kind of machine learning algorithm should the company use to build this model?</p>
<ul>
<li>Convolutional neural network (CNN) ❌</li>
<li>Amazon SageMaker DeepAR ✅</li>
<li>Scikit Learn Random Forest ❌</li>
<li>SciKit Learn Regression ❌</li>
</ul>
</li>
<li>
<p>A Data Scientist working for an autonomous vehicle company is building an ML model to detect and label people and various objects (for instance, cars and traffic signs) that may be encountered on a street. The Data Scientist has a dataset made up of labeled images, which will be used to train their machine learning model.</p>
<p>What kind of ML algorithm should be used?</p>
<ul>
<li>Instance segmentation ✅</li>
<li>Image localization ❌</li>
<li>Image classification ❌</li>
<li>Semantic segmentation ❌</li>
</ul>
</li>
<li>
<p>A Data Scientist is training a convolutional neural network model to detect incoming employees at the company’s front gate using a camera so that the system opens for them automatically. However, the model is taking too long to converge and the error oscillates for more than 10 epochs.</p>
<p>What should the Data Scientist do to improve upon this situation? (Select TWO.)</p>
<ul>
<li>Increase batch size ❌</li>
<li>Add more epochs ❌</li>
<li>Decrease weight decay ❌</li>
<li>Normalize the images before training ✅</li>
<li>Add batch normalization  ✅</li>
</ul>
</li>
</ol>
<h2 id="domain-4-ml-implementation-and-operations"><strong>Domain 4: ML Implementation and Operations</strong></h2>
<h3 id="41-build-ml-solutions-for-performance-availability-scalability-resiliency-and-fault-tolerance"><strong>4.1: Build ML solutions for performance, availability, scalability, resiliency, and fault tolerance</strong></h3>
<h4 id="high-availability-and-fault-tolerance"><strong>High availability and fault tolerance</strong></h4>
<p>At the heart of designing for failure are two concepts known as high availability and fault tolerance.</p>
<p>In a highly available solution, the system will continue to function even when any component of the architecture stops working. A key aspect of high availability is fault tolerance, which, when built into an architecture, ensures that applications will continue to function without degradation in performance, despite the complete failure of any component of the architecture.</p>
<p><!-- raw HTML omitted --></p>
<h4 id="one-method-of-achieving-high-availability-and-fault-tolerance-is-loose-coupling"><strong>One method of achieving high availability and fault tolerance is loose coupling</strong></h4>
<p>With a loosely coupled, distributed system, the failure of one component can be managed in between your application tiers so that the faults do not spread beyond that single point. Loose coupling is often <strong>achieved by making sure application components are independent of each other</strong>. For example, you should always decouple your storage layer with your compute layer because a training job only requires minimal time, but storing data is permanent. Decoupling helps turn off the compute resources when they are not needed.</p>
<p><strong>Loosely &amp; tightly coupled system:</strong></p>
<p>Tightly coupled:</p>
<ul>
<li>More interdependency</li>
<li>More coordination</li>
<li>More information</li>
</ul>
<p>⬇️</p>
<p>Loosely coupled:</p>
<ul>
<li>Less interdependency</li>
<li>Less coordination</li>
<li>Less information</li>
</ul>
<h4 id="queues-are-used-in-loose-coupling-to-pass-messages-between-components"><strong>Queues are used in loose coupling to pass messages between components</strong></h4>
<p>AWS SQS (queue service), AWS Step Function(workflow service)</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125160545179.png?raw=true" alt="image-20210125160545179"></p>
<h4 id="amazon-cloudwatch-helps-you-monitor-your-system"><strong>Amazon CloudWatch helps you monitor your system</strong></h4>
<h4 id="aws-cloudtrail-captures-api-calls-and-related-events"><strong>AWS CloudTrail captures API calls and related events</strong></h4>
<p>AWS CloudTrail captures API calls and related events made by or on behalf of your AWS account and delivers the log files to an Amazon S3 bucket that you specify. You can identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.</p>
<h4 id="you-can-design-for-failure-of-any-individual-component-by-leveraging-key-aws-services-and-features-">**You can design for failure of any individual component by leveraging key AWS services and features **</h4>
<h5 id="aws-glue-and-emr">AWS GLUE and EMR</h5>
<p><!-- raw HTML omitted -->You should decouple your ETL process from the ML pipeline. The compute power needed for ML isn’t the same as what you’d need for an ETL process—they have very different requirements.<!-- raw HTML omitted --></p>
<ul>
<li>An ETL process needs to read in files from multiple formats, transform them as needed, and then write them back to a persistent storage. Keep in mind that reading and writing takes a lot of memory and disk I/O, so when you decouple your ETL process, <strong>use a framework like Apache Spark</strong>, which can handle large amounts of data easily for ETL.</li>
<li>Training, on the other hand, may require GPUs which are much more suited to handle the training requirements than CPUs. However, GPUs are less cost-effective to keep running when a model is not being trained. <strong>So you can make use of this decoupled architecture by simply using an ETL service like AWS Glue or Amazon EMR, which use Apache Spark for your ETL jobs and Amazon SageMaker to train, test, and deploy your models.</strong></li>
</ul>
<h5 id="ams-sagemaker-endpoints">AMS SageMaker Endpoints</h5>
<p>To ensure a highly available ML serving endpoint, deploy Amazon SageMaker endpoints backed by multiple instances across Availability Zones.</p>
<p><!-- raw HTML omitted --></p>
<h5 id="ams-sagemaker">AMS SageMaker</h5>
<p>Amazon SageMaker makes it easy to containerize ML models for both training and inference. In doing so, you can create ML models made up of loosely coupled, distributed services that can be placed on any number of platforms, or close to the data that the applications are analyzing.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125170039194.png?raw=true" alt="image-20210125170039194"></p>
<h5 id="aws-auto-scaling">AWS Auto Scaling</h5>
<p>Use AWS Auto Scaling <strong>to build scalable solutions by configuring automatic scaling</strong> for the AWS resources such as Amazon SageMaker endpoints that are part of your application in response to the changes in traffic to your application.</p>
<p>With AWS Auto Scaling, you configure and manage scaling for your resources through a scaling plan. The scaling plan uses dynamic scaling and predictive scaling to automatically scale your application’s resources.</p>
<p><strong>This ensures that you add the required computing power to handle the load</strong> on your application, and <strong>then remove it when it&rsquo;s no longer required</strong>. The scaling plan lets you choose scaling strategies to define how to optimize your resource utilization. You can optimize for availability, for cost, or a balance of both.</p>
<p>As you increase the number of concurrent prediction requests, at some point the endpoint responds more slowly and eventually errors out for some requests. Automatically scaling the endpoint avoids these problems and improves prediction throughput. When the endpoint scales out, Amazon SageMaker automatically spreads instances across multiple Availability Zones. This provides Availability Zone-level fault tolerance and protects from an individual instance failure.</p>
<p>If the endpoint has only a moderate load, you can run it on a single instance and still get good performance. Use automatic scaling to ensure high availability during traffic fluctuations without having to constantly provision for peak traffic. For production workloads, use at least two instances. Because Amazon SageMaker automatically spreads the endpoint instances across multiple Availability Zones, a minimum of two instances ensures high availability and provides individual fault tolerance.</p>
<p><strong>To determine the scaling policy for automatic scaling in Amazon SageMaker, test for how much load (RPS) the endpoint can sustain. Then configure automatic scaling and observe how the model behaves when it scales out. Expected behavior is lower latency and fewer or no errors with automatic scaling.</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125170232859.png?raw=true" alt="image-20210125170232859"></p>
<p>Remark:</p>
<h5 id="auto-scaling-group"><strong>Auto Scaling Group</strong></h5>
<p>What is ASG?</p>
<p>In real life, the load on your websites and app can change; in the cloud, you can create and get rid of servers very quickly.</p>
<p>ASG is used to:</p>
<ul>
<li>Scale out(which is add EC2 instances) to match an increased load.</li>
<li>Scale in (which is remove EC2 instances) to match a decreased load.</li>
<li>Ensure we have a min and max number of machines running</li>
<li>Automatically register new instances to a load balancer</li>
</ul>
<h3 id="42-recommend-and-implement-the-appropriate-ml-services-and-features-for-a-given-problem"><strong>4.2: Recommend and implement the appropriate ML services and features for a given problem</strong></h3>
<h4 id="the-stack-for-amazon-machine-learning-has-three-tiers"><strong>The stack for Amazon machine learning has three tiers</strong></h4>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125212450908.png?raw=true" alt="image-20210125212450908"></p>
<h5 id="ml-frameworks--infrastructure">ML frameworks + infrastructure</h5>
<p>The bottom tier of the stack is for expert ML practitioners who work at the framework level. To work with these frameworks, you are comfortable building, training, tuning, and deploying ML models on the metal, so to speak. ML frameworks are the foundation from which innovation in ML is designed. The focus here is on making it easier for you to connect more broadly to the AWS ecosystem, whether that’s about pulling in IoT data from AWS IOT Greengrass, accessing state-of-the art chips (P3), or leveraging elastic inference.</p>
<p><strong>The vast majority of deep learning and ML in the cloud is done on P3 instances in AWS</strong>. You can use whichever ML deep learning framework you like, but some popular options are TensorFlow, MXNet, and PyTorch, which are all supported on AWS.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125215339947.png?raw=true" alt="image-20210125215339947"></p>
<h5 id="ml-services">ML Services</h5>
<p>AWS SageMaker is the heart of this tier.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125220216005.png?raw=true" alt="image-20210125220216005"></p>
<h5 id="ai-services">AI Services</h5>
<p>AWS services in the top tier are for customers who really don&rsquo;t want to deal with building and training their ML models. All of that has been abstracted away, leaving you with easy-to-use services designed to help you deal with common ML problems in various domains, like computer vision, NLP, and time series forecasting.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210125220258775.png?raw=true" alt="image-20210125220258775"></p>
<h5 id="amazon-ai-services">Amazon AI services</h5>
<ul>
<li>Amazon Translate</li>
<li>Amazon Lex</li>
<li>Amazon Polly</li>
<li>Amazon Transcribe</li>
<li>Amazon Rekognition</li>
<li>Amazon Comprehend</li>
</ul>
<h3 id="43-apply-basic-aws-security-practices-to-ml-solutions"><strong>4.3: Apply Basic AWS security practices to ML solutions</strong></h3>
<h4 id="below-is-a-summary-of-security-features-integrated-with-amazon-sagemaker"><strong>Below is a summary of security features integrated with Amazon SageMaker</strong></h4>
<ul>
<li>Authentication
<ul>
<li>IAM federation</li>
</ul>
</li>
<li>Gaining insight
<ul>
<li>Restrict access by IAM policy and condition keys</li>
</ul>
</li>
<li>Audit
<ul>
<li>API logs to AWS CloudTrail - exception of InvokeEndpoint</li>
</ul>
</li>
<li>Data Protection at rest
<ul>
<li>AWS KMS-based encryption for:
<ul>
<li>Notebooks</li>
<li>Training jobs</li>
<li>Amazon S3 location to store modelsEndpoint</li>
</ul>
</li>
</ul>
</li>
<li>Data Protection at motion
<ul>
<li>HTTPS for:
<ul>
<li>API/console</li>
<li>Notebooks</li>
<li>VPC-enabled</li>
<li>Interface endpoint</li>
<li>Limit by IPTraining jobs/endpoints</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="44-deploy-and-operationalize-ml-solutions"><strong>4.4: Deploy and operationalize ML solutions</strong></h3>
<h4 id="deploying-a-model-using-amazon-sagemaker-hosting-services-is-a-three-step-process"><strong>Deploying a model using Amazon SageMaker hosting services is a three-step process</strong></h4>
<h5 id="create-a-model-in-amazon-sagemaker">Create a model in Amazon SageMaker</h5>
<p>You need:</p>
<ul>
<li>The Amazon S3 path where the model artifacts are stored</li>
<li>The Docker registry path for the image that contains the inference code</li>
<li>A name that you can use for subsequent deployment steps</li>
</ul>
<h5 id="create-an-endpoint-configuration-for-an-https-endpoint">Create an endpoint configuration for an HTTPS endpoint</h5>
<p>You need:</p>
<ul>
<li>The name of one or more models in production variants</li>
<li>The ML compute instances that you want Amazon SageMaker to launch to host each production variant. When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, Amazon SageMaker launches them in multiple Availability Zones. This ensures continuous availability. Amazon SageMaker manages deploying the instances.</li>
</ul>
<h5 id="create-an-https-endpoint">Create an HTTPS endpoint</h5>
<p>You need to provide the endpoint configuration to Amazon SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration.</p>
<h4 id="you-may-want-to-pay-extra-attention-to-the-following-points-when-youre-delivering-an-ml-model-into-a-production-environment"><strong>You may want to pay extra attention to the following points when you’re delivering an ML model into a production environment:</strong></h4>
<ul>
<li>Apply software engineering disciplines. Add error recovery code and make sure that tests for unexpected data inputs exist. Perform the same kind of unit testing, quality assurance, and user acceptance testing that is performed for other systems. If the ML system has moved from the research stage to development, some of these expected software engineering practices might have been inconsistently applied. Automate this system using common DevOps tools like AWS CodeBuild and AWS CodeCommit.</li>
<li>Track, identify, and account for changes in data sources. The data might change over time. One change in data type in one source can break the whole pipeline. Changes in software that produces a data source can have flow-on effects.</li>
<li>Perform ongoing monitoring and evaluation of results. Evaluate the expectations versus the results of the ML system. Build methods to check the error rate and the classes of errors being made against project expectations. If the overall error rate is the same, are the same proportions of the different classes of errors still the same? Is model drift occurring?</li>
<li>Create methods to collect data from production inferences that can be used to improve future models.</li>
</ul>
<h4 id="amazon-sagemaker-supports-automatic-scaling-for-production-variants"><strong>Amazon SageMaker supports automatic scaling for production variants</strong></h4>
<p>Amazon SageMaker supports automatic scaling for production variants. Automatic scaling dynamically adjusts the number of instances provisioned for a production variant in response to changes in your workload. When the workload increases, automatic scaling brings more instances online. When the workload decreases, automatic scaling removes unnecessary instances so that you don&rsquo;t pay for provisioned variant instances that you aren&rsquo;t using.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210126003729710.png?raw=true" alt="image-20210126003729710"></p>
<h4 id="define-and-apply-a-scaling-policy-that-uses-amazon-cloudwatch-metrics"><strong>Define and apply a scaling policy that uses Amazon CloudWatch metrics</strong></h4>
<ul>
<li>Automatic scaling uses the policy to adjust the number of instances up or down in response to actual workloads</li>
<li>You can use the AWS Management Console to apply a scaling policy based on a predefined metric</li>
<li>A predefined metric is defined in an enumeration so you can specify it by name in code or use it in the console</li>
<li>Always <strong>load-test</strong> your automatic scaling configuration to ensure that it works correctly before using it to manage production traffic</li>
</ul>
<h3 id="sample-questions-3">Sample Questions:</h3>
<ol>
<li>
<p>A sports and betting company uses machine learning to predict the odds of winning during sporting events. It uses the Amazon SageMaker endpoint to serve its production model. The endpoint is on an m5.8xlarge instance.</p>
<p>What can the company do to ensure that this endpoint is highly available, while using the most cost-effective and easily managed solution?</p>
<ul>
<li>Create another endpoint. Put the two endpoints behind an Application Load Balancer. ❌</li>
<li>Increase the number of instances associated with the endpoint to more than one. ✅</li>
<li>Increase the instance size to m5.16 x-large. ❌</li>
<li>Add an elastic inference to the endpoint. ❌</li>
</ul>
</li>
<li>
<p>A healthcare company wants to deploy an ensemble of models behind a single endpoint with minimal management. The models include an XGBoost model trained on one of its structured datasets and a CNN model trained on an image dataset.</p>
<p>Which solution can the company use to reach this objective?</p>
<ul>
<li>
<p>Create an AWS Lambda function with Amazon API Gateway that preprocesses the incoming data. The function then creates the prediction from all the ensemble models. And finally, it returns the prediction. ✅</p>
</li>
<li>
<p>Create an AWS Deep Learning container that preprocesses the incoming data. The container then creates the prediction from all the ensemble models. And finally, it returns the prediction. ❌</p>
</li>
<li>
<p>Create an Amazon EC2 instance with the AWS Deep Learning AMI that preprocesses the incoming data. The instance then creates the prediction from all the ensemble models. And finally, it returns the prediction. ❌</p>
</li>
<li>
<p>Create an Amazon SageMaker endpoint that preprocesses the incoming data. The endpoint then creates the prediction from all the ensemble models. And finally, it returns the prediction. ❌</p>
</li>
</ul>
</li>
<li>
<p>A Machine Learning Engineer created a pipeline for training an ML model using an Amazon SageMaker training job. The training job began successfully, but then failed after running for five minutes.</p>
<p>How should the Engineer begin to debug this issue? (Select TWO.)</p>
<ul>
<li>Check AWS CloudTrail logs to check the error that caused the training to fail</li>
<li>Log into the Amazon SageMaker training job instance and check the job history</li>
<li>Go to Amazon CloudWatch logs and check the logs for the given training job ✅</li>
<li>Check the error in the given training job directly in the Amazon SageMaker console</li>
<li>Call the <strong>DescribeJob API</strong> to check the FailureReason option✅</li>
</ul>
</li>
<li>
<p>A news organization wants to extract metadata from its articles and blogs and index that metadata in Amazon Elasticsearch Service (Amazon ES) to enable faster searches.</p>
<p>What AWS service can the organization use to achieve this goal?</p>
<ul>
<li>Amazon Textract</li>
<li>Amazon Personalize</li>
<li>Amazon Rekognition Image</li>
<li>Amazon Comprehend ✅</li>
</ul>
</li>
<li>
<p>A news organization wants to extract metadata from its articles and blogs and index that metadata in Amazon Elasticsearch Service (Amazon ES) to enable faster searches.</p>
<p>What AWS service can the organization use to achieve this goal?</p>
<ul>
<li>Amazon Textract</li>
<li>Amazon Personalize</li>
<li>Amazon Rekognition Image</li>
<li>Amazon Comprehend ✅</li>
</ul>
</li>
<li>
<p>A machine translation company is deploying its language translation models behind an Amazon SageMaker endpoint. The company wants to deploy a solution directly on its website so that users can input text in one language and have it translated into a second language. The company wants to reach a solution with minimal maintenance and latency for spiky traffic times.</p>
<p>How should the company architect this solution?</p>
<ul>
<li>Create a function on an Amazon EC2 instance that uses CURL to call the InvokeEndpoint API. Call the Amazon EC2 instance from the website.</li>
<li>Use Lambda to call InvokeEndpoint. Use the Amazon API Gateway URL to call the AWS Lambda function. ✅</li>
<li>Use Amazon SageMaker InvokeEndpoint with API Gateway</li>
<li>Install the sagemaker-runtime library on the web server. Call InvokeEndpoint from the webserver.</li>
</ul>
</li>
<li>
<p>A Machine Learning Specialist has various CSV training datasets stored in an S3 bucket. Previous models trained with similar training data sizes using the Amazon SageMaker Linear learner algorithm have a slow training process. The Specialist wants to decrease the amount of time spent on training the model.</p>
<p>Which combination of steps should be taken by the Specialist? (Select TWO.)</p>
<ol>
<li>Convert the CSV training dataset into Apache Parquet format.</li>
<li>Train the model using Amazon SageMaker Pipe mode. ✅</li>
<li>Convert the CSV training dataset into Protobuf RecordIO format. ✅</li>
<li>Train the model using Amazon SageMaker File mode.</li>
<li>Stream the dataset into Amazon SageMaker using Amazon Kinesis Firehose to train the model.</li>
</ol>
</li>
<li>
<p>A Machine Learning Specialist is using a 100GB EBS volume as a storage disk for an Amazon SageMaker instance. After running a few training jobs, the Specialist realized that he needed a higher I/O throughput and a shorter job startup and execution time.</p>
<p>Which approach will give the MOST satisfactory result based on the requirements?</p>
<ol>
<li>Store the training dataset in Amazon S3 and use the Pipe input mode for training the model. ✅</li>
<li>Increase the size of the EBS volume to obtain higher I/O throughput.</li>
<li>Upgrade the SageMaker instance to a larger size.</li>
<li>Increase the EBS volume to 500GB and use the File mode for training the model.</li>
</ol>
</li>
<li>
<p>A graphics design startup is using multiple Amazon S3 buckets to store high-resolution media files for their various digital artworks. After securing a partnership deal with a leading media company, the two parties shall be sharing digital resources with one another as part of the contract. The media company frequently performs multiple object retrievals from the S3 buckets every day, which increased the startup’s data transfer costs.</p>
<p>As the Solutions Architect, what should you do to help the startup lower their operational costs?</p>
<ol>
<li>Advise the media company to create their own S3 bucket. Then run the <code>aws s3 sync s3://sourcebucket s3://destinationbucket</code> command to copy the objects from their S3 bucket to the other party’s S3 bucket. In this way, future retrievals can be made on the media company’s S3 bucket instead.</li>
<li>Enable the Requester Pays feature in all of the startup’s S3 buckets to make the media company pay the cost of the data transfer from the buckets. ✅</li>
<li>Create a new billing account for the social media company by using AWS Organizations. Apply SCPs on the organization to ensure that each account has access only to its own resources and each other’s S3 buckets.</li>
<li>Provide cross-account access for the media company, which has permissions to access contents in the S3 bucket. Cross-account retrieval of S3 objects is charged to the account that made the request.</li>
</ol>
</li>
<li>
<p>A government agency recently decided to modernize its network infrastructure using AWS. They are developing a solution to store confidential files containing Personally Identifiable Information (PII) and other sensitive financial records of its citizens. All data in the storage solution must be encrypted both at rest and in transit. In addition, all of its data must also be replicated in two locations that are at least 450 miles apart from each other.</p>
<p>As a DevOps Engineer, what solution should you implement to meet these requirements?</p>
<ol>
<li>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3 SSE-C encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</li>
<li>Set up primary and secondary S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets. ✅</li>
<li>Set up primary and secondary Amazon S3 buckets in two separate AWS Regions that are at least 450 miles apart. Create an IAM role to enforce access to the buckets only through HTTPS. Set up a bucket policy to enforce Amazon S3-Managed Keys (SSE-S3) encryption on all objects uploaded to the bucket. Enable cross-region replication (CRR) between the two buckets.</li>
<li>Set up primary and secondary S3 buckets in two separate Availability Zones that are at least 450 miles apart. Create a bucket policy to enforce access to the buckets only through HTTPS and enforce AWS KMS encryption on all objects uploaded to the bucket. Enable Transfer Acceleration between the two buckets. Set up a KMS Customer Master Key (CMK) in the primary region for encrypting objects.</li>
</ol>
</li>
<li>
<p>A multinational corporation is using Amazon Athena to analyze the data sets stored in Amazon S3. The Data Analyst needs to implement a solution that will control the maximum amount of data scanned in the S3 bucket and ensure that if the query exceeded the limit, all the succeeding queries will be canceled.</p>
<p>Which of the following approach can be used to fulfill this requirement?</p>
<ol>
<li>Set up a workload management (WLM) assignment rule in the primary workgroup.</li>
<li>Set data limits in the per query data usage control. ✅</li>
<li>Integrate API Gateway with Amazon Athena. Configure an account-level throttling to control the queries in the S3 bucket.</li>
<li>Create an IAM policy that will throttle the data limits in the primary workgroup.</li>
</ol>
</li>
<li>
<p>A company is using Amazon Athena query with Amazon QuickSight to visualize the AWS CloudTrail logs. The Security Administrator created a custom Athena query that reads the CloudTrail logs and checks if there are IAM user accounts or credentials created in the past 29, 30 or 31 days (depending on the current month). However, the Administrator always gets an <code>Insufficient Permissions</code> error whenever she tries to run the query from Amazon QuickSight.</p>
<p>What is the MOST suitable solution that the Administrator should do to fix this issue?</p>
<ol>
<li>Disable the Log File Integrity feature in AWS CloudTrail.</li>
<li>Enable Cross-Origin Resource Sharing (CORS) in the S3 bucket that is used by Athena.</li>
<li>Use the AWS Account Root User to run the Athena query from Amazon QuickSight.</li>
<li>Make sure that Amazon QuickSight can access the S3 buckets used by Athena. ✅</li>
</ol>
</li>
</ol>
]]></content>
        </item>
        
        <item>
            <title>Intro to Data Analytics Fundamental with AWS Solutions</title>
            <link>https://ffflora.cat/posts/2021/01/intro-to-data-analytics-fundamental-with-aws-solutions/</link>
            <pubDate>Thu, 14 Jan 2021 00:19:31 -0800</pubDate>
            
            <guid>https://ffflora.cat/posts/2021/01/intro-to-data-analytics-fundamental-with-aws-solutions/</guid>
            <description>The challenges identified in many data analysis solutions can be summarized by five key challenges: volume, velocity, variety, veracity, and value.
Volume - Data Storage  Structured(10%) data is organized and stored in the form of values that are grouped into rows and columns of a table. Semistructured(10%) data is often stored in a series of key-value pairs that are grouped into elements within a file. Unstructured(80%) data is not structured in a consistent way.</description>
            <content type="html"><![CDATA[<p>The challenges identified in many data analysis solutions can be summarized by five key challenges: <strong>volume</strong>, <strong>velocity</strong>, <strong>variety</strong>, <strong>veracity</strong>, and <strong>value</strong>.</p>
<h2 id="volume---data-storage">Volume - Data Storage</h2>
<ul>
<li><strong>Structured</strong>(10%) data is organized and stored in the form of values that are grouped into rows and columns of a table.</li>
<li><strong>Semistructured(10%)</strong> data is often stored in a series of key-value pairs that are grouped into elements within a file.</li>
<li><strong>Unstructured</strong>(80%) data is not structured in a consistent way. Some data may have structure similar to semi-structured data but others may only contain metadata.</li>
</ul>
<p>When businesses have <strong>more data</strong> than they are able to <strong>process and analyze</strong>, they have a <strong>volume problem</strong>.</p>
<h3 id="amazon-s3">Amazon S3</h3>
<p>Advantages of using S3:</p>
<ul>
<li>Decoupling storage from processing</li>
<li>Parallelization, running process in parallel</li>
<li>Centralized location (reduce latency)</li>
</ul>
<h4 id="aws-s3-concepts">AWS S3 concepts</h4>
<p>First, Amazon S3 stores data as <strong>objects</strong> within <strong>buckets</strong>.</p>
<p>An <strong>object</strong> is composed of a file and any metadata that describes that file. To store an object in Amazon S3, you upload the file you want to store into a bucket. When you upload a file, you can set permissions on the object and add any metadata.</p>
<p><strong>Buckets</strong> are logical containers for objects. You can have one or more buckets in your account and can control access for each bucket individually. You control who can create, delete, and list objects in the bucket. You can also view access logs for the bucket and its objects and choose the geographical region where Amazon S3 will store the bucket and its contents.</p>
<h4 id="accessing-your-content"><strong>Accessing your content</strong></h4>
<p>Once objects have been stored in an Amazon S3 bucket, they are given an <strong>object key</strong>. Use this, along with the bucket name, to access the object.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201028180458759.png?raw=true" alt="image-20201028180458759"></p>
<p>An <strong>object key</strong> is the unique identifier for an object in a bucket. Because the combination of a bucket, key, and version ID uniquely identifies each object, you can think of Amazon S3 as a basic data map between &ldquo;bucket + key + version&rdquo; and the object itself. Every object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and (optionally) version.</p>
<h3 id="aws-data-lakes">AWS Data Lakes</h3>
<p>A data lake is a <strong>centralized repository</strong> that allows you to store <strong>structured</strong>, <strong>semistructured</strong>, and <strong>unstructured</strong> data at any scale.</p>
<h4 id="some-cautions-of-data-lakes">Some <strong>cautions</strong> of data lakes:</h4>
<ul>
<li>Single source of truth:	Be careful not to let your data lake become a swamp. Enforce proper organization and structure for all data entering the lake.</li>
<li>Store any type of data, regardless of structure: Be careful to ensure that data within the data lake is relevant and does not go unused. Train users on how to access the data, and set retention policies to ensure the data stays refreshed.</li>
<li>Can be analyzed using AI and machine learning: Be careful to learn how to use data in new ways. Don&rsquo;t limit analytics to typical data warehouse-style analytics. AI and machine learning offer significant insights.</li>
</ul>
<h4 id="benefits-of-a-data-lake-on-aws">Benefits of a data lake on AWS</h4>
<ul>
<li>Are a <strong>cost-effective data storage</strong> solution. You can durably store a nearly unlimited amount of data using Amazon S3.</li>
<li>Implement industry-leading <strong>security and compliance</strong>. AWS uses stringent data security, compliance, privacy, and protection mechanisms.</li>
<li>Allow you to take advantage of <strong>many different data collection and ingestion tools</strong> to ingest data into your data lake. These services include Amazon Kinesis for streaming data and AWS Snowball appliances for large volumes of on-premises data.</li>
<li>Help you to <strong>categorize and manage your data</strong> simply and efficiently. Use AWS Glue to understand the data within your data lake, prepare it, and load it reliably into data stores. Once AWS Glue catalogs your data, it is immediately searchable, can be queried, and is available for ETL processing.</li>
<li>Help you turn data into <strong>meaningful insights</strong>. Harness the power of purpose-built analytic services for a wide range of use cases, such as interactive analysis, data processing using Apache Spark and Apache Hadoop, data warehousing, real-time analytics, operational analytics, dashboards, and visualizations.</li>
</ul>
<h4 id="aws-lake-formation">AWS Lake Formation</h4>
<p>AWS Lake Formation makes it easy to set up a secure data lake in days. A data lake is a centralized, curated, and secured repository that stores all your data, both in its original form and when prepared for analysis. A data lake enables you to break down data silos and combine different types of analytics to gain insights and guide better business decisions. AWS Lake Formation is in preview only.</p>
<p>AWS Lake Formation is a service that organizes and curates data within Amazon S3 data lakes. This service ensures the security and compliance of items within the lake as well as orchestrates transformation jobs utilizing the data lake and other AWS services.</p>
<h3 id="data-warehouse">Data Warehouse</h3>
<p>structured data.</p>
<p>Fast, centralized data retrieval.</p>
<p>Data lakes and daka warehouses are two different storage systems. Data lakes are not  a replacement for data warehouses.</p>
<p>A data warehouse is a <strong>central repository</strong> of <strong>structured</strong> data from <strong>many</strong> data sources. This data is <strong>transformed</strong>, <strong>aggregated</strong>, and <strong>prepared</strong> for business reporting and analysis.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030210016708.png?raw=true" alt="image-20201030210016708"></p>
<p>A data warehouse is a central repository of information coming from one or more data sources. Data flows into a data warehouse from transactional systems, relational databases, and other sources. These data sources can include structured, semistructured, and unstructured data. These data sources are transformed into structured data before they are stored in the data warehouse.</p>
<p>Data is stored within the data warehouse using a schema. A schema defines how data is stored within tables, columns, and rows. The schema enforces constraints on the data to ensure integrity of the data. The transformation process often involves the steps required to make the source data conform to the schema. Following the first successful ingestion of data into the data warehouse, the process of ingesting and transforming the data can continue at a regular cadence.</p>
<p>Business analysts, data scientists, and decision makers access the data through business intelligence (BI) tools, SQL clients, and other analytics applications. Businesses use reports, dashboards, and analytics tools to extract insights from their data, monitor business performance, and support decision making. These reports, dashboards, and analytics tools are powered by data warehouses, which store data efficiently to minimize I/O and deliver query results at blazing speeds to hundreds and thousands of users concurrently.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030233712765.png?raw=true" alt="image-20201030233712765"></p>
<h3 id="data-marts">Data Marts</h3>
<p>A subset of a data warehouse.</p>
<p>A subset of data from a data warehouse is called a <strong>data mart</strong>. Data marts only <strong>focus on one subject or functional area</strong>. A warehouse might contain all relevant sources for an enterprise, but a data mart might store <strong>only a single department’s sources</strong>. Because data marts are generally a copy of data already contained in a data warehouse, they are often <strong>fast and simple to implement.</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030231123696.png?raw=true" alt="image-20201030231123696"></p>
<h3 id="amazon-redshift-spectrum">Amazon Redshift Spectrum</h3>
<p>Amazon Redshift overcomes all of data lake&rsquo;s negatives by providing a <strong>cloud-based, scalable, secure environment</strong> for your data warehouse. Amazon Redshift is easy to set up, deploy, and manage and provides up to 10 times faster performance than other data warehousing solutions.</p>
<table>
<thead>
<tr>
<th align="center"><strong>Benefits of Amazon Redshift</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><strong>Faster performance</strong>  10x faster than other data warehouses</td>
</tr>
<tr>
<td align="center"><strong>Easy to set up</strong>, deploy, and manage</td>
</tr>
<tr>
<td align="center"><strong>Secure</strong></td>
</tr>
<tr>
<td align="center"><strong>Scales quickly</strong> to meet your needs</td>
</tr>
</tbody>
</table>
<h4 id="data-warehouse-vs-data-lakes"><strong>Data Warehouse vs Data Lakes</strong></h4>
<p>For analysis to be most effective, it should be performed on data that has been processed and cleansed. This often means implementing an ETL operation to collect, cleanse, and transform the data. This data is then placed in a data warehouse. It is very common for data from many different parts of the organization to be combined into a single data warehouse.</p>
<p>Amazon Redshift is a data warehousing solution specially designed for workloads of all sizes. Amazon Redshift Spectrum even provides the ability to query data that is housed in an Amazon S3 data lake.<img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030234922475.png?raw=true" alt="image-20201030234922475"></p>
<p><strong>Data lakes extend data warehouses</strong></p>
<p>Data lakes provide customers a means for including unstructured and semistructured data in their analytics. Analytic queries can be run over cataloged data within a data lake. This extends the reach of analytics beyond the confines of a single data warehouse.</p>
<p>Businesses can securely store data coming from applications and devices in its native format, with high availability, durability, at low cost, and at any scale. Businesses can easily access and analyze data in a variety of ways using the tools and frameworks of their choice in a high-performance, cost-effective way without having to move large amounts of data between storage and analytics systems.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201030235045512.png?raw=true" alt="image-20201030235045512"></p>
<table>
<thead>
<tr>
<th>Characteristics</th>
<th>Data Warehouse</th>
<th>Data Lakes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data</strong></td>
<td>Relational from transactional systems, operational databases, and line of business applications</td>
<td>Non-relational and relational from IoT devices, websites, mobile apps, social media, and corporate applications</td>
</tr>
<tr>
<td><strong>Schema</strong></td>
<td>Designed prior to implementation (schema-on-write)</td>
<td>Written at the time of analysis  (schema-on-read)</td>
</tr>
<tr>
<td><strong>Price/ performance</strong></td>
<td>Fastest query results using higher cost storage</td>
<td>Query results getting faster using  low-cost storage</td>
</tr>
<tr>
<td><strong>Data quality</strong></td>
<td>Highly curated data that serves as the central version of the truth</td>
<td>Any data, which may or may not be curated (e.g., raw data)</td>
</tr>
<tr>
<td><strong>Users</strong></td>
<td>Business analysts</td>
<td>Data scientists, data developers, and business analysts (using curated data)</td>
</tr>
<tr>
<td><strong>Analytics</strong></td>
<td>Batch reporting, BI, and visualizations</td>
<td>Machine learning, predictive analytics, data discovery, and profiling.</td>
</tr>
</tbody>
</table>
<h3 id="data-storage-on-a-big-scale"><strong>Data storage on a BIG scale</strong></h3>
<p>We have discussed several recommendations for storing data:</p>
<ul>
<li>When storing <strong>individual objects or files</strong>, we recommend Amazon <strong>S3</strong>.</li>
<li>When storing <strong>massive volumes of data, both semistructured and unstructured</strong>, we recommend <strong>building a data lake on Amazon S3</strong>.</li>
<li>When storing massive amounts of <strong>structured data for complex analysis, we recommend storing your data in Amazon Redshift.</strong></li>
</ul>
<h3 id="apache-hadoop"><strong>Apache Hadoop</strong></h3>
<p>When many people think of working with a massive volume of fast-moving data, the first thing that comes to mind is Hadoop. Within AWS, Hadoop frameworks are implemented using Amazon <strong>EMR</strong> and AWS G<strong>l</strong>ue. These services implement the Hadoop framework to <strong>ingest, transform, analyze, and move results to analytical data stores.</strong></p>
<p>Hadoop uses a <strong>distributed processing architecture</strong>, in which a task is mapped to a cluster of commodity servers for processing. Each piece of work distributed to the cluster servers can be run or re-run on any of the servers. The cluster servers frequently use the <strong>Hadoop Distributed File System (HDFS)</strong> to store data locally for processing. The results of the computation performed by those servers are then reduced to a single output set. One node, designated as the master node, controls the distribution of tasks and can automatically handle server failures.</p>
<h4 id="benefits-of-using-apache-hadoop"><strong>Benefits of using Apache Hadoop</strong></h4>
<ul>
<li>
<p><strong>Handle uncertainty better</strong>: Hadoop facilitates data navigation, discovery, and one-time data analysis. With Hadoop, you can compensate for unexpected occurrences by analyzing large amounts of data quickly to form a response.</p>
</li>
<li>
<p><strong>Manage Data Variety</strong>: Hadoop can process structured, semistructured, or unstructured data. This includes virtually any data format currently available.</p>
<p>In addition to natively handling many types of data (such as XML, CSV, text, log files, objects, SQL, JSON, and binary), you can use Haddop to transform data into formats that allow better integration into your existing data sets. Also, you can store data with or without a schema and perform large-scale ETL operations to transform your data.</p>
</li>
<li>
<p><strong>Wide Selection of Solutions</strong>: Because Hadoop is open source, several ecosystem projects are available to help you analyze the multiple types of data Hadoop can process and analyze.</p>
<p>These projects give you tremendous flexibility when you are developing data analytics solutions. Hadoop’s programming frameworks (such as Hive and Pig) can support almost any data analytics use case for your applications.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031000543210.png?raw=true" alt="image-20201031000543210"></p>
</li>
<li>
<p><strong>Build for volume and velocity</strong>: Because of Hadoop’s distributed architecture, Hadoop clusters can handle tremendous amounts of data affordably. Adding additional data processing capability is as simple as adding additional servers to your cluster (horizontal scaling).</p>
</li>
</ul>
<h4 id="implementing-hadoop-with-amazon-emr"><strong>Implementing Hadoop with Amazon EMR</strong></h4>
<p>Amazon EMR is the AWS service that implements Hadoop frameworks. The service will ingest data from nearly any data source type at nearly any speed! Amazon EMR has the ability to implement two different file systems: <strong>HDFS</strong> or the <strong>Elastic MapReduce File System (EMRFS)</strong>. A file system is a set of organizational rules that govern how files are stored.</p>
<h5 id="hdfs"><strong>HDFS</strong></h5>
<p>To handle massive volumes of data rapidly, the processing system required a way to distribute the load of reading and writing files across tens or even hundreds of high-powered servers. HDFS is distributed storage allowing files to be read and written to clusters of servers in <strong>parallel</strong>. This dramatically reduces the overall length of each and every operation.</p>
<p>It is helpful to understand the inner workings of an HDFS cluster. An HDFS cluster primarily consists of a <em><strong>NameNode</strong></em>, <strong>which manages the file system metadata, and <em>DataNodes</em>, which store the actual data.</strong></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001440898.png?raw=true" alt="image-20201031001440898"><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001458920.png?raw=true" alt="image-20201031001458920"><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001516185.png?raw=true" alt="image-20201031001516185"></p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001546662.png?raw=true" alt="image-20201031001546662"><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20201031001632817.png?raw=true" alt=""></p>
<p>Amazon EMR is the AWS service that implements Hadoop frameworks. An Amazon EMR process begins by ingesting data from one or more data sources and storing that data within a file system. If using HDFS, the file system is stored as an elastic block store volume. This storage volume is ephemeral meaning that the storage is of a temporary nature. Once the data has been copied into the HDFS volume, the transformation and analysis of the data is performed. The results are then sent to an analytical data store, such as an Amazon S3 data lake or Amazon Redshift data warehouse.</p>
<h5 id="amazon-emrfs"><strong>Amazon EMRFS</strong></h5>
<p>Amazon EMR provides an alternative to HDFS: the EMR File System (EMRFS). EMRFS can help ensure that there is a persistent &ldquo;source of truth&rdquo; for HDFS data stored in Amazon S3. When implementing EMRFS, there is no need to copy data into the cluster before transforming and analyzing the data as with HDFS. EMRFS can catalog data within a data lake on Amazon S3. The time that is saved by eliminating the copy step can dramatically improve performance of the cluster.</p>
<h2 id="velocity---data-processing">Velocity - Data Processing</h2>
<ol>
<li>Batch processing
<ol>
<li>scheduled batch processing</li>
<li>Periodic batch processing</li>
</ol>
</li>
<li>Streaming Processing
<ol>
<li>Near-real-time</li>
<li>Real-time</li>
</ol>
</li>
</ol>
<h3 id="characteristics-of-data-processing-velocity"><strong>Characteristics of data processing velocity</strong></h3>
<p><strong>Velocities on collecting data</strong></p>
<table>
<thead>
<tr>
<th>Data Processing</th>
<th>Velocities on Collecting Data</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch</strong>:</td>
<td>Velocity is very predictable with batch processing. It amounts to large bursts of data transfer at scheduled intervals.</td>
</tr>
<tr>
<td><strong>Periodic</strong>:</td>
<td>Velocity is less predictable with periodic processing. The loss of scheduled events can put a strain on systems and must be considered.</td>
</tr>
<tr>
<td><strong>Near real-time</strong>:</td>
<td>Velocity is a huge concern with near real-time processing. These systems require data to be processed within minutes of the initial collection of the data. This can put tremendous strain on the processing and analytics systems involved.</td>
</tr>
<tr>
<td><strong>Real-time:</strong></td>
<td>Velocity is the paramount concern for real-time processing systems. Information cannot take minutes to process. It must be processed in <strong>seconds</strong> to be valid and maintain its usefulness.</td>
</tr>
</tbody>
</table>
<p><strong>Velocities on processing data</strong></p>
<table>
<thead>
<tr>
<th>Data Processing</th>
<th>Velocities on Processing Data</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Batch and periodic</strong>:</td>
<td>Once the data has been collected, processing can be done in a controlled environment. There is time to plan for the appropriate resources.</td>
</tr>
<tr>
<td><strong>Near real-time and real-time</strong>:</td>
<td>Collection of the data leads to an immediate need for processing. Depending on the complexity of the processing (cleansing, scrubbing, curation), this can slow down the velocity of the solution significantly. Plan accordingly.</td>
</tr>
</tbody>
</table>
<h3 id="attributes-of-batch-and-stream-processing"><em><strong>*Attributes of batch and stream processing*</strong></em></h3>
<p>The table below highlights the difference between batch and stream processing:</p>
<p><img src="https://raw.githubusercontent.com/ffflora/data-science-notes/master/archived-pics/aws/image-20201015002429812.png" alt="image-20201015002429812"></p>
<h3 id="business-challenge">Business Challenge</h3>
<p>The slower collection of data followed by a rapid processing requirement is a common challenge.</p>
<p>The rapid collection of data followed by the rapid processing of data is a common signature of streaming analytics.</p>
<h3 id="batch-processing-architecture"><strong>Batch processing architecture</strong></h3>
<p>Batch processing can be performed in different ways using AWS services. i.e.,</p>
<p><strong>S3, Lambda, EMR, Glue, Redshift.</strong></p>
<p>Amazon <strong>EMR</strong> provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances.</p>
<p>Amazon <strong>EMR</strong> is a managed service for executing highly complex, massive batch workloads. This service also allows highly complex analytic operations.</p>
<p>AWS <strong>Glue</strong> is a fully managed extract, transform, and load (ETL) service that makes it easy for you to prepare and load your data for analytics.</p>
<p>Amazon <strong>Redshift</strong> is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake.</p>
<p>Amazon <strong>Redshift</strong> is a managed data warehouse service that stores large amounts of transactional data for the purpose of analytics.</p>
<p>AWS <strong>Lambda</strong> is a serverless compute service that can be used to trigger processing operations in a batch processing system.</p>
<h3 id="stream-data-processing">Stream Data Processing</h3>
<h4 id="aws-knesis">AWS Knesis</h4>
<p><strong>Amazon Kinesis Data Firehose</strong> is the easiest way to capture, transform, and load data streams into AWS data stores for near real-time analytics with existing business intelligence tools.</p>
<p><strong>Amazon Kinesis Data Analytics</strong> is the easiest way to process data streams in real time with SQL or Java without having to learn new programming languages or processing frameworks.</p>
<p><strong>&hellip;</strong></p>
<h5 id="other-stream-processing-architecture">Other stream processing architecture:</h5>
<p><strong>Amazon Athena</strong> is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p>
<p><strong>Amazon QuickSight</strong> is a fast, cloud-powered business intelligence (BI) service that makes it easy for you to deliver insights to everyone in your organization.</p>
<h2 id="variety--data-structure-and-types">Variety – data structure and types</h2>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210109152122711.png?raw=true" alt="image-20210109152122711"></p>
<p>Surprisingly, the most common data types auch as <code>csv</code> and <code>JSON</code> are semistructured data.</p>
<h3 id="intro-to-structured-data-stores">Intro to Structured Data Stores</h3>
<h4 id="flat-file-data">Flat-file Data</h4>
<p>Flat-file data generally resides in a worksheet or spreadsheet.</p>
<h4 id="relational-databases"><strong>Relational databases</strong></h4>
<p>A process known as normalization helps a business take flat-file data and turn it into a relational database. Normalization is a set of rules that work together to reduce redundancy, increase reliability, and improve the consistency of data storage.</p>
<h4 id="types-of-information-systems"><strong>Types of information systems</strong></h4>
<p>There are two main ways—known as information systems—of organizing data within a relational database. The data can be organized to focus on the storage of transactions or the process of analyzing transactions.</p>
<p>Transactional databases are called online transaction processing (OLTP) databases. The data gathered by OLTP databases is often fed into another type of database that focuses on analyzing the transactional data. Online analytical processing (OLAP) databases gather data from OLTP systems for the purpose of organizing it for analytical operations.</p>
<h4 id="comparing-oltp-and-olap"><em><strong>*Comparing OLTP and OLAP*</strong></em></h4>
<table>
<thead>
<tr>
<th><strong>Characteristic</strong></th>
<th><strong>OLTP</strong></th>
<th><strong>OLAP</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nature</strong></td>
<td>Constant transactions (queries/updates)</td>
<td>Periodic large updates, complex queries</td>
</tr>
<tr>
<td><strong>Examples</strong></td>
<td>Accounting database, online retail transactions</td>
<td>Reporting, decision support</td>
</tr>
<tr>
<td><strong>Type</strong></td>
<td>Operational data</td>
<td>Consolidated data</td>
</tr>
<tr>
<td><strong>Data retention</strong></td>
<td>Short-term (2-6 months)</td>
<td>Long-term (2-5 years)</td>
</tr>
<tr>
<td><strong>Storage</strong></td>
<td>Gigabytes (GB)</td>
<td>Terabytes (TB)/petabytes (PB)</td>
</tr>
<tr>
<td><strong>Users</strong></td>
<td>Many</td>
<td>Few</td>
</tr>
<tr>
<td><strong>Protection</strong></td>
<td>Robust, constant data protection and fault tolerance</td>
<td>Periodic protection</td>
</tr>
</tbody>
</table>
<p>In an <strong>OLTP system</strong>, the most common queries are called lookup queries. These queries need to return several columns of data for each matching record. The filters on this data are generally based on the key columns in that table. In this type of system, you might query to get details for a specific order.</p>
<p>In an <strong>OLAP system</strong>, the most common queries are aggregate queries. These queries take large numbers of rows and reduce them to a single row by aggregating the values in one or more columns. In this type of system, you might query to find out the total number of items sold on a specific date.</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Row-based indexes</strong></th>
<th><strong>Columnar indexes</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Storage on disk</strong></td>
<td>Row by row</td>
<td>Column by column</td>
</tr>
<tr>
<td><strong>Read/write</strong></td>
<td>Best at random reads and writes</td>
<td>Best at sequential reads and writes</td>
</tr>
<tr>
<td><strong>Best for</strong></td>
<td>Returning full rows of data based on a key</td>
<td>Returning aggregations of column values</td>
</tr>
<tr>
<td><strong>Implementation</strong></td>
<td>Transactional systems</td>
<td>Analytical processing</td>
</tr>
<tr>
<td><strong>Data compression</strong></td>
<td>Low to medium compression can be achieved</td>
<td>High compression is the norm</td>
</tr>
</tbody>
</table>
<h5 id="aws-solution">AWS Solution:</h5>
<p>Within AWS, the <strong>Amazon Relational Database Service (Amazon RDS)</strong> provides the needs for many different relational database management systems. It supports the most popular database engines including Amazon Aurora, MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server.</p>
<p>**Amazon Redshift **is a fast, scalable data warehouse that makes it simple and cost effective to analyze all your data across your data warehouse and data lake. Amazon Redshift delivers 10 times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk. You can set up and deploy a new data warehouse in minutes and run queries across petabytes of data in your Amazon Redshift data warehouse and exabytes of data in your data lake built on Amazon S3.</p>
<p>Amazon Redshift implements columnar indexing to achieve the the right performance for analytical workloads.</p>
<h3 id="intro-to-semistructured-and-unstructured-data-stores">Intro to Semistructured and Unstructured Data Stores</h3>
<p>Semistructured and unstructured data are often stored in non-relational database systems, sometimes called NoSQL databases.</p>
<h4 id="data-schemas">Data Schemas</h4>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210110210251527.png?raw=true" alt="image-20210110210251527"></p>
<h5 id="aws-solutions"><strong>AWS Solutions:</strong></h5>
<p>Amazon <strong>DynamoDB</strong> is a key-value and document database that delivers single-digit millisecond performance at any scale. It&rsquo;s a fully managed, multiregion, multimaster database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second.</p>
<h3 id="graph-databases"><strong>Graph databases</strong></h3>
<p>Graph databases are purpose-built to store any type of data: structured, semistructured, or unstructured. The purpose for organization in a graph database is to navigate <strong>relationships</strong>. Data within the database is queried using specific languages associated with the software tool you have implemented.</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210111235609272.png?raw=true" alt="image-20210111235609272"></p>
<h5 id="the-aws-solution">The AWS Solution:</h5>
<p><strong>Amazon Neptune</strong> is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected data sets.</p>
<p>The core of Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency.</p>
<table>
<thead>
<tr>
<th><strong>Characteristic</strong></th>
<th><strong>Relational</strong></th>
<th><strong>Non-relational</strong></th>
<th><strong>Graph</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Representation</strong></td>
<td>Multiple tables, each containing columns and rows</td>
<td>Collection of documents Single table with keys and values</td>
<td>Collections of nodes and relationships</td>
</tr>
<tr>
<td><strong>Data</strong> <strong>design</strong></td>
<td>Normalized relational or dimensional data warehouse.</td>
<td>Denormalized document, wide column or key value</td>
<td>Denormalized entity relationship</td>
</tr>
<tr>
<td><strong>Optimized</strong></td>
<td>Optimized for storage</td>
<td>Optimized for compute</td>
<td>Optimized for relationships</td>
</tr>
<tr>
<td><strong>Query</strong> <strong>style</strong></td>
<td>Language: SQL</td>
<td>Language: many Uses object querying</td>
<td>Language: many Uses object querying</td>
</tr>
<tr>
<td><strong>Scaling</strong></td>
<td>Scale vertically</td>
<td>Scale horizontally</td>
<td>Hybrid</td>
</tr>
<tr>
<td><strong>Implementation</strong></td>
<td>OLTP business systems, OLAP data warehouse</td>
<td>OLTP web/mobile apps</td>
<td>OLTP web/mobile apps</td>
</tr>
</tbody>
</table>
<p>A multidemensional data warehouse is best suited for a <strong>relational database</strong>.</p>
<p>Log files are generally produced in the form of XML or JSON files, which are very well suited for storage in a <strong>document database</strong>.</p>
<p>Data collected from online gaming websites is often very rapid in generation and temporary in nature. This data is well suited for a <strong>key-value database</strong>.</p>
<p>Transactional data from a social subscription service could be stored in a relational database, but due to the social component, it would be better suited to the advantages gained by using a <strong>graph database</strong>.</p>
<h5 id="remark-horizontal-vs-vertical-scaling">Remark: Horizontal vs Vertical Scaling</h5>
<p>Scalable Dimensions:</p>
<ul>
<li>
<p>concurrent Connections,</p>
</li>
<li>
<p>CPU high or low</p>
</li>
<li>
<p>Memory(Quantity + Speed)</p>
</li>
<li>
<p>Netword Interfaces</p>
<p><img src="https://github.com/ffflora/data-science-notes/blob/master/archived-pics/aws/image-20210112001502324.png?raw=true" alt="image-20210112001502324"></p>
</li>
</ul>
<p>Scale Horizontally by adding more nodes and a load balancer.</p>
<p>Horizontal has more startup cost, but better efficiency and safer.</p>
<p>Scale Vertically by adding more powerful hardware.</p>
<p>Vertical has lower cost efficiency and a theoretical maximum.</p>
<h2 id="veracity---cleansing-and-transformation">Veracity - Cleansing and Transformation</h2>
<h3 id="definitions"><strong>Definitions</strong></h3>
<p><strong>Curation</strong> is the action or process of selecting, organizing, and looking after the items in a collection.
<strong>Data integrity</strong> is the maintenance and assurance of the accuracy and consistency of data over its entire lifecycle.
<strong>Data veracity</strong> is the degree to which data is accurate, precise, and trusted.</p>
<h3 id="understanding-database-consistency---acid-and-base"><strong>Understanding database consistency</strong> - ACID and BASE</h3>
<h4 id="acid"><strong>ACID</strong></h4>
<p><em><strong>ACID</strong> is an acronym for <strong>A</strong>tomicity, <strong>C</strong>onsistency, <strong>I</strong>solation, and <strong>D</strong>urability. It is a method for maintaining consistency and integrity in a structured database.</em></p>
<h4 id="base"><strong>BASE</strong></h4>
<p><strong>BASE</strong> is an acronym for <strong>B</strong>asically <strong>A</strong>vailable <strong>S</strong>oft state <strong>E</strong>ventually consistent. *It is a method for maintaining consistency and integrity in a structured or semistructured database.</p>
<table>
<thead>
<tr>
<th><strong>ACID compliance</strong></th>
<th><strong>BASE compliance</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Strong consistency</td>
<td>Weak consistency – stale data is OK</td>
</tr>
<tr>
<td>Isolation is key</td>
<td>Availability is key</td>
</tr>
<tr>
<td>Focus on committed results</td>
<td>Best effort results</td>
</tr>
<tr>
<td>Conservative (pessimistic) availability</td>
<td>Aggressive (optimistic) availability</td>
</tr>
</tbody>
</table>
<h4 id="aws-solution-1">AWS Solution</h4>
<p>When it comes to performing the data transformation component of ETL, there are two options within AWS: Amazon EMR and AWS Glue. These two services provide similar results but require different amounts of knowledge and time investment.</p>
<p><strong>Amazon EMR</strong> is a more hands-on approach to creating your data pipeline. This service provides a robust data collection and processing platform. Using this service requires you to have strong technical knowledge and know-how on your team. The upside of this is that you can create a more customized pipeline to fit your business needs. Additionally, your infrastructure costs may be lower than running the same workload on AWS Glue.</p>
<p><strong>AWS Glue</strong> is a serverless, managed ETL tool that provides a much more streamlined experience than Amazon EMR. This makes the service great for simple ETL tasks, but you will not have as much flexibility as with Amazon EMR. You can also use AWS Glue as a metastore for your final transformed data by using the AWS Glue Data Catalog. This catalog is a drop-in replacement for a Hive metastore.</p>
<h2 id="value---reporting-and-business-intelligence">Value - Reporting and Business Intelligence</h2>
<ul>
<li>
<p>What were total sales in April?</p>
<p>Questions relating to past events are answered using <strong>descriptive analytics.</strong></p>
</li>
<li>
<p>What is the year-over-year total sales for the Asia Pacific region?</p>
<p>Questions comparing current data sets to past data sets are answered using <strong>diagnostic analytics</strong>.</p>
</li>
<li>
<p>What is the projected growth for smoking-related hospitalizations next year?</p>
<p>Questions looking for predictions of future events are answered using <strong>predictive analytics</strong>.</p>
</li>
<li>
<p>What products should I buy if I like the Seattle Seahawks?</p>
<p>Questions looking for recommendations based on preferences or prior purchase history are answered using <strong>prescriptive analytics</strong>.</p>
</li>
<li>
<p>What is the average number of vehicles spotted by my video doorbell?</p>
<p>Questions that require analysis of video, images, and voice are answered using <strong>cognitive analytics.</strong></p>
</li>
</ul>
<h4 id="aws-solutions-1">AWS Solutions:</h4>
<p><strong>Amazon QuickSight</strong> is a fast, easy-to-use, cloud-powered business analytics service that makes it easy for all employees within an organization to build visualizations, perform one-time analyses, and quickly get business insights from their data, any time, on any device.</p>
<p>Interactive dashboards provide dashboard consumers with a self-service way to consume and slice and dice their data to answer questions without having to rely on a business intelligence team.</p>
]]></content>
        </item>
        
        <item>
            <title>SQL Notes (2)</title>
            <link>https://ffflora.cat/posts/2020/05/sql-notes-2/</link>
            <pubDate>Sun, 24 May 2020 00:00:43 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2020/05/sql-notes-2/</guid>
            <description>SQL Cheatsheet SELECT SELECT DISTINCT vend_id FROM Vendors LIMIT 5; LIMIT 5 OFFSET 5; // return 5 results start from line 5 ORDER BY  This should be the last clause after the SELECT statement. It is legal to ORDER BY the columns that are not being selected. Default is ordering from A - Z, if descending, need to add DESC in the end. DESC only works on col name that listed in front.</description>
            <content type="html"><![CDATA[<h1 id="sql-cheatsheet">SQL Cheatsheet</h1>
<h4 id="select">SELECT</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">DISTINCT</span> vend_id 
<span style="color:#66d9ef">FROM</span> Vendors 
<span style="color:#66d9ef">LIMIT</span> <span style="color:#ae81ff">5</span>;

<span style="color:#66d9ef">LIMIT</span> <span style="color:#ae81ff">5</span> <span style="color:#66d9ef">OFFSET</span> <span style="color:#ae81ff">5</span>; <span style="color:#f92672">//</span> <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">5</span> results <span style="color:#66d9ef">start</span> <span style="color:#66d9ef">from</span> line <span style="color:#ae81ff">5</span> 
</code></pre></div><h4 id="order-by">ORDER BY</h4>
<ul>
<li>This should be the <strong>last clause</strong> after the <code>SELECT</code> statement.</li>
<li>It is legal to ORDER BY the columns that are not being selected.</li>
<li>Default is ordering from A - Z, if descending, need to add <code>DESC</code> in the end.</li>
<li>DESC only works on col name that listed in front. If want to order multiple cols descendingly, need to add <code>DESC</code> after each col listed.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> prod_name
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_name;

<span style="color:#66d9ef">SELECT</span> prod_id, prod_price, prod_name
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_price, prod_name;  <span style="color:#75715e">-- ORDER BY multiple cols 
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> <span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>; <span style="color:#75715e">-- This works the same as the last line 
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> <span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span> <span style="color:#66d9ef">DESC</span> ;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_id, prod_price, prod_name
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_price <span style="color:#66d9ef">DESC</span>, prod_name; <span style="color:#75715e">-- DESC only works on col name that listed in front.
</span><span style="color:#75715e"></span>
</code></pre></div><h4 id="where">WHERE</h4>
<ul>
<li>WHERE first, then ORDER BY.</li>
<li>Use <code>IS NULL</code> with <code>WHERE</code> to check for null.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> prod_name, prod_price <span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> prod_price <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">10</span> 
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_price;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_name, prod_price <span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> prod_price <span style="color:#66d9ef">BETWEEN</span> <span style="color:#ae81ff">5</span> <span style="color:#66d9ef">AND</span> <span style="color:#ae81ff">10</span> 
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_price;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_name, prod_price <span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> prod_price <span style="color:#66d9ef">IS</span> <span style="color:#66d9ef">NULL</span>;
</code></pre></div><h4 id="more-filtering">More filtering</h4>
<ul>
<li>Use <code>AND</code> and <code>OR</code> operator</li>
<li><code>WHERE</code> clause could use with many <code>AND</code> or <code>OR</code></li>
<li>Usually <code>AND</code> has a higher priority than <code>OR</code>, therefore use parentheses in <code>WHERE</code> clause with these operators, which could remove all the ambiguity.</li>
<li><code>IN</code> has the same function with <code>OR</code>, but usually is more clear and more efficient, and it runs more faster.</li>
<li><code>IN</code> could include other <code>SELECT</code> clauses.</li>
<li><code>NOT</code> in <code>WHERE</code> clause only does one thing: negate all the conditions afterwards. It never use by itself.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL">    
<span style="color:#66d9ef">SELECT</span> prod_id, prod_price, prod_name
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> vend_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;DLL01&#39;</span> <span style="color:#66d9ef">AND</span> prod_price <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">4</span>; 

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_name, prod_price
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> (vend_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;DLL01&#39;</span> <span style="color:#66d9ef">OR</span> vend_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;BRS01&#39;</span>) <span style="color:#75715e">-- Add parentheses, otherwise will execute AND first and output wrong results.
</span><span style="color:#75715e"></span>    <span style="color:#66d9ef">AND</span> prod_price <span style="color:#f92672">&gt;=</span><span style="color:#ae81ff">10</span>;
    
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_name, prod_price
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> vend_id <span style="color:#66d9ef">IN</span> ( <span style="color:#e6db74">&#39;DLL01&#39;</span>, <span style="color:#e6db74">&#39;BRS01&#39;</span> ) 
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_name;

<span style="color:#75715e">-- SAME AS ⬇️--
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_name, prod_price
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> vend_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;DLL01&#39;</span> <span style="color:#66d9ef">OR</span> vend_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;BRS01&#39;</span> 
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_name;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_name
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> <span style="color:#66d9ef">NOT</span> vend_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;DLL01&#39;</span> 
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> prod_name;

<span style="color:#75715e">-- third line as same as: --
</span><span style="color:#75715e"></span><span style="color:#66d9ef">WHERE</span> vend_id <span style="color:#f92672">!=</span> <span style="color:#e6db74">&#39;DLL01&#39;</span> 

</code></pre></div><h4 id="wildcard">Wildcard</h4>
<ul>
<li>wildcard: special character that matches some keywords</li>
<li>search pattern: a combination of keyword and/or wildcard</li>
<li><code>LIKE</code> is ( °◅° ) predicate, rather than an operator.</li>
<li><code>%</code>: any character that shows up for any times.
<ul>
<li>Useful situation: to search for some email address that match some condition: <code>WHERE email LIKE 'b%@vii.im'</code></li>
<li>But <code>%</code> won&rsquo;t match any <code>NULL</code>: <code>WHERE prod_name LIKE '%'</code> never returns results that the values are <code>NULL</code>.</li>
</ul>
</li>
<li><code>_</code> has the same functionality as <code>%</code>, but it only  matches for single character.</li>
<li><code>[]</code> : a set of characters, which uses to match the keywords in specific location. This search pattern uses any <strong>one</strong> char of the set.
<ul>
<li>In this case, negation is represented by <code>^</code> sign, or <code>NOT</code> operator.</li>
</ul>
</li>
<li>Don&rsquo;t overuse wildcards: operators &gt; wildcards.</li>
<li>If place wildcard in the beginning of the search process, would make the process <em>slow</em>.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> prod_id, prod_name
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> prod_name <span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;Fish%&#39;</span>;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_id, prod_name
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> prod_name <span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;__ inch teddy bear&#39;</span>; <span style="color:#75715e">-- notice that the space after the __ is needed. 
</span><span style="color:#75715e"></span>
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> cust_contact
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_contact <span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;[JM]%&#39;</span> <span style="color:#75715e">-- names that start with J or M, and follow with any characters
</span><span style="color:#75715e"></span><span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> cust_contact;

<span style="color:#75715e">/*Output*/</span>
Jim Jones
John Smith 
Michelle Green

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> cust_contact
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_contact <span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;^[JM]%&#39;</span> <span style="color:#75715e">-- names that not start with J or M, and follow with any characters
</span><span style="color:#75715e"></span><span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> cust_contact;

<span style="color:#75715e">-- Same as: --
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> cust_contact
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> <span style="color:#66d9ef">NOT</span> cust_contact <span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;[JM]%&#39;</span> 
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> cust_contact;

</code></pre></div><h4 id="calculated-field">Calculated Field</h4>
<ul>
<li>
<p>Example of calculated field:</p>
<ul>
<li>company&rsquo;s name and address, but usually they are in different tables.</li>
<li>city, state, zip code usually in different tables, but they are needed together when print the address</li>
<li>calculation like mean, avg</li>
</ul>
</li>
<li>
<p>They are not physically in the database, but they are created when running the <code>SELECT</code> clauses.</p>
</li>
<li>
<p>Use <code>+</code> or <code>||</code> to concatenate cols.</p>
</li>
<li>
<p>TRIM:</p>
<ul>
<li>remember to use <code>RTRIM()</code> to get rid of the extra spaces on the right of the value.</li>
<li><code>LTRIM()</code>: get rid of the extra space of the left of the value.</li>
<li><code>TRIM()</code>: get rid of the space of both sides.</li>
</ul>
</li>
<li>
<p>rename: <code>AS</code></p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> vend_name <span style="color:#f92672">||</span> <span style="color:#e6db74">&#39; (&#39;</span> <span style="color:#f92672">||</span> vend_country <span style="color:#f92672">||</span> <span style="color:#e6db74">&#39;)&#39;</span> <span style="color:#75715e">-- use || in SQLite, use + in SQL
</span><span style="color:#75715e"></span><span style="color:#66d9ef">FROM</span> Vendors
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> vend_name;

<span style="color:#75715e">/* Use RTRIM() to get rid of the extra space to the right of the value. */</span>
<span style="color:#66d9ef">SELECT</span> RTRIM(vend_name) <span style="color:#f92672">||</span> <span style="color:#e6db74">&#39; (&#39;</span> <span style="color:#f92672">||</span> RTRIM(vend_country) <span style="color:#f92672">||</span> <span style="color:#e6db74">&#39;)&#39;</span> 
<span style="color:#66d9ef">FROM</span> Vendors
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> vend_name;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> prod_id,
       quantity,
       item_price,
       quantity<span style="color:#f92672">*</span>item_price <span style="color:#66d9ef">AS</span> expanded_price
<span style="color:#66d9ef">FROM</span> OrderItems
<span style="color:#66d9ef">WHERE</span> order_num <span style="color:#f92672">=</span> <span style="color:#ae81ff">20008</span>;

</code></pre></div><h4 id="more-functions">More functions</h4>
<ul>
<li><code>UPPER()</code>, <code>LOWER()</code></li>
<li><code>LENGTH()</code>,<code>LTRIM()</code>,<code>RTRIM()</code>,<code>TRIM()</code></li>
<li><code>RIGHT()</code>,<code>LEFT()</code>: returns the character to the right/left of the string.</li>
<li><code>SOUNDEX()</code>: returns the value that is sound-like the keyword.
<ul>
<li>For Example, a name in the table is &lsquo;Michelle&rsquo;, but the actual value is &lsquo;Michael&rsquo;,  if search by the real name will have nothing return, thus in this case use <code>SOUNDEX()</code></li>
</ul>
</li>
<li>Functions are not all portable between different SQL systems.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> cust_name, cust_contact
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> SOUNDEX(cust_contact) <span style="color:#f92672">=</span> SOUNDEX(<span style="color:#e6db74">&#39;Michael Green&#39;</span>); 

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> order_num
<span style="color:#66d9ef">FROM</span> Orders
<span style="color:#66d9ef">WHERE</span> strftime(<span style="color:#e6db74">&#39;%Y&#39;</span>, order_date) <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;2012&#39;</span>;
</code></pre></div><h4 id="data-manipulation-functions">Data Manipulation Functions</h4>
<ul>
<li>Aggregation functions are most portable functions.
<ul>
<li>
<p>line number of some cols: <code>COUNT()</code></p>
<ul>
<li><code>COUNT(*)</code> counts all the lines in a col, no matter each line is NULL or not.</li>
<li><code>COUNT(col)</code> counts all the lines that have valid values.</li>
</ul>
</li>
<li>
<p>sum of certain values: <code>SUM()</code></p>
<ul>
<li>it ignore the NULL lines.</li>
</ul>
</li>
<li>
<p><code>MAX()</code>,<code>MIN()</code>,<code>AVG()</code></p>
<ul>
<li><code>AVG()</code> can only use on a single col, and it neglects the lines that the values are NULL.</li>
<li>when dealing with categorical data, <code>MAX()</code> return the last line of the col after <strong>sorting</strong>.</li>
<li><code>MAX()</code> ignore NULL lines.</li>
</ul>
</li>
<li>
<p>&hellip;</p>
</li>
</ul>
</li>
<li><strong>DISTINCT</strong> values</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">AVG</span>(prod_price) <span style="color:#66d9ef">AS</span> avg_price
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> vend_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;DLL01&#39;</span>; <span style="color:#75715e">-- calculate the avg price from certain vendor 
</span><span style="color:#75715e"></span>
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#75715e">/* Subquery */</span>
<span style="color:#66d9ef">SELECT</span> prod_name, <span style="color:#66d9ef">AVG</span>(prod_price) <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">avg</span> 
<span style="color:#66d9ef">FROM</span>
   (<span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">DISTINCT</span> prod_name,prod_price <span style="color:#66d9ef">FROM</span> Products);
<span style="color:#f92672">&gt;&gt;&gt;</span> <span style="color:#ae81ff">8</span> inch teddy bear<span style="color:#f92672">|</span><span style="color:#ae81ff">6</span>.<span style="color:#ae81ff">82333333333333</span>

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#75715e">/* Combined Aggregation Functions */</span>
<span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#66d9ef">AS</span> num_items,
       <span style="color:#66d9ef">MIN</span>(prod_price) <span style="color:#66d9ef">AS</span> price_min,
       <span style="color:#66d9ef">MAX</span>(prod_price) <span style="color:#66d9ef">AS</span> price_max,
       <span style="color:#66d9ef">AVG</span>(prod_price) <span style="color:#66d9ef">AS</span> price_avg
 <span style="color:#66d9ef">FROM</span> Products;


</code></pre></div><h4 id="group-by--having">GROUP BY &amp; HAVING</h4>
<ul>
<li>All the cols need to be listed in the <code>GROUP BY</code> clause, except aggregation functions.</li>
<li>Can&rsquo;t group by aggregation functions.</li>
<li>If there are NULL in the lines, all the NULLs will be returned as a group.</li>
<li>Orders in the scripts:
WHERE, GROUP BY, ORDER BY</li>
<li><code>WHERE</code> is about filtering by lines; <code>HAVING</code> is about filtering by groups.</li>
<li>Another way to understand <code>WHERE</code> and <code>HAVING</code>: <code>WHERE</code> filters before grouping, yet <code>HAVING</code> filters after grouping. The lines excluded by <code>WHERE</code> are not in the groups.</li>
<li>Differences btw <code>ORDER BY</code> vs <code>GROUP BY</code>:</li>
</ul>
<table>
<thead>
<tr>
<th>ORDER BY</th>
<th>GROUP BY</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sorts generated output.</td>
<td>Grouping rows. However the output may not be in group order.</td>
</tr>
<tr>
<td>Can use it upon any cols (includes the cols that are not selected)</td>
<td>Only selected cols or expressions cols may be used, and every selected col expression must be used.</td>
</tr>
<tr>
<td>Never required.</td>
<td>Required if using cols(or expressions) with agg functions.</td>
</tr>
</tbody>
</table>
<ul>
<li>Should include <code>ORDER BY</code> subquery to the <code>GROUP BY</code>, in order to get the right sorted results.</li>
</ul>
<p>Conclusion (Clauses and ordering)</p>
<table>
<thead>
<tr>
<th>Clause</th>
<th>Description</th>
<th>Is-required</th>
</tr>
</thead>
<tbody>
<tr>
<td>SELECT</td>
<td>Cols(expressions) to be returned</td>
<td>√</td>
</tr>
<tr>
<td>FROM</td>
<td>Table to retrieve data  from</td>
<td>only if select data from table</td>
</tr>
<tr>
<td>WHERE</td>
<td>row-level filtering</td>
<td>x</td>
</tr>
<tr>
<td>GROUP BY</td>
<td>Groups specs</td>
<td>yes if only aggregation calculation are involved by groups</td>
</tr>
<tr>
<td>HAVING</td>
<td>GROUP-level filtering</td>
<td>x</td>
</tr>
<tr>
<td>ORDER BY</td>
<td>Sort output</td>
<td>x</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> vend_id, <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#66d9ef">AS</span> num_prods
<span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> vend_id;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#75715e">/* Return the customers that have more than two orders. */</span>
<span style="color:#66d9ef">SELECT</span> cust_id, <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#66d9ef">AS</span> orders <span style="color:#66d9ef">FROM</span> Orders
<span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> cust_id
<span style="color:#66d9ef">HAVING</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">2</span>;
<span style="color:#75715e">-- Can&#39;t use WHERE here, because the filtering is based on an aggregation function.
</span><span style="color:#75715e"></span>
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#75715e">/* Return the vendors that have more than two products that the prices are at least 4. */</span>
<span style="color:#66d9ef">SELECT</span> vend_id, <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#66d9ef">AS</span> num_prods <span style="color:#66d9ef">FROM</span> Products
<span style="color:#66d9ef">WHERE</span> prod_price <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">4</span>
<span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> vend_id
<span style="color:#66d9ef">HAVING</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">2</span>;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#75715e">/* Return the order numbers and number of items sold, which are at least three, sort by items number */</span>
<span style="color:#66d9ef">SELECT</span> order_num, <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#66d9ef">AS</span> items <span style="color:#66d9ef">FROM</span> OrderItems
<span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> order_num
<span style="color:#66d9ef">HAVING</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">3</span>
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> items, order_num;
</code></pre></div><h4 id="subquery">SUBQUERY</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#75715e">/* Return the customer info which have bought the product PGAN01 */</span>
<span style="color:#66d9ef">SELECT</span> cust_name,cust_contact 
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_id <span style="color:#66d9ef">IN</span> (<span style="color:#66d9ef">SELECT</span> cust_id
    <span style="color:#66d9ef">FROM</span> Orders
    <span style="color:#66d9ef">WHERE</span> order_num <span style="color:#66d9ef">IN</span> (<span style="color:#66d9ef">SELECT</span> order_num
                        <span style="color:#66d9ef">FROM</span> OrderItems
                        <span style="color:#66d9ef">WHERE</span> prod_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;RGAN01&#39;</span>));
                        
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">SELECT</span> cust_name,
       cust_state,
        (<span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>)
        <span style="color:#66d9ef">FROM</span> Orders
        <span style="color:#66d9ef">WHERE</span> Orders.cust_id <span style="color:#f92672">=</span> Customers.cust_id) <span style="color:#66d9ef">AS</span> orders
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> cust_name;
</code></pre></div><h4 id="join">JOIN</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> vend_name, prod_name, prod_price <span style="color:#66d9ef">FROM</span> Vendors, Products
<span style="color:#66d9ef">WHERE</span> Vendors.vend_id <span style="color:#f92672">=</span> Products.vend_id;

<span style="color:#75715e">-- Same as : -- 
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> vend_name, prod_name, prod_price <span style="color:#66d9ef">FROM</span> Vendors <span style="color:#66d9ef">INNER</span> <span style="color:#66d9ef">JOIN</span> Products
<span style="color:#66d9ef">ON</span> Vendors.vend_id <span style="color:#f92672">=</span> Products.vend_id;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> cust_name, cust_contact
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_id <span style="color:#66d9ef">IN</span> (<span style="color:#66d9ef">SELECT</span> cust_id
                  <span style="color:#66d9ef">FROM</span> Orders
                  <span style="color:#66d9ef">WHERE</span> order_num <span style="color:#66d9ef">IN</span> (<span style="color:#66d9ef">SELECT</span> order_num
        <span style="color:#66d9ef">FROM</span> OrderItems
        <span style="color:#66d9ef">WHERE</span> prod_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;RGAN01&#39;</span>));
        
<span style="color:#75715e">-- Same as : --
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> cust_name, cust_contact
<span style="color:#66d9ef">FROM</span> Customers, Orders, OrderItems
<span style="color:#66d9ef">WHERE</span> Customers.cust_id <span style="color:#f92672">=</span> Orders.cust_id
<span style="color:#66d9ef">AND</span> OrderItems.order_num <span style="color:#f92672">=</span> Orders.order_num <span style="color:#66d9ef">AND</span> prod_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;RGAN01&#39;</span>;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> cust_name
 <span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_contact <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Jim Jones&#39;</span>

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> Customers.cust_id, Orders.order_num <span style="color:#66d9ef">FROM</span> Customers <span style="color:#66d9ef">LEFT</span> <span style="color:#66d9ef">OUTER</span> <span style="color:#66d9ef">JOIN</span> Orders
<span style="color:#66d9ef">ON</span> Customers.cust_id <span style="color:#f92672">=</span> Orders.cust_id;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#75715e">/* Return all the customers with the number of orders. */</span>
<span style="color:#66d9ef">SELECT</span> Customers.cust_id, <span style="color:#66d9ef">COUNT</span>(Orders.order_num) <span style="color:#66d9ef">AS</span> num_ord
<span style="color:#66d9ef">FROM</span> Customers <span style="color:#66d9ef">INNER</span> <span style="color:#66d9ef">JOIN</span> Orders
<span style="color:#66d9ef">ON</span> Customers.cust_id <span style="color:#f92672">=</span> Orders.cust_id
<span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> Customers.cust_id;
</code></pre></div><h4 id="combine-queries">Combine queries</h4>
<ul>
<li><code>UNION</code> can combine multiple SELECT results, return as one set.</li>
<li>When use <code>UNION</code>, each selection must have same cols, expressions or aggregation functions;</li>
<li>The datatype of the selection must be compatible when use <code>UNION</code></li>
<li>It is by default that <code>UNION</code> would remove duplicated rows, to avoid that, use <code>UNION ALL</code>.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> cust_name, cust_contact, cust_email <span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_state <span style="color:#66d9ef">IN</span> (<span style="color:#e6db74">&#39;IL&#39;</span>,<span style="color:#e6db74">&#39;IN&#39;</span>,<span style="color:#e6db74">&#39;MI&#39;</span>);
<span style="color:#66d9ef">UNION</span>
<span style="color:#66d9ef">SELECT</span> cust_name, cust_contact, cust_email <span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Fun4All&#39;</span>;

<span style="color:#f92672">&gt;&gt;&gt;</span> <span style="color:#75715e">--OUTPUT--
</span><span style="color:#75715e"></span>Fun4All
Fun4All
Village Toys
The Toy Store

Denise L. Stephens 
Jim Jones
John Smith
Kim Howard

dstephens<span style="color:#f92672">@</span>fun4all.com 
jjones<span style="color:#f92672">@</span>fun4all.com 
sales<span style="color:#f92672">@</span>villagetoys.com 
<span style="color:#66d9ef">NULL</span>

<span style="color:#75715e">/* Alternative way */</span>
<span style="color:#66d9ef">SELECT</span> cust_name, cust_contact, cust_email
<span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_state <span style="color:#66d9ef">IN</span> (<span style="color:#e6db74">&#39;IL&#39;</span>,<span style="color:#e6db74">&#39;IN&#39;</span>,<span style="color:#e6db74">&#39;MI&#39;</span>)
<span style="color:#66d9ef">OR</span> cust_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Fun4All&#39;</span>;

</code></pre></div><h4 id="insert-create">INSERT, CREATE</h4>
<ul>
<li><code>INSERT INTO</code> + <code>VALUES</code> to an already exist table;</li>
<li><code>INSERT INTO</code>+ <code>SELECT</code> to an already exist table from another table;</li>
<li>Copy cols and values from an old table to a new table: <code>SELECT INTO</code></li>
<li><code>CREATE TEMPORARY TABLE</code></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL">
<span style="color:#66d9ef">INSERT</span> <span style="color:#66d9ef">INTO</span> Customers(cust_id,
                      cust_name,
                      cust_address,
                      cust_city,
                      cust_state,
                      cust_zip,
                      cust_country,
                      cust_contact,
                      cust_email)
<span style="color:#66d9ef">VALUES</span>(<span style="color:#e6db74">&#39;1000000006&#39;</span>,
       <span style="color:#e6db74">&#39;Toy Land&#39;</span>,
       <span style="color:#e6db74">&#39;123 Any Street&#39;</span>,
       <span style="color:#e6db74">&#39;New York&#39;</span>,
       <span style="color:#e6db74">&#39;NY&#39;</span>,
       <span style="color:#e6db74">&#39;11111&#39;</span>,
       <span style="color:#e6db74">&#39;USA&#39;</span>,
       <span style="color:#66d9ef">NULL</span>,
       <span style="color:#66d9ef">NULL</span>);
       
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">INSERT</span> <span style="color:#66d9ef">INTO</span> Customers(cust_id,
                      cust_contact,
                    cust_email,
                    cust_name,
                    cust_address,
                    cust_city,
                    cust_state,
                    cust_zip,
                    cust_country)
 <span style="color:#66d9ef">SELECT</span> cust_id,
       cust_contact,
       cust_email,
       cust_name,
       cust_address,
       cust_city,
       cust_state,
       cust_zip,
       cust_country
<span style="color:#66d9ef">FROM</span> CustNew;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span>
<span style="color:#66d9ef">INTO</span> CustCopy
<span style="color:#66d9ef">FROM</span> Customers;

<span style="color:#75715e">/* But this line has been used slightly differently in MariaDB、MySQL、Oracle、PostgreSQL and SQLite: */</span>

<span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> CustCopy <span style="color:#66d9ef">AS</span>
<span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span> <span style="color:#66d9ef">FROM</span> Customers;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TEMPORARY</span> <span style="color:#66d9ef">TABLE</span> Scandals <span style="color:#66d9ef">AS</span> 
(
    <span style="color:#66d9ef">SELECT</span> <span style="color:#f92672">*</span>
    <span style="color:#66d9ef">FROM</span> shoes
    <span style="color:#66d9ef">WHERE</span> shoe_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;scandals&#39;</span>
);
</code></pre></div><h4 id="update--delete">UPDATE &amp; DELETE</h4>
<ul>
<li>Use <code>UPDATE</code> carefully, include <code>WHERE</code></li>
<li>If want to delete some values, set them to <code>NULL</code>.</li>
<li>If want to delete all the rows from the table, use <code>TRUNCATE TABLE</code> than <code>DELETE</code>, it does the same as <code>DELETE</code> but it runs faster.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">UPDATE</span> Customers
<span style="color:#66d9ef">SET</span> cust_email <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;kim@thetoystore.com&#39;</span> <span style="color:#66d9ef">WHERE</span> cust_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;1000000005&#39;</span>;

<span style="color:#75715e">-- Update more than one cols:--
</span><span style="color:#75715e"></span><span style="color:#66d9ef">UPDATE</span> Customers
<span style="color:#66d9ef">SET</span> cust_contact <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Sam Roberts&#39;</span>,
cust_email <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;sam@toyland.com&#39;</span> <span style="color:#66d9ef">WHERE</span> cust_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;1000000006&#39;</span>;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">DELETE</span> <span style="color:#66d9ef">FROM</span> Customers
<span style="color:#66d9ef">WHERE</span> cust_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;1000000006&#39;</span>;


</code></pre></div><h4 id="create">CREATE</h4>
<ul>
<li>Define a new Table: <code>CREATE</code>
<img src="/Users/flora/git/data-science-notes/SQL/media/15903058475682/15905621498702.jpg" alt="-w507"></li>
<li>Make changes to the created table: <code>ALTER TABLE</code></li>
<li>Delete table: <code>DROP TABLE</code></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">ALTER</span> <span style="color:#66d9ef">TABLE</span> Vendors
<span style="color:#66d9ef">ADD</span> vend_phone CHAR(<span style="color:#ae81ff">20</span>);

<span style="color:#66d9ef">ALTER</span> <span style="color:#66d9ef">TABLE</span> Vendors
<span style="color:#66d9ef">DROP</span> <span style="color:#66d9ef">COLUMN</span> vend_phone;

<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span><span style="color:#66d9ef">DROP</span> <span style="color:#66d9ef">TABLE</span> CustCopy;


</code></pre></div><h4 id="views">VIEWS</h4>
<ul>
<li>Views return the results from the search queries that happened elsewhere, thus views don&rsquo;t contain the actual data or tables, they just the results.</li>
<li><code>CREATE VIEW </code>, <code>DROP VIEW</code></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">VIEW</span> ProductCustomers <span style="color:#66d9ef">AS</span>
<span style="color:#66d9ef">SELECT</span> cust_name, cust_contact, prod_id <span style="color:#66d9ef">FROM</span> Customers, Orders, OrderItems
<span style="color:#66d9ef">WHERE</span> Customers.cust_id <span style="color:#f92672">=</span> Orders.cust_id
<span style="color:#66d9ef">AND</span> OrderItems.order_num <span style="color:#f92672">=</span> Orders.order_num;


<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span>
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span>
<span style="color:#75715e">--------------------------------
</span><span style="color:#75715e"></span>
<span style="color:#75715e">--------------------------------
</span></code></pre></div><h4 id="window-functions">Window Functions</h4>
<ul>
<li>
<p>A window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. But unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row — the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result.</p>
</li>
<li>
<p><code>OVER</code> and <code>PARTITION BY</code></p>
</li>
<li>
<p>You can’t use window functions and standard aggregations in the same query. More specifically, you can’t include window functions in a <code>GROUP BY</code> clause.</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> standard_amt_usd,
       <span style="color:#66d9ef">SUM</span>(standard_amt_usd) OVER (<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> occurred_at) <span style="color:#66d9ef">AS</span> running_total
<span style="color:#66d9ef">FROM</span> orders

<span style="color:#75715e">----------------------------
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">SELECT</span> standard_amt_usd,
       DATE_TRUNC(<span style="color:#e6db74">&#39;year&#39;</span>, occurred_at) <span style="color:#66d9ef">as</span> <span style="color:#66d9ef">year</span>,
       <span style="color:#66d9ef">SUM</span>(standard_amt_usd) OVER (PARTITION <span style="color:#66d9ef">BY</span> DATE_TRUNC(<span style="color:#e6db74">&#39;year&#39;</span>, occurred_at) <span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> occurred_at) <span style="color:#66d9ef">AS</span> running_total
<span style="color:#66d9ef">FROM</span> orders
</code></pre></div>]]></content>
        </item>
        
        <item>
            <title>SQL Notes (1)</title>
            <link>https://ffflora.cat/posts/2020/05/sql-notes-1/</link>
            <pubDate>Mon, 04 May 2020 00:10:29 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2020/05/sql-notes-1/</guid>
            <description>Database level： CREATE DATABASE test; DROP DATABASE test; SHOW DATABASE; -- show all the databases USE test; -- use some specific database In a Database: SHOW TABLE; /*If you want to modify or update a certain table*/ ALTER TABLE table_name; ADD column_name datatype; DROP COLUMN column_name; SELECT DISTINCT col1,col2 FROM table_name; -- A col may contain duplicate values SELECT ... FROM... WHERE conditions; -- filter the rows returned from FROM COUNT SELECT COUNT(*) FROM table_name; -- In real life, * is not a good idea.</description>
            <content type="html"><![CDATA[<h4 id="database-level">Database level：</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">DATABASE</span> test;
<span style="color:#66d9ef">DROP</span> <span style="color:#66d9ef">DATABASE</span> test;
<span style="color:#66d9ef">SHOW</span> <span style="color:#66d9ef">DATABASE</span>; <span style="color:#75715e">-- show all the databases
</span><span style="color:#75715e"></span>USE test; <span style="color:#75715e">-- use some specific database
</span></code></pre></div><h4 id="in-a-database">In a Database:</h4>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SHOW</span> <span style="color:#66d9ef">TABLE</span>;

<span style="color:#75715e">/*If you want to modify or update a certain table*/</span>
<span style="color:#66d9ef">ALTER</span> <span style="color:#66d9ef">TABLE</span> <span style="color:#66d9ef">table_name</span>;
<span style="color:#66d9ef">ADD</span> <span style="color:#66d9ef">column_name</span> datatype;
<span style="color:#66d9ef">DROP</span> <span style="color:#66d9ef">COLUMN</span> <span style="color:#66d9ef">column_name</span>;
<span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">DISTINCT</span> col1,col2 <span style="color:#66d9ef">FROM</span> <span style="color:#66d9ef">table_name</span>; <span style="color:#75715e">-- A col may contain duplicate values
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> ... <span style="color:#66d9ef">FROM</span>... <span style="color:#66d9ef">WHERE</span> conditions; <span style="color:#75715e">-- filter the rows returned from FROM
</span></code></pre></div><h5 id="count">COUNT</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#f92672">*</span>) <span style="color:#66d9ef">FROM</span> <span style="color:#66d9ef">table_name</span>; <span style="color:#75715e">-- In real life, * is not a good idea.
</span><span style="color:#75715e"></span><span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">COUNT</span>(col) <span style="color:#66d9ef">FROM</span> <span style="color:#66d9ef">table_name</span>;
<span style="color:#66d9ef">SELECT</span> <span style="color:#66d9ef">COUNT</span>(<span style="color:#66d9ef">DISTINCT</span> col) <span style="color:#66d9ef">FROM</span> ... ;
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#75715e">/*LIMIT is useful when want to get all cols but not all rows*/</span>
<span style="color:#66d9ef">LIMIT</span> <span style="color:#ae81ff">5</span>; <span style="color:#75715e">-- 5 rows and all cols
</span><span style="color:#75715e"></span>
<span style="color:#75715e">/* ORDER BY: sorting */</span>
<span style="color:#66d9ef">SELECT</span>... <span style="color:#66d9ef">FROM</span>... <span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> col1 <span style="color:#66d9ef">ASC</span><span style="color:#f92672">/</span><span style="color:#66d9ef">DESC</span> 

ROUND(...,<span style="color:#ae81ff">2</span>); <span style="color:#75715e">-- round to 2 decimal places
</span></code></pre></div><h5 id="between-in-like">BETWEEN, IN, LIKE</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL">value <span style="color:#66d9ef">BETWEEN</span> low <span style="color:#66d9ef">AND</span> high
value <span style="color:#66d9ef">IN</span>(<span style="color:#66d9ef">SELECT</span> value <span style="color:#66d9ef">FROM</span> ... );<span style="color:#75715e">-- here is a subquery
</span><span style="color:#75715e"></span>
<span style="color:#66d9ef">SELECT</span> ... <span style="color:#66d9ef">FROM</span> ... 
<span style="color:#66d9ef">WHERE</span> firstname <span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;Jen%&#39;</span>;<span style="color:#75715e">-- % pettern matching sequence.
</span><span style="color:#75715e"></span>		...		<span style="color:#66d9ef">LIKE</span> <span style="color:#e6db74">&#39;JEN_&#39;</span>;<span style="color:#75715e">-- _ pattern matching single char.
</span></code></pre></div><h5 id="aggregate-function">Aggregate Function:</h5>
<p>min, max, avg, sum, count,&hellip; which take a lot rows of data and return a single value.</p>
<h5 id="group-by">GROUP BY:</h5>
<p>Which group by clause devides the rows returned from SELECT into groups</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span>... <span style="color:#66d9ef">FROM</span>... <span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> col;
<span style="color:#66d9ef">SELECT</span> customer_id, <span style="color:#66d9ef">SUM</span>(amount)
<span style="color:#66d9ef">FROM</span> payment
<span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> costumoer_id
<span style="color:#66d9ef">ORDER</span> <span style="color:#66d9ef">BY</span> <span style="color:#66d9ef">SUM</span>(amount) <span style="color:#66d9ef">DESC</span>

<span style="color:#66d9ef">SELECT</span> col1, some_aggr_fun(col2)
<span style="color:#66d9ef">FROM</span> <span style="color:#66d9ef">table_name</span>
<span style="color:#66d9ef">WHERE</span> ...
<span style="color:#66d9ef">GROUP</span> <span style="color:#66d9ef">BY</span> col1
<span style="color:#66d9ef">HAVING</span> (conditions) <span style="color:#75715e">-- ★★ The condition here is related to the aggr_fun to the first line. ★★
</span></code></pre></div><h5 id="where-vs-having-">WHERE v.s. HAVING ★★★</h5>
<ul>
<li>
<p>WHERE:</p>
<ul>
<li>more cheapter and efficient.</li>
<li>applied when read data from table</li>
<li>there are a lot of optimization techniques, such as indexes, to skip a bunch of rows.</li>
</ul>
</li>
<li>
<p>HAVING:</p>
<ul>
<li><strong>Huge</strong> resource consumption</li>
<li><strong>used to filter data after aggregation function.</strong></li>
<li>usually applied <strong>after <code>GROUP BY</code>.</strong></li>
</ul>
</li>
</ul>
<h5 id="join">JOIN</h5>
<p><code>JOIN</code> means <code>INNER JOIN</code> in SQL.</p>
<p><img src="https://i.stack.imgur.com/VQ5XP.png" alt=""></p>
<h5 id="union">UNION</h5>
<p>Combine result sets of two or more SELECT statements into a single result set.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">SELECT</span> co11, col2
<span style="color:#66d9ef">FROM</span> table1
<span style="color:#66d9ef">UNION</span>
<span style="color:#66d9ef">SELECT</span> col3,col4
<span style="color:#66d9ef">FROM</span> table2
</code></pre></div><h5 id="timestamp-and-extract">TIMESTAMP and EXTRACT</h5>
<p><code>SELECT EXTRACT (DAY FROM payment_date);</code></p>
<h5 id="create-database-and-tables">CREATE DATABASE and TABLES</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> <span style="color:#66d9ef">table_name</span>(col_name1 data_type <span style="color:#66d9ef">PRIMARY</span> <span style="color:#66d9ef">KEY</span>, <span style="color:#75715e">-- this contraint is combination of NOT FULL and UNIQUE
</span><span style="color:#75715e"></span>                        col_name2 data_type,
                          ... )
</code></pre></div><h5 id="constraints">CONSTRAINTS:</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL">
<span style="color:#66d9ef">CHECK</span> <span style="color:#75715e">-- enables to check a condition when you insert or update a data
</span><span style="color:#75715e"></span><span style="color:#66d9ef">NOT</span> <span style="color:#66d9ef">FULL</span> <span style="color:#75715e">-- the value of the table cannot be null
</span><span style="color:#75715e"></span><span style="color:#66d9ef">UNIQUE</span> <span style="color:#75715e">--  the value of the table must be unique across the whole table
</span><span style="color:#75715e"></span><span style="color:#66d9ef">PRIMARY</span> <span style="color:#66d9ef">KEY</span> <span style="color:#75715e">-- must be NOT FULL and UNIQUE
</span><span style="color:#75715e"></span><span style="color:#66d9ef">FOREIGN</span> <span style="color:#66d9ef">KEY</span> <span style="color:#75715e">-- provides a link between data in two tables. it points to a PRIMARY KEY in another table.
</span><span style="color:#75715e"></span><span style="color:#66d9ef">REFERENCES</span> <span style="color:#75715e">-- constr. the value of the col that exists in a col in another table.
</span></code></pre></div><p><img src="https://i.ytimg.com/vi/Osv7AhGq_Vc/maxresdefault.jpg" alt=""></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> <span style="color:#66d9ef">table_name</span> 
(col_name <span style="color:#66d9ef">TYPE</span> col_constraint, table_constraint)
<span style="color:#66d9ef">INHERITS</span> exsiting_table_name
</code></pre></div><h5 id="insert">INSERT</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">INSERT</span> <span style="color:#66d9ef">INTO</span> <span style="color:#66d9ef">table_name</span> (col1,col2,...)
<span style="color:#66d9ef">VALUES</span> (value1,value2,...)

<span style="color:#75715e">/* insert multiple rows: */</span>
<span style="color:#66d9ef">INSERT</span> <span style="color:#66d9ef">INTO</span> <span style="color:#66d9ef">table_name</span>(col1,...)
<span style="color:#66d9ef">VALUES</span>(val1,val2,...),
		(val3,val4,...),...

<span style="color:#75715e">/* insert from other table */</span>
<span style="color:#66d9ef">INSERT</span> <span style="color:#66d9ef">INTO</span> <span style="color:#66d9ef">table_name</span>
<span style="color:#66d9ef">SELECT</span> col1
<span style="color:#66d9ef">FROM</span> another_table
<span style="color:#66d9ef">WHERE</span> condition
</code></pre></div><h5 id="update-delete">UPDATE, DELETE</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">UPDATE</span> <span style="color:#66d9ef">table_name</span>
<span style="color:#66d9ef">SET</span> col1 <span style="color:#f92672">=</span> val1,col2  <span style="color:#f92672">=</span> val2, ...
<span style="color:#66d9ef">WHERE</span> condition

<span style="color:#66d9ef">DELETE</span> <span style="color:#66d9ef">FROM</span> <span style="color:#66d9ef">table_name</span>
<span style="color:#66d9ef">WHERE</span> condition
</code></pre></div><h5 id="alterdrop-table">ALTER/DROP TABLE</h5>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-SQL" data-lang="SQL"><span style="color:#66d9ef">ALTER</span> <span style="color:#66d9ef">TABLE</span> <span style="color:#66d9ef">table_name</span>(a series <span style="color:#66d9ef">of</span> actions)
<span style="color:#66d9ef">DROP</span> <span style="color:#66d9ef">TABLE</span> <span style="color:#66d9ef">table_name</span>
</code></pre></div>]]></content>
        </item>
        
        <item>
            <title>How to Use Auto Completion in GCP</title>
            <link>https://ffflora.cat/posts/2019/08/how-to-use-auto-completion-in-gcp/</link>
            <pubDate>Wed, 28 Aug 2019 21:30:55 -0700</pubDate>
            
            <guid>https://ffflora.cat/posts/2019/08/how-to-use-auto-completion-in-gcp/</guid>
            <description>Auto-completion gcloud interactive has auto prompting for commands and flags, and displays inline help snippets in the lower section as the command is typed.
Static information, like command and sub-command names, and flag names and enumerated flag values, are auto-completed using dropdown menus.
Install the beta components:
gcloud components install beta Enter the gcloud interactive mode:
gcloud beta interactive When using the interactive mode, click on the Tab key to complete file path and resource arguments.</description>
            <content type="html"><![CDATA[<h2 id="auto-completion">Auto-completion</h2>
<p><code>gcloud interactive</code> has auto prompting for commands and flags, and displays inline help snippets in the lower section as the command is typed.</p>
<p>Static information, like command and sub-command names, and flag names and enumerated flag values, are auto-completed using dropdown menus.</p>
<p>Install the beta components:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">gcloud components install beta
</code></pre></div><p>Enter the gcloud interactive mode:</p>
<pre><code>gcloud beta interactive
</code></pre><p>When using the interactive mode, click on the <strong>Tab</strong> key to complete file path and resource arguments. If a dropdown menu appears, use the <strong>Tab</strong> key to move through the list, and the <strong>Space bar</strong> to select your choice.</p>
<p>Try it out! Start typing the following command, using auto-complete to finish the command:</p>
<pre><code>gcloud compute instances describe &lt;your_vm&gt;
</code></pre><p>Across the bottom of Cloud Shell you can see the shortcut to toggles this feature. Try out the F2 toggle:</p>
<p><code>F2</code>: help: STATE Toggles the active help section, ON when enabled, OFF when disabled.</p>
]]></content>
        </item>
        
    </channel>
</rss>
